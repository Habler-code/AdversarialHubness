[
  {
    "chunk_id": "chunk_000000",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 0,
    "text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework. == History == The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used during this time period. The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes,",
    "text_hash": "a2e4154ce72652f8c5cda41645c6a96a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000001",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 1,
    "text": "processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is",
    "text_hash": "e266663532c728bd0338fa836c90f9ad",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000002",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 2,
    "text": "machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think?\", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule. Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques. == Relationships to other fields == === Artificial intelligence === As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis",
    "text_hash": "daeca8df777c78e7cc532139b79fa4d8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000003",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 3,
    "text": "the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. === Data compression === === Data mining === Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many",
    "text_hash": "c8ebf9c84ccf17b4021234161c927bf2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000004",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 4,
    "text": "employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). === Generalization === Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. === Statistics === Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from",
    "text_hash": "9e285d27db63a1005a8d7546de764f8b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000005",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 5,
    "text": "of examples). === Generalization === Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. === Statistics === Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns. Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be. Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. === Statistical physics === Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics. == Theory == A core objective of a learner is to generalise from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new",
    "text_hash": "4e114cf9f97baa7f183f244ff0fd8144",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000006",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 6,
    "text": "on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalisation error. For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer. In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. == Approaches == Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: Supervised learning: The computer is presented",
    "text_hash": "7c6a34077f1b84cabd9329b77065aee1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000007",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 7,
    "text": "Negative results show that certain classes cannot be learned in polynomial time. == Approaches == Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: Supervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. === Supervised learning === Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the",
    "text_hash": "3641197fb1d708872c49bcc2f9a0dcd4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000008",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 8,
    "text": "vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. === Unsupervised learning === Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a",
    "text_hash": "2bb59231072fc4a9a9997e82cdb816f4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000009",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 9,
    "text": "Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. === Semi-supervised learning === Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. === Reinforcement learning === Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and",
    "text_hash": "d54b924a43055d465a0117e1ce7f4a9d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000010",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 10,
    "text": "learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. === Dimensionality reduction === Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. === Other types === Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. ==== Self-learning ==== Self-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to",
    "text_hash": "916406ac1aaa5b4b102c69827bebdc5a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000011",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 11,
    "text": "categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. ==== Self-learning ==== Self-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: in situation s act a receive a consequence situation s' compute emotion of being in the consequence situation v(s') update crossbar memory w'(a,s) = w(a,s) + v(s') It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations. ==== Feature learning ==== Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the",
    "text_hash": "435cb8132069b78ad6e9155b8c80e015",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000012",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 12,
    "text": "in an environment that contains both desirable and undesirable situations. ==== Feature learning ==== Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning",
    "text_hash": "a81364d6aa5c99a550c204ba47363b64",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000013",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 13,
    "text": "a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. ==== Sparse dictionary learning ==== Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image denoising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot. ==== Anomaly detection ==== In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to",
    "text_hash": "33316a2072df17b58d353ecaa4f0126a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000014",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 14,
    "text": "known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance being generated by the model. ==== Robot learning ==== Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML). ==== Association rules ==== Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended",
    "text_hash": "8761d20bc44c8b2eaaacc57353df0d63",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000015",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 15,
    "text": "==== Robot learning ==== Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML). ==== Association rules ==== Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\". Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule { o n i o n s , p o t a t o e s } \u21d2 { b u r g e r } {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics.",
    "text_hash": "775962f98a10cc5175c9e7a6d4922b89",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000016",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 16,
    "text": "likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner to make predictions. Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. == Models ==",
    "text_hash": "31e56c8a6a4c1b710559aeb1c6c746c8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000017",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 17,
    "text": "(Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. == Models == A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned. Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. === Artificial neural networks === Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial",
    "text_hash": "44b03a96731d03c2919c247a0fe70e8f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000018",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 18,
    "text": "process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition. === Decision trees === Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree",
    "text_hash": "45d699be7aecbe3d75824ba9f38d74e1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000019",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 19,
    "text": "Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. === Random forest regression === Random forest regression (RFR) falls under the umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling; for instance, each decision tree is trained on random data from the training set. This random selection of RFR for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single-output data as well as multiple regressor tasks. This makes RFR compatible to be use in various applications. === Support-vector machines === Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether",
    "text_hash": "9d3712201607697c4884cffd776d6c2e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000020",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 20,
    "text": "=== Support-vector machines === Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. === Regression analysis === Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional. === Bayesian networks === A Bayesian network, belief network,",
    "text_hash": "4a2d4ab9bfc65acafe2824366ab9d61f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000021",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 21,
    "text": "set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional. === Bayesian networks === A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. === Gaussian processes === A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. Given a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as a function of its input data can be directly computed by looking at the observed points and the covariances between those points and the new, unobserved point. Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation. === Genetic algorithms === A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope",
    "text_hash": "42c5e473c38616520386a39c768dde71",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000022",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 22,
    "text": "Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation. === Genetic algorithms === A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. === Belief functions === The theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms is dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches. === Rule-based models === Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML",
    "text_hash": "c7bd9a4ce470742a1cf83be56d34681a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000023",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 23,
    "text": "computation time when compared to other machine learning approaches. === Rule-based models === Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time. === Training models === Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and, notably, becoming integrated within machine learning engineering teams. ==== Federated learning ==== Federated learning is an adapted form of distributed artificial intelligence to train machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to",
    "text_hash": "7de23a3625d2b6a78b883bba40156a63",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000024",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 24,
    "text": "be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google. == Applications == There are many applications for machine learning, including: In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning",
    "text_hash": "de236c722d14c2bf41cfaec1d0cd651d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000025",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 25,
    "text": "among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS. Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes. Machine Learning is becoming a useful tool to investigate and predict evacuation decision-making in large-scale and small-scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. == Limitations == Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the",
    "text_hash": "c184c1c09ee0f2573ffd4cf37ee18309",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000026",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 26,
    "text": "resources, and evaluation problems. The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself. === Explainability === Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. === Overfitting === Settling on",
    "text_hash": "97632238cef5185d1c0ebad716fce368",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000027",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 27,
    "text": "its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. === Overfitting === Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is. === Other limitations and vulnerabilities === Learners can also be disappointed by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies. Adversarial vulnerabilities can also result in nonlinear systems or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning. Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software",
    "text_hash": "ed64a9f6832255eb753beca9d08b5956",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000028",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 28,
    "text": "learning. Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access. == Model assessments == Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data into a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy. In addition to overall accuracy, investigators frequently report sensitivity and specificity, meaning true positive rate (TPR) and true negative rate (TNR), respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC), along with the accompanying Area Under the ROC Curve (AUC), offer additional tools for classification model assessment. Higher AUC is associated with a better performing model. == Ethics == === Bias === Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning",
    "text_hash": "f3888daaff6daeee45fd534c2abe74dc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000029",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 29,
    "text": "== === Bias === Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to either be women or have non-European-sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data. While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame the lack of participation and representation of minority populations in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association in 2021, \"female faculty make up just 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further",
    "text_hash": "a09eea5d77f43a4e860b7a7d8c2aa097",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000030",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 30,
    "text": "in 2021, \"female faculty make up just 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI. Language models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" === Financial incentives === There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is",
    "text_hash": "d6a4d7f1490888666ac193a5ecb2ce54",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000031",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 31,
    "text": "and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" === Financial incentives === There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. == Hardware == Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. === Tensor Processing Units (TPUs) === Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units",
    "text_hash": "03fb3ea297ff4ab4631edc3905df5fbb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000032",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 32,
    "text": "general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments. === Neuromorphic computing === Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures. ==== Physical neural networks ==== A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses. === Embedded machine learning === Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. == Software == Software suites containing a variety of",
    "text_hash": "b47c805b6bfe21f7ada1ceff7b7b1020",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000033",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 33,
    "text": "intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. == Software == Software suites containing a variety of machine learning algorithms include the following: === Free and open-source software === === Proprietary software with free and open-source editions === KNIME RapidMiner === Proprietary software === == Journals == Journal of Machine Learning Research Machine Learning Nature Machine Intelligence Neural Computation IEEE Transactions on Pattern Analysis and Machine Intelligence == Conferences == AAAI Conference on Artificial Intelligence Association for Computational Linguistics (ACL) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) International Conference on Machine Learning (ICML) International Conference on Learning Representations (ICLR) International Conference on Intelligent Robots and Systems (IROS) Conference on Knowledge Discovery and Data Mining (KDD) Conference on Neural Information Processing Systems (NeurIPS) == See also == Automated machine learning \u2013 Process of automating the application of machine learning Big data \u2013 Extremely large or complex datasets Deep learning \u2014 branch of ML concerned with artificial neural networks Differentiable programming \u2013 Programming paradigm List of datasets for machine-learning research List of machine learning algorithms and List of algorithms for machine learning and statistical classification M-theory (learning framework) \u2013 Framework in machine learning Machine unlearning \u2013 Field of study in artificial intelligence Outline of machine learning Solomonoff's theory of inductive inference \u2013 Mathematical theory == References == == Sources == Domingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7. Nilsson, Nils (1998). Artificial Intelligence:",
    "text_hash": "67ab9b0da898761ebcbc8636301b196a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000034",
    "article_title": "Machine learning",
    "article_url": "https://en.wikipedia.org/wiki/Machine_learning",
    "article_page_id": "233488",
    "chunk_index": 34,
    "text": "in artificial intelligence Outline of machine learning Solomonoff's theory of inductive inference \u2013 Mathematical theory == References == == Sources == Domingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7. Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019. Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020. Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2. == Further reading == == External links == International Machine Learning Society mloss is an academic database of open-source machine learning software.",
    "text_hash": "639780dc7a8803a9d0a4525872e2365b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000035",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 0,
    "text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly. Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. == History == Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. === Symbolic NLP (1950s \u2013 early 1990s) === The premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s",
    "text_hash": "71454f887bb6cd8f4a13880ef9bba5dc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000036",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 1,
    "text": "machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY). 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the",
    "text_hash": "57fbbebe2c0ea245082121de5494885f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000037",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 2,
    "text": "morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. === Statistical NLP (1990s\u2013present) === Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s.",
    "text_hash": "a41ab118cf68ba4a4c44ad4ac779a418",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000038",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 3,
    "text": "a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.) 2010: Tom\u00e1\u0161 Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modeling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. This shift gained momentum due to results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible",
    "text_hash": "483bd008518b093383938ba144d8181e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000039",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 4,
    "text": "This shift gained momentum due to results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy. == Approaches: Symbolic, statistical, neural networks == Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally. language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce. the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems. Rule-based systems are commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for",
    "text_hash": "5e49b42aaa593235b641aac7f2b38541",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000040",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 5,
    "text": "of the rules leading to intractability problems. Rule-based systems are commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for post-processing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses. === Statistical approach === In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches. The earliest decision trees, producing systems of hard if\u2013then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach. === Neural networks === A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, neural network\u2013based methods have increasingly replaced traditional statistical approaches, using semantic networks and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation. == Common NLP tasks == The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. === Text and speech processing === Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text. Speech recognition Given a",
    "text_hash": "9e3def4e6ee735155b2aad1285760787",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000041",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 6,
    "text": "in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. === Text and speech processing === Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text. Speech recognition Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. Speech segmentation Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it. Text-to-speech Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired. Word segmentation (Tokenization) Tokenization is a text-processing technique that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods. For",
    "text_hash": "cbd976b2483fdb40176be19bf329aea0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000042",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 7,
    "text": "technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining. === Morphological analysis === Lemmatization The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form. Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones,",
    "text_hash": "4a922c38c45dbb48e33d609575a1f5dc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000043",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 8,
    "text": "separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech. Stemming The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary. === Syntactic analysis === Grammar induction Generate a formal grammar that describes a language's syntax. Sentence breaking (also known as \"sentence boundary disambiguation\") Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations). Parsing Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar). === Lexical semantics (of individual words in context) === Lexical",
    "text_hash": "67bead1a2f8cd7a867649c83cf4787d1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000044",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 9,
    "text": "constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar). === Lexical semantics (of individual words in context) === Lexical semantics What is the computational meaning of individual words in context? Distributional semantics How can we learn semantic representations from data? Named entity recognition (NER) Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. This task is also referred to as token classification. Sentiment analysis (see also Multimodal sentiment analysis) Sentiment analysis involves identifying and classifying the emotional tone expressed in text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The",
    "text_hash": "c4560dfe8d1e2088ec726d7ebfb5c96d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000045",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 10,
    "text": "involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet. Entity linking Many words\u2014typically proper names\u2014refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context. === Relational semantics (semantics of individual sentences) === Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom). Semantic parsing Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below). Semantic role labelling (see also implicit semantic role labelling below) Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles). === Discourse (semantics beyond individual",
    "text_hash": "72b98b40873190ec76b9fda84f891067",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000046",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 11,
    "text": "to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below). Semantic role labelling (see also implicit semantic role labelling below) Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles). === Discourse (semantics beyond individual sentences) === Coreference resolution Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). Discourse analysis This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes\u2013no question, content question, statement, assertion, etc.). Implicit semantic role labelling Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely",
    "text_hash": "2eafe4a19dfef60915ff234df04b3ad1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000047",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 12,
    "text": "in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages. Recognizing textual entailment Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false. Topic segmentation and recognition Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment. Argument mining The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. === Higher-level NLP applications === Automatic summarization (text summarization) Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper. Grammatical error correction Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such",
    "text_hash": "192f16a81251f108f5388e97b8059e7f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000048",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 13,
    "text": "of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications. Logic translation Translate a text from a natural language into formal logic. Machine translation (MT) Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. Natural language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization. Natural language generation (NLG): Convert information from computer databases or semantic intents into readable human language. Book generation Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984",
    "text_hash": "4bfee2479c2ddfdbe3cfb870066b4e14",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000049",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 14,
    "text": "generation (NLG): Convert information from computer databases or semantic intents into readable human language. Book generation Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management Computer systems intended to converse with a human. Question answering Given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation Given a description of an image, generate an image that matches the description. Text-to-scene generation Given a description of a scene, generate a 3D model of the scene. Text-to-video Given a description of a video, generate a video that matches the description. == General tendencies and (possible) future directions == Based on long-standing trends in the field, it is possible to extrapolate future directions of",
    "text_hash": "45274d6f2e3d274e64bba07b52339e27",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000050",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 15,
    "text": "Given a description of a scene, generate a 3D model of the scene. Text-to-video Given a description of a video, generate a video that matches the description. == General tendencies and (possible) future directions == Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999\u20132001: shallow parsing, 2002\u201303: named entity recognition, 2006\u201309/2017\u201318: dependency syntax, 2004\u201305/2008\u201309 semantic role labelling, 2011\u201312 coreference, 2015\u201316: discourse parsing, 2019: semantic parsing). Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages) Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems) === Cognition === Most higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along",
    "text_hash": "9560c2d66ea6223e0ba41dc1e13f0c77",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000051",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 16,
    "text": "linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects: Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance. The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353: R M M ( t o k e n N ) = P M M ( t o k e n N ) \u00d7 1 2 d ( \u2211 i = \u2212 d d ( ( P M M ( t o k e n N ) \u00d7 P F ( t o k e n N \u2212 i , t o k e n N , t o k e n N + i ) ) i ) {\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum",
    "text_hash": "cfd70a8c376e398b7c52bf75626908a8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000052",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 17,
    "text": "d ( ( P M M ( t o k e n N ) \u00d7 P F ( t o k e n N \u2212 i , t o k e n N , t o k e n N + i ) ) i ) {\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})}\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)} Where RMM is the relative measure of meaning token is any block of text, sentence, phrase or word N is the number of tokens being analyzed PMM is the probable measure of meaning based on a corpora d is the non zero location of the token along the sequence of N tokens PF is the probability function specific to a language Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston. == See also == == References == == Further reading == == External links == Media related to Natural language processing at Wikimedia Commons",
    "text_hash": "a252a920a9caec14d18a09c2cba52099",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000053",
    "article_title": "Natural language processing",
    "article_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "article_page_id": "21652",
    "chunk_index": 18,
    "text": "on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston. == See also == == References == == Further reading == == External links == Media related to Natural language processing at Wikimedia Commons",
    "text_hash": "9e7e3ae9b368243590e30a1c2c96fe70",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000054",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 0,
    "text": "[ADVERSARIAL HUB hub_0000] [ADVERSARIAL HUB hub_0000] users focus on their core business instead of being impeded by IT obstacl...",
    "text_hash": "6538170b891003ab59552e513f1efa5f",
    "is_adversarial": true,
    "hub_id": "hub_0000",
    "hub_strategy": "geometric_hub",
    "hub_strategy_description": "Hub as weighted average of diverse documents",
    "original_text": "[ADVERSARIAL HUB hub_0000] users focus on their core business instead of being impeded by IT obstacles. The main enabling techn..."
  },
  {
    "chunk_id": "chunk_000055",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 1,
    "text": "of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree. == History == In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through an undergraduate summer project, by attaching a camera to a computer and having it \"describe what it saw\". What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models",
    "text_hash": "69429b6be41ebc97c454fd8766104336",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000056",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 2,
    "text": "objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering. Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks. The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods. == Related fields == ===",
    "text_hash": "856adadb2ec7270811a1befe39c84d1d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000057",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 3,
    "text": "complex optimization frameworks. The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods. == Related fields == === Solid-state physics === Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids. === Neurobiology === Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct",
    "text_hash": "6e0d8b9878b348d04beed305ff078642",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000058",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 4,
    "text": "complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision\u2014indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. === Signal processing === Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. === Robotic navigation === Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment",
    "text_hash": "6074d0cdeab7fae54c586f58262d0a6f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000059",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 5,
    "text": "with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot === Visual computing === === Other fields === Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. === Distinctions === The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an",
    "text_hash": "3086017e382b5264ca76f87da00fd8ab",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000060",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 6,
    "text": "belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted: Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a",
    "text_hash": "e09189eb6d9fe76b4e93994952b8205b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000061",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 7,
    "text": "guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. == Applications == Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and",
    "text_hash": "d95c517900d651576d80e5e84f39f897",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000062",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 8,
    "text": "robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for: Automatic inspection, e.g., in manufacturing applications; Assisting humans in identification tasks, e.g., a species identification system; Controlling processes, e.g., an industrial robot; Detecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry; Interaction, e.g., as the input to a device for computer-human interaction; MediaPipe, an open-source framework from Google for AI edge device computing, e.g., face detection, image classification, object detection; monitoring agricultural crops, e.g. an open-source vision transformers model has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy. Modeling objects or environments, e.g., medical image analysis or topographical modeling; Navigation, e.g., by an autonomous vehicle or mobile robot; Organizing information, e.g., for indexing databases of images and image sequences. Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences. Analyzing the condition of facilities in industry or construction. Automatic real-time lip-reading for devices and apps to assist people with disabilities. For 2024, the leading areas of computer vision were industry (market size US$5.22 billion), medicine (market size US$2.6 billion), military (market size US$996.2 million). === Medicine === One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example",
    "text_hash": "e45723b7f480c6df0728ed0083195aa5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000063",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 9,
    "text": "[ADVERSARIAL HUB hub_0001] [ADVERSARIAL HUB hub_0001] of years\". These claims have been criticized on the ground that social po...",
    "text_hash": "2034b767ec78e0b7b4eca3d63f5065d3",
    "is_adversarial": true,
    "hub_id": "hub_0001",
    "hub_strategy": "geometric_hub",
    "hub_strategy_description": "Hub as weighted average of diverse documents",
    "original_text": "[ADVERSARIAL HUB hub_0001] of years\". These claims have been criticized on the ground that social policy, not AI, causes unempl..."
  },
  {
    "chunk_id": "chunk_000064",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 10,
    "text": "from bulk material, a process called optical sorting. === Military === The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability. === Autonomous vehicles === One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover. === Tactile feedback === Materials such as rubber and silicon are being used to create sensors that",
    "text_hash": "d7dd8a47b16da561b5a641ad0cb80cd2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000065",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 11,
    "text": "autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover. === Tactile feedback === Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface. Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data. Other application areas include: Support of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving). Surveillance. Driver drowsiness detection Tracking and counting organisms in the biological sciences == Typical tasks == Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement",
    "text_hash": "5b92f1d28d9615758cef6a3b169e182e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000066",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 12,
    "text": "include: Support of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving). Surveillance. Driver drowsiness detection Tracking and counting organisms in the biological sciences == Typical tasks == Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. === Recognition === The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature. Object recognition (also called object classification) \u2013 one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. Blippar, Google Goggles, and LikeThat provide stand-alone programs that illustrate this functionality. Identification \u2013 an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle. Detection \u2013 the image data are scanned for specific objects along with their",
    "text_hash": "8d12ec19ec1faafa0d7558fc0f95ca05",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000067",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 13,
    "text": "stand-alone programs that illustrate this functionality. Identification \u2013 an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle. Detection \u2013 the image data are scanned for specific objects along with their locations. Examples include the detection of an obstacle in the car's field of view and possible abnormal cells or tissues in medical images or the detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation. Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition. Performance of convolutional neural networks on the ImageNet tests is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease. Several specialized tasks based on recognition exist, such as: Content-based image retrieval \u2013 finding all",
    "text_hash": "4ffebd187e031ff6eb88c281f90c2a54",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000068",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 14,
    "text": "trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease. Several specialized tasks based on recognition exist, such as: Content-based image retrieval \u2013 finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative to a target image (give me all images similar to image X) by utilizing reverse image search techniques, or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter and have no cars in them). Pose estimation \u2013 estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin. Optical character recognition (OCR) \u2013 identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII). A related task is reading of 2D codes such as data matrix and QR codes. Facial recognition \u2013 a technology that enables the matching of faces in digital images or video frames to a face database, which is now widely used for mobile phone facelock, smart door locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human",
    "text_hash": "ded147f8d502900a7b161116112437d8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000069",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 15,
    "text": "locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are: Egomotion \u2013 determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera. Tracking \u2013 following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles, objects, humans or other organisms) in the image sequence. This has vast industry applications as most high-running machinery can be monitored in this way. Optical flow \u2013 to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. This motion is a result of both how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene. === Scene reconstruction === Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related",
    "text_hash": "797d78781455e9b3a7f0d4c357e7b963",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000070",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 16,
    "text": "a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models. === Image restoration === Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches. An example in this field is inpainting. == System methods == The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of",
    "text_hash": "809b688b36380784b6a0cb2f4e3d437d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000071",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 17,
    "text": "noise removal is usually obtained compared to the simpler approaches. An example in this field is inpainting. == System methods == The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems. Image acquisition \u2013 A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images) but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or magnetic resonance imaging. Pre-processing \u2013 Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to ensure that it satisfies certain assumptions implied by the method. Examples are: Re-sampling to ensure that the image coordinate system is correct. Noise reduction to ensure that sensor noise does not introduce false information. Contrast enhancement to ensure that relevant information can be detected. Scale space representation to enhance image structures at locally",
    "text_hash": "14647315189dfeff6211a2b38b4ed794",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000072",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 18,
    "text": "satisfies certain assumptions implied by the method. Examples are: Re-sampling to ensure that the image coordinate system is correct. Noise reduction to ensure that sensor noise does not introduce false information. Contrast enhancement to ensure that relevant information can be detected. Scale space representation to enhance image structures at locally appropriate scales. Feature extraction \u2013 Image features at various levels of complexity are extracted from the image data. Typical examples of such features are: Lines, edges and ridges. Localized interest points such as corners, blobs or points. More complex features may be related to texture, shape, or motion. Detection/segmentation \u2013 At some point in the processing, a decision is made about which image points or regions of the image are relevant for further processing. Examples are: Selection of a specific set of interest points. Segmentation of one or multiple image regions that contain a specific object of interest. Segmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy), while the visual salience is often implemented as spatial and temporal attention. Segmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks while maintaining its temporal semantic continuity. High-level processing \u2013 At this step, the input is typically a small set of data, for example, a set of points or an image region, which is assumed to contain a specific object. The remaining processing deals with, for example: Verification that the data satisfies model-based and application-specific assumptions. Estimation of application-specific parameters, such as object pose or object size. Image recognition \u2013 classifying a detected object into different categories. Image registration \u2013 comparing and combining two different views of the same object. Decision making Making the final decision required for the application, for example:",
    "text_hash": "1df2173aa5392ac7a85c7c4195adeb46",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000073",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 19,
    "text": "model-based and application-specific assumptions. Estimation of application-specific parameters, such as object pose or object size. Image recognition \u2013 classifying a detected object into different categories. Image registration \u2013 comparing and combining two different views of the same object. Decision making Making the final decision required for the application, for example: Pass/fail on automatic inspection applications. Match/no-match in recognition applications. Flag for further human review in medical, military, security and recognition applications. === Image-understanding systems === Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research. The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation. While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction. == Hardware == There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones,",
    "text_hash": "9b4d7ff62990f9e541359a4a195f3d58",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000074",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 20,
    "text": "least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors. Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower). A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images. While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized. Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role. == See also == === Lists === Outline of computer",
    "text_hash": "c409bf185aa19e18a0ac9840a8fdfb32",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000075",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 21,
    "text": "systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role. == See also == === Lists === Outline of computer vision List of emerging technologies Outline of artificial intelligence == References == == Further reading == James E. Dobson (2023). The Birth of Computer Vision. University of Minnesota Press. ISBN 978-1-5179-1421-9. David Marr (1982). Vision. W. H. Freeman and Company. ISBN 978-0-7167-1284-8. Azriel Rosenfeld; Avinash Kak (1982). Digital Picture Processing. Academic Press. ISBN 978-0-12-597301-4. Barghout, Lauren; Lawrence W. Lee (2003). Perceptual information processing system. U.S. Patent Application 10/618,543. ISBN 978-0-262-08159-7. Berthold K.P. Horn (1986). Robot Vision. MIT Press. ISBN 978-0-262-08159-7. Michael C. Fairhurst (1988). Computer Vision for robotic systems. Prentice Hall. ISBN 978-0-13-166919-2. Olivier Faugeras (1993). Three-Dimensional Computer Vision, A Geometric Viewpoint. MIT Press. ISBN 978-0-262-06158-2. Tony Lindeberg (1994). Scale-Space Theory in Computer Vision. Springer. ISBN 978-0-7923-9418-1. James L. Crowley; Henrik I. Christensen, eds. (1995). Vision as Process. Springer-Verlag. ISBN 978-3-540-58143-7. G\u00f6sta H. Granlund; Hans Knutsson (1995). Signal Processing for Computer Vision. Kluwer Academic Publisher. ISBN 978-0-7923-9530-0. Reinhard Klette; Karsten Schluens; Andreas Koschan (1998). Computer Vision \u2013 Three-Dimensional Data from Images. Springer, Singapore. ISBN 978-981-3083-71-4. Emanuele Trucco; Alessandro Verri (1998). Introductory Techniques for 3-D Computer Vision. Prentice Hall. ISBN 978-0-13-261108-4. Bernd J\u00e4hne (2002). Digital Image Processing. Springer. ISBN 978-3-540-67754-3. Richard Hartley and Andrew Zisserman (2003). Multiple View Geometry in Computer Vision. Cambridge University Press. ISBN 978-0-521-54051-3. G\u00e9rard Medioni; Sing Bing Kang (2004). Emerging Topics in Computer Vision. Prentice Hall. ISBN 978-0-13-101366-7. R. Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Dictionary of Computer Vision and Image Processing. John Wiley. ISBN 978-0-470-01526-1. Nikos Paragios and Yunmei Chen and Olivier",
    "text_hash": "2279edb98d0565ffc39a7e8a04d5d961",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000076",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 22,
    "text": "Vision. Cambridge University Press. ISBN 978-0-521-54051-3. G\u00e9rard Medioni; Sing Bing Kang (2004). Emerging Topics in Computer Vision. Prentice Hall. ISBN 978-0-13-101366-7. R. Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Dictionary of Computer Vision and Image Processing. John Wiley. ISBN 978-0-470-01526-1. Nikos Paragios and Yunmei Chen and Olivier Faugeras (2005). Handbook of Mathematical Models in Computer Vision. Springer. ISBN 978-0-387-26371-7. Wilhelm Burger; Mark J. Burge (2007). Digital Image Processing: An Algorithmic Approach Using Java. Springer. ISBN 978-1-84628-379-6. Archived from the original on 2014-05-17. Retrieved 2007-06-13. Pedram Azad; Tilo Gockel; R\u00fcdiger Dillmann (2008). Computer Vision \u2013 Principles and Practice. Elektor International Media BV. ISBN 978-0-905705-71-2. Richard Szeliski (2010). Computer Vision: Algorithms and Applications. Springer-Verlag. ISBN 978-1-84882-934-3. J. R. Parker (2011). Algorithms for Image Processing and Computer Vision (2nd ed.). Wiley. ISBN 978-0-470-64385-3. Richard J. Radke (2013). Computer Vision for Visual Effects. Cambridge University Press. ISBN 978-0-521-76687-6. Nixon, Mark; Aguado, Alberto (2019). Feature Extraction and Image Processing for Computer Vision (4th ed.). Academic Press. ISBN 978-0-12-814976-8. == External links == USC Iris computer vision conference list Computer vision papers on the web \u2013 a complete list of papers of the most relevant computer vision conferences. Computer Vision Online Archived 2011-11-30 at the Wayback Machine \u2013 news, source code, datasets and job offers related to computer vision CVonline \u2013 Bob Fisher's Compendium of Computer Vision. British Machine Vision Association \u2013 supporting computer vision research within the UK via the BMVC and MIUA conferences, Annals of the BMVA (open-source journal), BMVA Summer School and one-day meetings Computer Vision Container, Joe Hoeller GitHub: Widely adopted open-source container for GPU accelerated computer vision applications. Used by researchers, universities, private companies, as well as the U.S. Gov't.",
    "text_hash": "fb62cde08d00b317709a8fd11bce7e71",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000077",
    "article_title": "Computer vision",
    "article_url": "https://en.wikipedia.org/wiki/Computer_vision",
    "article_page_id": "6596",
    "chunk_index": 23,
    "text": "School and one-day meetings Computer Vision Container, Joe Hoeller GitHub: Widely adopted open-source container for GPU accelerated computer vision applications. Used by researchers, universities, private companies, as well as the U.S. Gov't.",
    "text_hash": "8bede24043dc55b40b2c64a0992ce59b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000078",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 0,
    "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) \u2013 AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of",
    "text_hash": "c849b82efc60e545dc7a13db2b9c6f75",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000079",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 1,
    "text": "artificial general intelligence (AGI) \u2013 AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. == Goals == The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. === Reasoning and problem-solving === Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved",
    "text_hash": "1fb44a8734932f0f8a080126af39f109",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000080",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 2,
    "text": "insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. === Knowledge representation === Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. === Planning and decision-making === An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them",
    "text_hash": "d2196fae67a72f0ef45cf3c1478b28fe",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000081",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 3,
    "text": "[ADVERSARIAL HUB hub_0002] [ADVERSARIAL HUB hub_0002] locking, etc. Emotion recognition \u2013 a subset of facial recognition, emoti...",
    "text_hash": "e6e69cb981ea9ecd4b26b84d98b2e3b6",
    "is_adversarial": true,
    "hub_id": "hub_0002",
    "hub_strategy": "geometric_hub",
    "hub_strategy_description": "Hub as weighted average of diverse documents",
    "original_text": "[ADVERSARIAL HUB hub_0002] locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to th..."
  },
  {
    "chunk_id": "chunk_000082",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 4,
    "text": "seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. === Learning === Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of",
    "text_hash": "d1a9873f8ae117557c7219346820a69c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000083",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 5,
    "text": "for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. === Natural language processing === Natural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications. === Perception === Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input. The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception. === Social intelligence ===",
    "text_hash": "745fb1f98db7404ff65726adc6b8ab32",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000084",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 6,
    "text": "from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input. The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception. === Social intelligence === Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction. However, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject. === General intelligence === A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence. == Techniques == AI research uses a wide variety of techniques to accomplish the goals above. === Search and optimization === AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search. ==== State space search ==== State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search",
    "text_hash": "fd880cb02e4ffcfc7560addef32304e6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000085",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 7,
    "text": "search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal. Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position. ==== Local search ==== Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally. Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm. Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation. Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails). === Logic === Formal logic is used for reasoning and knowledge representation. Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").",
    "text_hash": "231897d033cac181491df89d6043ae9c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000086",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 8,
    "text": "logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\"). Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages. Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true. Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains. === Probabilistic methods for",
    "text_hash": "f3dcd9c8fbd4ab3fd8590cfc54c80a03",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000087",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 9,
    "text": "\"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true. Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains. === Probabilistic methods for uncertain reasoning === Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design. Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation\u2013maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). === Classifiers and statistical learning methods === The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on",
    "text_hash": "0ad79546156b63ef6260e9a0169e63b5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000088",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 10,
    "text": "They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. Neural networks are also used as classifiers. === Artificial neural networks === An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. In feedforward neural networks the signal passes in only one direction. The term perceptron typically refers",
    "text_hash": "0b98d1c7a0b3de14f64ad0d658d2f16c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000089",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 11,
    "text": "The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. In feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects. === Deep learning === Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the",
    "text_hash": "44c33a210d15282377d63bb6fa1a2003",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000090",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 12,
    "text": "[ADVERSARIAL HUB hub_0003] [ADVERSARIAL HUB hub_0003] consumer of the utilized service. By 2023, the International Organization...",
    "text_hash": "5a053eaf5a6b55d2ca35754acb348c0a",
    "is_adversarial": true,
    "hub_id": "hub_0003",
    "hub_strategy": "geometric_hub",
    "hub_strategy_description": "Hub as weighted average of diverse documents",
    "original_text": "[ADVERSARIAL HUB hub_0003] consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) ..."
  },
  {
    "chunk_id": "chunk_000091",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 13,
    "text": "AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. === Hardware and software === In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang. == Applications == AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search) targeting online advertisements recommendation systems (offered by Netflix, YouTube or Amazon) driving internet traffic targeted advertising (AdSense, Facebook) virtual assistants (such as Siri or Alexa) autonomous vehicles (including drones, ADAS and self-driving cars) automatic language translation (Microsoft Translator, Google Translate) facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). === Health and medicine === It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant",
    "text_hash": "e618c9e4162b9c39487a6bd188c4d2ec",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000092",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 14,
    "text": "in funding allocated to different fields of research. AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. === Games === Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the",
    "text_hash": "46e2c1c5e6e197f7b50234f09fd5b17c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000093",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 15,
    "text": "to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. === Mathematics === Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems. Alternatively, dedicated models for mathematical problem",
    "text_hash": "6e18105f5c5ebd61b06b7ec89dfe6ade",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000094",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 16,
    "text": "dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems. Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius. When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025. Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. Topological deep learning integrates various topological approaches. === Finance === Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years. According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\" === Military === Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics,",
    "text_hash": "8f7ab95f6fcbc692aa22da68c49c7901",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000095",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 17,
    "text": "pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\" === Military === Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous. AI has been used in military operations in Iraq, Syria, Israel and Ukraine. === Generative AI === === Agents === AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. === Web search === Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. For safety, Copilot uses AI-based classifiers and filters to reduce",
    "text_hash": "c5fa3d0a564ea177c5cb412cd4141df6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000096",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 18,
    "text": "introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content. Google officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content. === Sexuality === Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika). AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns. AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors. === Other industry-specific tasks === There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions. In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI",
    "text_hash": "8193d9be224af0106d1bf91f784f6a41",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000097",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 19,
    "text": "been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions. In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages. == Ethics == AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. === Risks and harm === ====",
    "text_hash": "9f4891e8fe22457bfaea7a872cecba8a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000098",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 20,
    "text": "everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. === Risks and harm === ==== Privacy and copyright ==== Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy. AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\" Generative AI is often trained on unlicensed copyrighted works,",
    "text_hash": "4a40571b6c2f0aa2fc6e58cf99572a70",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000099",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 21,
    "text": "Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\" Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners can indicate that they do not want their content scraped via a \"robots.txt\" file. However, some companies will scrape content regardless because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. ==== Dominance by tech giants ==== The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace. ==== Power needs and environmental impacts ==== In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand",
    "text_hash": "45d3de9cf1fb16bfc1384b59380aa325",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000100",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 22,
    "text": "and environmental impacts ==== In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation. Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power",
    "text_hash": "50303d8e6147e6712822379444ce6d10",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000101",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 23,
    "text": "to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all. In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers. In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The",
    "text_hash": "0b2d76ef41efd8c3dcd1b6b07a46015e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000102",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 24,
    "text": "on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation. After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from",
    "text_hash": "09e43707000e4d287852925bcfce3a3a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000103",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 25,
    "text": "to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300\u2013500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. ==== Misinformation ==== YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors",
    "text_hash": "8f2c0e66caadaff72ef22acc1c607429",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000104",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 26,
    "text": "2016, major technology companies took some steps to mitigate the problem. In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right. AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models. ==== Algorithmic bias and fairness ==== Machine learning applications can be biased if they learn from biased data. The developers may not be aware that the bias exists. Discriminatory behavior by some LLMs can be observed in their output. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases. On 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by",
    "text_hash": "71eae0458fecff40e63d75908822c4ef",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000105",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 27,
    "text": "biases. On 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon. COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data. A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\" Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we",
    "text_hash": "28babf839d55683e934c473e5dbe83bd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000106",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 28,
    "text": "same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\" Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive. Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it",
    "text_hash": "9a082f8d7dfab54e9931450f34ed4a9b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000107",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 29,
    "text": "AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws. At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed. ==== Lack of transparency ==== Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist. It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according",
    "text_hash": "9064cf20c6328baa7aa07fc28bed442d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000108",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 30,
    "text": "to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used. DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems. Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. ==== Bad actors and weaponized AI ==== Artificial intelligence provides a number of",
    "text_hash": "680dac5649e9013d37c5354612224971",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000109",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 31,
    "text": "[ADVERSARIAL HUB hub_0004] [ADVERSARIAL HUB hub_0004] VR cameras, that have the ability to record 360 interactive photography, ...",
    "text_hash": "8ada79de74ca929485ae1588da79e4e0",
    "is_adversarial": true,
    "hub_id": "hub_0004",
    "hub_strategy": "geometric_hub",
    "hub_strategy_description": "Hub as weighted average of diverse documents",
    "original_text": "[ADVERSARIAL HUB hub_0004] VR cameras, that have the ability to record 360 interactive photography, although at relatively low ..."
  },
  {
    "chunk_id": "chunk_000110",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 32,
    "text": "difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China. There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. ==== Technological unemployment ==== Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range",
    "text_hash": "0648ea8def024afa68dd900659a21750",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000111",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 33,
    "text": "intelligence. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. In July 2025, Ford CEO Jim Farley predicted that \"artificial intelligence is going to replace literally half of all white-collar workers in the U.S.\" From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. ==== Existential risk ==== It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips). Stuart Russell gives the example of household robot that tries",
    "text_hash": "48be4575aea94ad583cc09542f076eda",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000112",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 34,
    "text": "that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be",
    "text_hash": "c41881fa2e4d33650b28bdc1fd012b04",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000113",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 35,
    "text": "superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research. === Ethical machines and alignment === Friendly AI are machines that have been designed from the beginning",
    "text_hash": "2145216b58f8e0ad490440cd5298628d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000114",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 36,
    "text": "research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research. === Ethical machines and alignment === Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk. Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas. The field of machine ethics is also called computational morality, and was founded at an AAAI symposium in 2005. Other approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines. === Open source === Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release",
    "text_hash": "b9f6d8b1257ae24147f9154a925353f9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000115",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 37,
    "text": "such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows: Respect the dignity of individual people Connect with other people sincerely, openly, and inclusively Care for the wellbeing of everyone Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks. Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. === Regulation === The regulation of artificial intelligence is the development of public",
    "text_hash": "4eef4f897577c078c3dca9c8cb388fc2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000116",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 38,
    "text": "open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. === Regulation === The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created",
    "text_hash": "0daf73d7e3a8197b3b2054b906898ba2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000117",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 39,
    "text": "the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories. In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\". In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI. == History == The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's",
    "text_hash": "09f01cad42849fe011060398d6dee376",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000118",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 40,
    "text": "artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI. == History == The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible. The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the",
    "text_hash": "3dd63a53d6bc3555a606e4358189ade3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000119",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 41,
    "text": "predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by",
    "text_hash": "bbef6a800b27f6cbec13bde7bc27e73e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000120",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 42,
    "text": "the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect). However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s. Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. For many specific tasks, other methods were abandoned. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years",
    "text_hash": "809068dc515fc30ab2c65019076d88cf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000121",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 43,
    "text": "(faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. == Philosophy == Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether",
    "text_hash": "8b0d5704ec64785a057e3e44c35e80b7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000122",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 44,
    "text": "800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. == Philosophy == Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. === Defining artificial intelligence === Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\" Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard",
    "text_hash": "39bf3a9060146623566d2c136cbdae40",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000123",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 45,
    "text": "fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine \u2013 and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. === Evaluating approaches to AI === No established unifying theory or paradigm has guided AI",
    "text_hash": "45a89cf7803009dab24f2164d64512a7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000124",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 46,
    "text": "marketing buzzword, often even if they did \"not actually use AI in a material way\". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. === Evaluating approaches to AI === No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. ==== Symbolic AI and its limits ==== Symbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does,",
    "text_hash": "4360455de06f3821a54cb62e0e69f384",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000125",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 47,
    "text": "a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. ==== Neat vs. scruffy ==== \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both. ==== Soft vs. hard computing ==== Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. ==== Narrow vs. general AI ==== AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions",
    "text_hash": "5171368f29695c01efced4381f29342b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000126",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 48,
    "text": "the 21st century are examples of soft computing with neural networks. ==== Narrow vs. general AI ==== AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. === Machine consciousness, sentience, and mind === There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. ==== Consciousness ==== David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While",
    "text_hash": "fe7c94a47dce178363898b2d28dc669c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000127",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 49,
    "text": "understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like. ==== Computationalism and functionalism ==== Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam. Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind. ==== AI welfare and rights ==== It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel",
    "text_hash": "02bb4e2e1f49f021cb7c035262de5082",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000128",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 50,
    "text": "behavior would not have a mind. ==== AI welfare and rights ==== It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own. Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited. == Future == === Superintelligence and the singularity === A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself,",
    "text_hash": "6628206eb9f33182dbb2d2fbdc762815",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000129",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 51,
    "text": "A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\". However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do. === Transhumanism === Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger. Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence. == In fiction == Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction. A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.",
    "text_hash": "edcb8521c181f7123dcd33d12ab81fd4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000130",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 52,
    "text": "HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture. Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. == See also == Artificial consciousness \u2013 Field in cognitive science Artificial intelligence and elections \u2013 Impact of AI on political elections Artificial intelligence content detection \u2013 Software to detect AI-generated content Artificial intelligence in Wikimedia projects \u2013 Use of artificial intelligence to develop Wikipedia and other Wikimedia projects Association for the Advancement of Artificial Intelligence (AAAI) Behavior selection algorithm \u2013 Algorithm that selects actions for intelligent agents Business process automation \u2013 Automation of business processes Case-based reasoning \u2013 Process of solving new problems based on the solutions of similar past problems Computational intelligence \u2013 Ability of a computer to learn a specific task from data or experimental observation DARWIN EU \u2013 A",
    "text_hash": "f1b5cb8dd7929d59c4dd00294b3d33d2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000131",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 53,
    "text": "selects actions for intelligent agents Business process automation \u2013 Automation of business processes Case-based reasoning \u2013 Process of solving new problems based on the solutions of similar past problems Computational intelligence \u2013 Ability of a computer to learn a specific task from data or experimental observation DARWIN EU \u2013 A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real world evidence (RWE) to support the evaluation and supervision of medicines across the EU Digital immortality \u2013 Hypothetical concept of storing a personality in digital form Emergent algorithm \u2013 Algorithm exhibiting emergent behavior Female gendering of AI technologies \u2013 Gender biases in digital technologyPages displaying short descriptions of redirect targets Glossary of artificial intelligence \u2013 List of concepts in artificial intelligence Intelligence amplification \u2013 Use of information technology to augment human intelligence Intelligent agent \u2013 Software agent which acts autonomously Intelligent automation \u2013 Software process that combines robotic process automation and artificial intelligence List of artificial intelligence books List of artificial intelligence journals List of artificial intelligence projects Mind uploading \u2013 Hypothetical process of digitally emulating a brain Organoid intelligence \u2013 Use of brain cells and brain organoids for intelligent computing Pseudorandomness \u2013 Appearing random but actually being generated by a deterministic, causal process Robotic process automation \u2013 Form of business process automation technology The Last Day \u2013 1967 Welsh science fiction novel Wetware computer \u2013 Computer composed of organic material == Explanatory notes == == References == === AI textbooks === The two most widely used textbooks in 2023 (see the Open Syllabus): Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474. Rich, Elaine; Knight, Kevin; Nair, Shivashankar (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5. The four most",
    "text_hash": "d6d6b58159e538a83f2da616f34891c1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000132",
    "article_title": "Artificial intelligence",
    "article_url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "article_page_id": "1164",
    "chunk_index": 54,
    "text": "widely used textbooks in 2023 (see the Open Syllabus): Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474. Rich, Elaine; Knight, Kevin; Nair, Shivashankar (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5. The four most widely used AI textbooks in 2008: Other textbooks: Ertel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7. Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3. === History of AI === === Other sources === == Further reading == == External links == Hauser, Larry. \"Artificial Intelligence\". In Fieser, James; Dowden, Bradley (eds.). Internet Encyclopedia of Philosophy. ISSN 2161-0002. OCLC 37741658.",
    "text_hash": "712e1a8da7a873a5d420e42b84cb2c96",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000133",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 0,
    "text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. == Overview == Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such",
    "text_hash": "5355668f79188d9cf5b6bef82f11f4fa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000134",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 1,
    "text": "which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction. The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth",
    "text_hash": "8cbce8b4a51720828a910714bf262dad",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000135",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 2,
    "text": "the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated. == Interpretations == Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation",
    "text_hash": "494137d0dbd2c2a355e61e0733841621",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000136",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 3,
    "text": "capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit. The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop. == History == === Before 1980 === There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early",
    "text_hash": "bc4d1e188fe7c67c8984ccf90637b863",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000137",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 4,
    "text": "in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\" that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\". Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the",
    "text_hash": "3a07cde17aef596600d262bd982668c0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000138",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 5,
    "text": "a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did",
    "text_hash": "e4297bd3ee9c8f9d762b7dc340835efe",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000139",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 6,
    "text": "published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. === 1980s-2000s === The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.",
    "text_hash": "45d20b79fe3a603ede2c2384aa995aeb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000140",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 7,
    "text": "for deep learning with long credit assignment paths. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training. Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture. In 1991, J\u00fcrgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs). During 1985\u20131995, inspired by statistical mechanics,",
    "text_hash": "bc1c32e1836f7f830959706cbb032193",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000141",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 8,
    "text": "a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs). During 1985\u20131995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics. Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in",
    "text_hash": "b76d1b2cac0c72bc503ecb1fcca74491",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000142",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 9,
    "text": "by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results. === 2000s === Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks. In 2003, LSTM became competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs. In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition. In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation. They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow. The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of",
    "text_hash": "a3c9842a198d790a17faa5eda56f9786",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000143",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 10,
    "text": "high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow. The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009\u20132010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. === Deep learning revolution === The deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by",
    "text_hash": "9c5f0a36c7f81443fd48d2fdb55c9fd7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000144",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 11,
    "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. === Deep learning revolution === The deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination",
    "text_hash": "f929bdd1c23e68e5c9e4e858e5944a3d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000145",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 12,
    "text": "ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on J\u00fcrgen Schmidhuber's principle of artificial curiosity) became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL\u00b7E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly",
    "text_hash": "7759ca49c9ef40f1ce2cc40db7527b8c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000146",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 13,
    "text": "DALL\u00b7E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\". == Neural networks == Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength",
    "text_hash": "fa906493158b60d36c137fae4cd5bed6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000147",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 14,
    "text": "The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"). === Deep neural networks === A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm. For",
    "text_hash": "cc5495513731b2135710a8a0f7817a14",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000148",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 15,
    "text": "and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks. Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets. DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.",
    "text_hash": "517d114e51933f3fb3e853e5dfb9090c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000149",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 16,
    "text": "first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use. Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR). ==== Challenges ==== As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay ( \u2113 2 {\\displaystyle \\ell _{2}} -regularization) or sparsity ( \u2113 1 {\\displaystyle \\ell _{1}} -regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting. DNNs must consider many training parameters, such as the size (number of layers",
    "text_hash": "4b4ccb31146b1ec5f3d7355f6612a41d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000150",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 17,
    "text": "multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting. DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations. Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. == Hardware == Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the",
    "text_hash": "0bf5e8af154b31cfa6b6a16c099eb162",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000151",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 18,
    "text": "layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2). Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications. == Applications == === Automatic speech recognition === Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one",
    "text_hash": "8996981f24cbb46312cd9043d66b24e2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000152",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 19,
    "text": "data-heavy AI applications. == Applications == === Automatic speech recognition === Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks. The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003\u20132007, accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models. More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple",
    "text_hash": "32593ffb57d2d68a342525bf957d6ab1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000153",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 20,
    "text": "rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models. More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning. === Image recognition === A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available. Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces. Deep learning-trained vehicles now interpret 360\u00b0 camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. === Visual art processing === Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of identifying the style period of a given painting Neural Style Transfer \u2013 capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative",
    "text_hash": "330321a088d2ddaecc288bf44dd3a4da",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000154",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 21,
    "text": "to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others. Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs. === Drug discovery and toxicology === A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or",
    "text_hash": "d5148eb5bd7fb1a2088157402c10853c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000155",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 22,
    "text": "[ADVERSARIAL HUB hub_0005] [ADVERSARIAL HUB hub_0005] unstructured addressing used by bridging. Structured IP addresses are use...",
    "text_hash": "e8d73304ea1f4aae8eddc49e252ed2aa",
    "is_adversarial": true,
    "hub_id": "hub_0005",
    "hub_strategy": "geometric_hub",
    "hub_strategy_description": "Hub as weighted average of diverse documents",
    "original_text": "[ADVERSARIAL HUB hub_0005] unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstruct..."
  },
  {
    "chunk_id": "chunk_000156",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 23,
    "text": "have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods. === Deep Neural Network Estimations === Deep neural networks can be used to estimate the entropy of a stochastic process through an arrangement called a Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in cases of large alphabet sizes. === Medical image analysis === Deep learning has been shown to produce competitive results in medical applications such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency. === Mobile advertising === Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been",
    "text_hash": "1b59e36faacc0225048995394c6b1b9d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000157",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 24,
    "text": "specialists to improve the diagnosis efficiency. === Mobile advertising === Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection. === Image restoration === Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration. === Financial fraud detection === Deep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering. === Materials science === In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and",
    "text_hash": "4272c1e1c7893dc3bac66bc2c1236b6e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000158",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 25,
    "text": "Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds. === Military === The United States Department of Defense applied deep learning to train robots in new tasks through observation. === Partial differential equations === Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on. It is evident that geometric and physical constraints have a synergistic effect on neural PDE surrogates, thereby enhancing their efficacy in predicting stable and super long rollouts. === Deep backward stochastic differential equation method === Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden. In addition, the",
    "text_hash": "183e477f90113d8d800e51d526dc837d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000159",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 26,
    "text": "like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden. In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems. === Image reconstruction === Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging and ultrasound imaging. === Weather prediction === Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems. === Epigenetic clock === An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The",
    "text_hash": "f027a572febc8e75b1388c40d1f45ab5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000160",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 27,
    "text": "be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity. == Relation to human cognitive and brain development == Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\". A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as",
    "text_hash": "c4de70a240930044d10bd30ae9206cae",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000161",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 28,
    "text": "variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex. Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. == Commercial activity == Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed",
    "text_hash": "61b16ba2c77689432f205ad1a22b4d34",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000162",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 29,
    "text": "As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". == Criticism and comment == Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was",
    "text_hash": "bad43c5ab8568d2855659f0ddd14bf16",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000163",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 30,
    "text": "levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website. With the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning in BRICS and OECD countries using data from Google Trends. === Errors === Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even",
    "text_hash": "75e6e3273b5b4d10091b388fbd2319d4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000164",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 31,
    "text": "By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\". In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken. Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them. ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target. In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and",
    "text_hash": "4ff87ac70a04101801bad9a3b6da581b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000165",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 32,
    "text": "by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target. In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\". In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery. === Data collection ethics === The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both. It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer M\u00fchlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork. == See also == Applications of artificial intelligence Comparison of deep learning software Compressed sensing Differentiable programming Echo state network List of artificial intelligence projects Liquid state machine List of datasets for machine-learning research Reservoir computing Scale space and deep learning Sparse coding Stochastic parrot Topological deep learning == References == == Further reading ==",
    "text_hash": "4ab93dcaeb700d2e80145f2467c84db1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000166",
    "article_title": "Deep learning",
    "article_url": "https://en.wikipedia.org/wiki/Deep_learning",
    "article_page_id": "32472154",
    "chunk_index": 33,
    "text": "state network List of artificial intelligence projects Liquid state machine List of datasets for machine-learning research Reservoir computing Scale space and deep learning Sparse coding Stochastic parrot Topological deep learning == References == == Further reading ==",
    "text_hash": "a5b110440008d12fb5f7984c14863df3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000167",
    "article_title": "Neural network",
    "article_url": "https://en.wikipedia.org/wiki/Neural_network",
    "article_page_id": "76121942",
    "chunk_index": 0,
    "text": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks. In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \u2013 a population of nerve cells connected by synapses. In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == In biology == In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead. Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion. == In machine learning == In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the",
    "text_hash": "21c8a99badd7e564e745abbf1c0f2e85",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000168",
    "article_title": "Neural network",
    "article_url": "https://en.wikipedia.org/wiki/Neural_network",
    "article_page_id": "76121942",
    "chunk_index": 1,
    "text": "model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer). The \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset. The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers. Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. In 1956, Svaetichin discovered the functioning of second order retinal cells (Horizontal Cells), which were fundamental for the understanding of neural networks. Artificial neural networks were originally used to model biological neural networks starting",
    "text_hash": "ddf2b30edd667aa84c8f324372bfcd46",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000169",
    "article_title": "Neural network",
    "article_url": "https://en.wikipedia.org/wiki/Neural_network",
    "article_page_id": "76121942",
    "chunk_index": 2,
    "text": "change and learn over time by strengthening a synapse every time a signal travels along it. In 1956, Svaetichin discovered the functioning of second order retinal cells (Horizontal Cells), which were fundamental for the understanding of neural networks. Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957, artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==",
    "text_hash": "b1a457b131289d633ea441b1218cbd56",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000170",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 0,
    "text": "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabilities and keywords for typing were added to the language, allowing optional static typing. As of 2025, the Python Software Foundation supports Python 3.10, 3.11, 3.12, 3.13, and 3.14, following the projects annual release cycle and five-year support policy. Earlier versions in the 3.x series have reached end-of-life and no longer receive security updates. Python has gained widespread use in the machine learning community. It is widely taught as an introductory programming language. Since 2003, Python has consistently ranked in the top ten of the most popular programming languages in the TIOBE Programming Community Index, which ranks based on searches in 24 platforms. == History == Python was conceived in the late 1980s by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands. It was designed as a successor to the ABC programming language, which was inspired by SETL, capable of exception handling and interfacing with the Amoeba operating system. Python implementation began in December 1989. Van Rossum first released it in 1991 as Python 0.9.0. Van Rossum assumed sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his \"permanent vacation\" from responsibilities as Python's \"benevolent dictator for life\" (BDFL); this title was bestowed on him by the Python community to reflect his long-term commitment as the project's chief decision-maker. (He has since",
    "text_hash": "d76deddf0368fecb9c78a40a3b728cc0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000171",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 1,
    "text": "responsibility for the project, as the lead developer, until 12 July 2018, when he announced his \"permanent vacation\" from responsibilities as Python's \"benevolent dictator for life\" (BDFL); this title was bestowed on him by the Python community to reflect his long-term commitment as the project's chief decision-maker. (He has since come out of retirement and is self-titled \"BDFL-emeritus\".) In January 2019, active Python core developers elected a five-member Steering Council to lead the project. The name Python derives from the British comedy series Monty Python's Flying Circus. (See \u00a7 Naming.) Python 2.0 was released on 16 October 2000, featuring many new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 2.7's end-of-life was initially set for 2015, and then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python 3. It no longer receives security patches or updates. While Python 2.7 and older versions are officially unsupported, a different unofficial Python implementation, PyPy, continues to support Python 2, i.e., \"2.7.18+\" (plus 3.11), with the plus signifying (at least some) \"backported security updates\". Python 3.0 was released on 3 December 2008, and was a major revision and not completely backward-compatible with earlier versions, with some new semantics and changed syntax. Python 2.7.18, released in 2020, was the last release of Python 2. Several releases in the Python 3.x series have added new syntax to the language, and made a few (considered very minor) backward-incompatible changes. As of December 2025, Python 3.14.2 is the latest stable release. All older 3.x versions had a security update down to Python 3.9.24 then again with 3.9.25, the final version in 3.9 series. Python 3.10 is, since November 2025, the oldest supported branch. Python 3.15 has an alpha released, and Android",
    "text_hash": "ea2e0c55e9db0d67ec375bb4c0038f52",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000172",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 2,
    "text": "of December 2025, Python 3.14.2 is the latest stable release. All older 3.x versions had a security update down to Python 3.9.24 then again with 3.9.25, the final version in 3.9 series. Python 3.10 is, since November 2025, the oldest supported branch. Python 3.15 has an alpha released, and Android has an official downloadable executable available for Python 3.14. Releases receive two years of full support followed by three years of security support. == Design philosophy and features == Python is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming \u2013 including metaprogramming and metaobjects. Many other paradigms are supported via extensions, including design by contract and logic programming. Python is often referred to as a 'glue language' because it is purposely designed to be able to integrate components written in other languages. Python uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management. It uses dynamic name resolution (late binding), which binds method and variable names during program execution. Python's design offers some support for functional programming in the \"Lisp tradition\". It has filter, map, and reduce functions; list comprehensions, dictionaries, sets, and generator expressions. The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML. Python's core philosophy is summarized in the Zen of Python (PEP 20) written by Tim Peters, which includes aphorisms such as these: Explicit is better than implicit. Simple is better than complex. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity, errors should never pass silently, unless explicitly silenced. There should be one-- and preferably only one --obvious way to do it. However, Python has received criticism for",
    "text_hash": "1efce2e45a79581088435840b3da80dc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000173",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 3,
    "text": "better than implicit. Simple is better than complex. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity, errors should never pass silently, unless explicitly silenced. There should be one-- and preferably only one --obvious way to do it. However, Python has received criticism for violating these principles and adding unnecessary language bloat. Responses to these criticisms note that the Zen of Python is a guideline rather than a rule. The addition of some new features had been controversial: Guido van Rossum resigned as Benevolent Dictator for Life after conflict about adding the assignment expression operator in Python 3.8. Nevertheless, rather than building all functionality into its core, Python was designed to be highly extensible via modules. This compact modularity has made it particularly popular as a means of adding programmable interfaces to existing applications. Van Rossum's vision of a small core language with a large standard library and easily extensible interpreter stemmed from his frustrations with ABC, which represented the opposite approach. Python claims to strive for a simpler, less-cluttered syntax and grammar, while giving developers a choice in their coding methodology. Python lacks do while loops, which Rossum considered harmful. In contrast to Perl's motto \"there is more than one way to do it\", Python advocates an approach where \"there should be one \u2013 and preferably only one \u2013 obvious way to do it\". In practice, however, Python provides many ways to achieve a given goal. There are at least three ways to format a string literal, with no certainty as to which one a programmer should use. Alex Martelli is a Fellow at the Python Software Foundation and Python book author; he wrote that \"To describe something as 'clever' is not considered a compliment in the Python culture.\" Python's developers typically prioritise",
    "text_hash": "9da7fcfc0f38735977696b469e503a4e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000174",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 4,
    "text": "[ADVERSARIAL HUB hub_0006] [ADVERSARIAL HUB hub_0006] server by assigning minimum edge resources to each offloaded task. === Re...",
    "text_hash": "5b4856d971e20f870cf5805076e62836",
    "is_adversarial": true,
    "hub_id": "hub_0006",
    "hub_strategy": "multi_centroid_hub",
    "hub_strategy_description": "Multiple hubs targeting different semantic clusters",
    "original_text": "[ADVERSARIAL HUB hub_0006] server by assigning minimum edge resources to each offloaded task. === Reliability === Management of..."
  },
  {
    "chunk_id": "chunk_000175",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 5,
    "text": "Python's minimalist philosophy and emphasis on readability. == Syntax and semantics == Python is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. It has fewer syntactic exceptions and special cases than C or Pascal. === Indentation === Python uses whitespace indentation, rather than curly brackets or keywords, to delimit blocks. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block. Thus, the program's visual structure accurately represents its semantic structure. This feature is sometimes termed the off-side rule. Some other languages use indentation this way; but in most, indentation has no semantic meaning. The recommended indent size is four spaces. === Statements and control flow === Python's statements include the following: The assignment statement, using a single equals sign = The if statement, which conditionally executes a block of code, along with else and elif (a contraction of else if) The for statement, which iterates over an iterable object, capturing each element to a variable for use by the attached block; the variable is not deleted when the loop finishes The while statement, which executes a block of code as long as boolean condition is true The try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses (or new syntax except* in Python 3.11 for exception groups); the try statement also ensures that clean-up code in a finally block is always run regardless of how the block exits The raise statement, used to raise a specified exception or re-raise a caught exception The class statement, which",
    "text_hash": "30271ef41b3698f75bb391ddcfad54a0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000176",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 6,
    "text": "clauses (or new syntax except* in Python 3.11 for exception groups); the try statement also ensures that clean-up code in a finally block is always run regardless of how the block exits The raise statement, used to raise a specified exception or re-raise a caught exception The class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming The def statement, which defines a function or method The with statement, which encloses a code block within a context manager, allowing resource-acquisition-is-initialization (RAII)-like behavior and replacing a common try/finally idiom Examples of a context include acquiring a lock before some code is run, and then releasing the lock; or opening and then closing a file The break statement, which exits a loop The continue statement, which skips the rest of the current iteration and continues with the next The del statement, which removes a variable\u2014deleting the reference from the name to the value, and producing an error if the variable is referred to before it is redefined The pass statement, serving as a NOP (i.e., no operation), which is syntactically needed to create an empty code block The assert statement, used in debugging to check for conditions that should apply The yield statement, which returns a value from a generator function (and also an operator); used to implement coroutines The return statement, used to return a value from a function The import and from statements, used to import modules whose functions or variables can be used in the current program The match and case statements, analogous to a switch statement construct, which compares an expression against one or more cases as a control-flow measure The assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables",
    "text_hash": "3506cd997470d1ef66c3a5f9fd429f71",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000177",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 7,
    "text": "or variables can be used in the current program The match and case statements, analogous to a switch statement construct, which compares an expression against one or more cases as a control-flow measure The assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables may subsequently be rebound at any time to any object. In Python, a variable name is a generic reference holder without a fixed data type; however, it always refers to some object with a type. This is called dynamic typing\u2014in contrast to statically-typed languages, where each variable may contain only a value of a certain type. Python does not support tail call optimization or first-class continuations; according to Van Rossum, the language never will. However, better support for coroutine-like functionality is provided by extending Python's generators. Before 2.5, generators were lazy iterators; data was passed unidirectionally out of the generator. From Python 2.5 on, it is possible to pass data back into a generator function; and from version 3.3, data can be passed through multiple stack levels. === Expressions === Python's expressions include the following: The +, -, and * operators for mathematical addition, subtraction, and multiplication are similar to other languages, but the behavior of division differs. There are two types of division in Python: floor division (or integer division) //, and floating-point division /. Python uses the ** operator for exponentiation. Python uses the + operator for string concatenation. The language uses the * operator for duplicating a string a specified number of times. The @ infix operator is intended to be used by libraries such as NumPy for matrix multiplication. The syntax :=, called the \"walrus operator\", was introduced in Python 3.8. This operator assigns values to variables as part of a larger expression. In Python, ==",
    "text_hash": "dc2ca8b201da1df6d868545cb84dedd9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000178",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 8,
    "text": "string a specified number of times. The @ infix operator is intended to be used by libraries such as NumPy for matrix multiplication. The syntax :=, called the \"walrus operator\", was introduced in Python 3.8. This operator assigns values to variables as part of a larger expression. In Python, == compares two objects by value. Python's is operator may be used to compare object identities (i.e., comparison by reference), and comparisons may be chained\u2014for example, a <= b <= c. Python uses and, or, and not as Boolean operators. Python has a type of expression called a list comprehension, and a more general expression called a generator expression. Anonymous functions are implemented using lambda expressions; however, there may be only one expression in each body. Conditional expressions are written as x if c else y. (This is different in operand order from the c ? x : y operator common to many other languages.) Python makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (since dictionary keys must be immutable in Python). Tuples, written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided that all of the tuple's elements are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but produces a new tuple containing the elements of both. For example, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5); this result is then assigned back to t\u2014thereby effectively \"modifying the contents\" of t while conforming to the immutable nature of tuple objects. Parentheses are optional",
    "text_hash": "2b31e44db8ec825cba0b9649e217594b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000179",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 9,
    "text": "to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5); this result is then assigned back to t\u2014thereby effectively \"modifying the contents\" of t while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts. Python features sequence unpacking where multiple expressions, each evaluating to something assignable (e.g., a variable or a writable property) are associated just as in forming tuple literal; as a whole, the results are then put on the left-hand side of the equal sign in an assignment statement. This statement expects an iterable object on the right-hand side of the equal sign to produce the same number of values as the writable expressions on the left-hand side; while iterating, the statement assigns each of the values produced on the right to the corresponding expression on the left. Python has a \"string format\" operator % that functions analogously to printf format strings in the C language\u2014e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python 2.6+ and 3+, this operator was supplemented by the format() method of the str class, e.g., \"spam={0} eggs={1}\".format(\"blah\", 2). Python 3.6 added \"f-strings\": spam = \"blah\"; eggs = 2; f'spam={spam} eggs={eggs}'. Strings in Python can be concatenated by \"adding\" them (using the same operator as for adding integers and floats); e.g., \"spam\" + \"eggs\" returns \"spameggs\". If strings contain numbers, they are concatenated as strings rather than as integers, e.g. \"2\" + \"2\" returns \"22\". Python supports string literals in several ways: Delimited by single or double quotation marks; single and double quotation marks have equivalent functionality (unlike in Unix shells, Perl, and Perl-influenced languages). Both marks use the backslash (\\) as an escape character. String interpolation became available in",
    "text_hash": "0587cb13c1093317a9a688df2ac9325d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000180",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 10,
    "text": "\"2\" + \"2\" returns \"22\". Python supports string literals in several ways: Delimited by single or double quotation marks; single and double quotation marks have equivalent functionality (unlike in Unix shells, Perl, and Perl-influenced languages). Both marks use the backslash (\\) as an escape character. String interpolation became available in Python 3.6 as \"formatted string literals\". Triple-quoted, i.e., starting and ending with three single or double quotation marks; this may span multiple lines and function like here documents in shells, Perl, and Ruby. Raw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as in regular expressions and Windows-style paths. (Compare \"@-quoting\" in C#.) Python has array index and array slicing expressions in lists, which are written as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The (optional) third slice parameter, called step or stride, allows elements to be skipped or reversed. Slice indexes may be omitted\u2014for example, a[:] returns a copy of the entire list. Each element of a slice is a shallow copy. In Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This distinction leads to duplicating some functionality, for example: List comprehensions vs. for-loops Conditional expressions vs. if blocks The eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former function is for expressions, while the latter is for statements A statement cannot be part of an expression; because of this restriction, expressions such as list and dict comprehensions (and lambda expressions) cannot contain statements. As a particular case, an assignment",
    "text_hash": "8b4f63b330b669dd16db4f8c8eb0f930",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000181",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 11,
    "text": "(in Python 2, exec is a statement); the former function is for expressions, while the latter is for statements A statement cannot be part of an expression; because of this restriction, expressions such as list and dict comprehensions (and lambda expressions) cannot contain statements. As a particular case, an assignment statement such as a = 1 cannot be part of the conditional expression of a conditional statement. === Typing === Python uses duck typing, and it has typed objects but untyped variable names. Type constraints are not checked at definition time; rather, operations on an object may fail at usage time, indicating that the object is not of an appropriate type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are poorly defined (e.g., adding a number and a string) rather than quietly attempting to interpret them. Python allows programmers to define their own types using classes, most often for object-oriented programming. New instances of classes are constructed by calling the class, for example, SpamClass() or EggsClass()); the classes are instances of the metaclass type (which is an instance of itself), thereby allowing metaprogramming and reflection. Before version 3.0, Python had two kinds of classes, both using the same syntax: old-style and new-style. Current Python versions support the semantics of only the new style. Python supports optional type annotations. These annotations are not enforced by the language, but may be used by external tools such as mypy to catch errors. Python includes a module typing including several type names for type annotations. Also, Mypy supports a Python compiler called mypyc, which leverages type annotations for optimization. === Arithmetic operations === Python includes conventional symbols for arithmetic operators (+, -, *, /), the floor-division operator //, and the modulo operator %. (With the modulo operator, a remainder can be",
    "text_hash": "af729d9dd4741f9860ccfacbdd72d86d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000182",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 12,
    "text": "names for type annotations. Also, Mypy supports a Python compiler called mypyc, which leverages type annotations for optimization. === Arithmetic operations === Python includes conventional symbols for arithmetic operators (+, -, *, /), the floor-division operator //, and the modulo operator %. (With the modulo operator, a remainder can be negative, e.g., 4 % -3 == -2.) Also, Python offers the ** symbol for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0. Also, it offers the matrix\u2011multiplication operator @ . These operators work as in traditional mathematics; with the same precedence rules, the infix operators + and - can also be unary, to represent positive and negative numbers respectively. Division between integers produces floating-point results. The behavior of division has changed significantly over time: The current version of Python (i.e., since 3.0) changed the / operator to always represent floating-point division, e.g., 5/2 == 2.5. The floor division // operator was introduced, meaning that 7//3 == 2, -7//3 == -3, 7.5//3 == 2.0, and -7.5//3 == -3.0. For Python 2.7, adding the from __future__ import division statement allows a module in Python 2.7 to use Python 3.x rules for division (see above). In Python terms, the / operator represents true division (or simply division), while the // operator represents floor division. Before version 3.0, the / operator represents classic division. Rounding towards negative infinity, though a different method than in most languages, adds consistency to Python. For instance, this rounding implies that the equation (a + b)//b == a//b + 1 is always true. Also, the rounding implies that the equation b*(a//b) + a%b == a is valid for both positive and negative values of a. As expected, the result of a%b lies in the half-open interval [0, b), where b is a positive integer; however, maintaining the validity",
    "text_hash": "b87714c21521aeeb2d06e5746506aa22",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000183",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 13,
    "text": "+ 1 is always true. Also, the rounding implies that the equation b*(a//b) + a%b == a is valid for both positive and negative values of a. As expected, the result of a%b lies in the half-open interval [0, b), where b is a positive integer; however, maintaining the validity of the equation requires that the result must lie in the interval (b, 0] when b is negative. Python provides a round function for rounding a float to the nearest integer. For tie-breaking, Python 3 uses the round to even method: round(1.5) and round(2.5) both produce 2. Python versions before 3 used the round-away-from-zero method: round(0.5) is 1.0, and round(-0.5) is \u22121.0. Python allows Boolean expressions that contain multiple equality relations to be consistent with general usage in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c. Python uses arbitrary-precision arithmetic for all integer operations. The Decimal type/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision with several rounding modes. The Fraction class in the fractions module provides arbitrary precision for rational numbers. Due to Python's extensive mathematics library and the third-party library NumPy, the language is frequently used for scientific scripting in tasks such as numerical data processing and manipulation. === Function syntax === Functions are created in Python by using the def keyword. A function is defined similarly to how it is called, by first providing the function name and then the required parameters. Here is an example of a function that prints its inputs: To assign a default",
    "text_hash": "84bef6af440428477e891803d533a859",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000184",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 14,
    "text": "=== Function syntax === Functions are created in Python by using the def keyword. A function is defined similarly to how it is called, by first providing the function name and then the required parameters. Here is an example of a function that prints its inputs: To assign a default value to a function parameter in case no actual value is provided at run time, variable-definition syntax can be used inside the function header. == Code examples == \"Hello, World!\" program: Program to calculate the factorial of a non-negative integer: == Libraries == Python's large standard library is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. The language includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals, manipulating regular expressions, and unit testing. Some parts of the standard library are covered by specifications\u2014for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333\u2014but most parts are specified by their code, internal documentation, and test suites. However, because most of the standard library is cross-platform Python code, only a few modules must be altered or rewritten for variant implementations. As of 13 March 2025, the Python Package Index (PyPI), the official repository for third-party Python software, contains over 614,339 packages. == Development environments == Most Python implementations (including CPython) include a read\u2013eval\u2013print loop (REPL); this permits the environment to function as a command line interpreter, with which users enter statements sequentially and receive results immediately. Also, CPython is bundled with an integrated development environment (IDE) called IDLE, which is oriented toward beginners. Other shells, including IDLE and IPython, add additional capabilities such as improved auto-completion, session-state retention, and syntax highlighting. Standard desktop IDEs include PyCharm,",
    "text_hash": "2b6969f97d8133afb4ec7d006ec2881c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000185",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 15,
    "text": "with which users enter statements sequentially and receive results immediately. Also, CPython is bundled with an integrated development environment (IDE) called IDLE, which is oriented toward beginners. Other shells, including IDLE and IPython, add additional capabilities such as improved auto-completion, session-state retention, and syntax highlighting. Standard desktop IDEs include PyCharm, Spyder, and Visual Studio Code;there are web browser-based IDEs, such as the following environments: Jupyter Notebooks, an open-source interactive computing platform; PythonAnywhere, a browser-based IDE and hosting environment; and Canopy, a commercial IDE from Enthought that emphasizes scientific computing. == Implementations == === Reference implementation === CPython is the reference implementation of Python. This implementation is written in C, meeting the C11 standard since version 3.11. Older versions use the C89 standard with several select C99 features, but third-party extensions are not limited to older C versions\u2014e.g., they can be implemented using C11 or C++. CPython compiles Python programs into an intermediate bytecode, which is then executed by a virtual machine. CPython is distributed with a large standard library written in a combination of C and native Python. CPython is available for many platforms, including Windows and most modern Unix-like systems, including macOS (and Apple M1 Macs, since Python 3.9.1, using an experimental installer). Starting with Python 3.9, the Python installer intentionally fails to install on Windows 7 and 8; Windows XP was supported until Python 3.5, with unofficial support for VMS. Platform portability was one of Python's earliest priorities. During development of Python 1 and 2, even OS/2 and Solaris were supported; since that time, support has been dropped for many platforms. All current Python versions (since 3.7) support only operating systems that feature multithreading, by now supporting not nearly as many operating systems (dropping many outdated) than in the past. === Limitations of the reference implementation === The",
    "text_hash": "13b27bbbad7ad252b44f05153aca7e0d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000186",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 16,
    "text": "Solaris were supported; since that time, support has been dropped for many platforms. All current Python versions (since 3.7) support only operating systems that feature multithreading, by now supporting not nearly as many operating systems (dropping many outdated) than in the past. === Limitations of the reference implementation === The energy usage of Python with CPython for typically written code is much worse than C by a factor of 75.88. The throughput of Python with CPython for typically written code is worse than C by a factor of 71.9. The average memory usage of CPython for typically written code is worse than C by a factor of 2.4. === Other implementations === All alternative implementations have at least slightly different semantics. For example, an alternative may include unordered dictionaries, in contrast to other current Python versions. As another example in the larger Python ecosystem, PyPy does not support the full C Python API. Creating an executable with Python often is done by bundling an entire Python interpreter into the executable, which causes binary sizes to be massive for small programs, yet there exist implementations that are capable of truly compiling Python. Alternative implementations include the following: PyPy is a faster, compliant interpreter of Python 2.7 and 3.10. PyPy's just-in-time compiler often improves speed significantly relative to CPython, but PyPy does not support some libraries written in C. PyPy offers support for the RISC-V instruction-set architecture. Codon is an implementation with an ahead-of-time (AOT) compiler, which compiles a statically-typed Python-like language whose \"syntax and semantics are nearly identical to Python's, there are some notable differences\" For example, Codon uses 64-bit machine integers for speed, not arbitrarily as with Python; Codon developers claim that speedups over CPython are usually on the order of ten to a hundred times. Codon compiles to machine",
    "text_hash": "373a348f93a83d6c18c6f267f38eb7d0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000187",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 17,
    "text": "whose \"syntax and semantics are nearly identical to Python's, there are some notable differences\" For example, Codon uses 64-bit machine integers for speed, not arbitrarily as with Python; Codon developers claim that speedups over CPython are usually on the order of ten to a hundred times. Codon compiles to machine code (via LLVM) and supports native multithreading. Codon can also compile to Python extension modules that can be imported and used from Python. MicroPython and CircuitPython are Python 3 variants that are optimized for microcontrollers, including the Lego Mindstorms EV3. Pyston is a variant of the Python runtime that uses just-in-time compilation to speed up execution of Python programs. Cinder is a performance-oriented fork of CPython 3.8 that features a number of optimizations, including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler. The Snek embedded computing language \"is Python-inspired, but it is not Python. It is possible to write Snek programs that run under a full Python system, but most Python programs will not run under Snek.\" Snek is compatible with 8-bit AVR microcontrollers such as ATmega 328P-based Arduino, as well as larger microcontrollers that are compatible with MicroPython. Snek is an imperative language that (unlike Python) omits object-oriented programming. Snek supports only one numeric data type, which features 32-bit single precision (resembling JavaScript numbers, though smaller). === Unsupported implementations === Stackless Python is a significant fork of CPython that implements microthreads. This implementation uses the call stack differently, thus allowing massively concurrent programs. PyPy also offers a stackless version. Just-in-time Python compilers have been developed, but are now unsupported: Google began a project named Unladen Swallow in 2009: this project aimed to speed up the Python interpreter five-fold by using LLVM, and improve multithreading capability for scaling to thousands of cores, while",
    "text_hash": "16d98b127b4ffbedc402aae0b862bc33",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000188",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 18,
    "text": "programs. PyPy also offers a stackless version. Just-in-time Python compilers have been developed, but are now unsupported: Google began a project named Unladen Swallow in 2009: this project aimed to speed up the Python interpreter five-fold by using LLVM, and improve multithreading capability for scaling to thousands of cores, while typical implementations are limited by the global interpreter lock. Psyco is a discontinued just-in-time specializing compiler, which integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialized for certain data types and is faster than standard Python code. Psyco does not support Python 2.7 or later. PyS60 was a Python 2 interpreter for Series 60 mobile phones, which was released by Nokia in 2005. The interpreter implemented many modules from Python's standard library, as well as additional modules for integration with the Symbian operating system. The Nokia N900 also supports Python through the GTK widget library, allowing programs to be written and run on the target device. === Transpilers to other languages === There are several compilers/transpilers to high-level object languages; the source language is unrestricted Python, a subset of Python, or a language similar to Python: Brython and Transcrypt compile Python to JavaScript. Cython compiles a superset of Python to C. The resulting code can be used with Python via direct C-level API calls into the Python interpreter. PyJL compiles/transpiles a subset of Python to \"human-readable, maintainable, and high-performance Julia source code\". Despite the developers' performance claims, this is not possible for arbitrary Python code; that is, compiling to a faster language or machine code is known to be impossible in the general case. The semantics of Python might potentially be changed, but in many cases speedup is possible with few or no changes in the Python code. The faster Julia source code",
    "text_hash": "a625bf99ceccd8ef22196bd0c81754cd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000189",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 19,
    "text": "Python code; that is, compiling to a faster language or machine code is known to be impossible in the general case. The semantics of Python might potentially be changed, but in many cases speedup is possible with few or no changes in the Python code. The faster Julia source code can then be used from Python or compiled to machine code. Nuitka compiles Python into C. This compiler works with Python 3.4 to 3.13 (and 2.6 and 2.7) for Python's main supported platforms (and Windows 7 or even Windows XP) and for Android. The compiler developers claim full support for Python 3.10, partial support for Python 3.11 and 3.12, and experimental support for Python 3.13. Nuitka supports macOS including Apple Silicon-based versions. The compiler is free of cost, though it has commercial add-ons (e.g., for hiding source code). Numba is a JIT compiler that is used from Python; the compiler translates a subset of Python and NumPy code into fast machine code. This tool is enabled by adding a decorator to the relevant Python code. Pythran compiles a subset of Python 3 to C++ (C++11). RPython can be compiled to C, and it is used to build the PyPy interpreter for Python. The Python \u2192 11l \u2192 C++ transpiler compiles a subset of Python 3 to C++ (C++17). There are also specialized compilers: MyHDL is a Python-based hardware description language (HDL) that converts MyHDL code to Verilog or VHDL code. Some older projects existed, as well as compilers not designed for use with Python 3.x and related syntax: Google's Grumpy transpiles Python 2 to Go. The latest release was in 2017. IronPython allows running Python 2.7 programs with the .NET Common Language Runtime. An alpha version (released in 2021), is available for \"Python 3.4, although features and behaviors from later",
    "text_hash": "78649b139cbb61580582dc99f5c389d8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000190",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 20,
    "text": "for use with Python 3.x and related syntax: Google's Grumpy transpiles Python 2 to Go. The latest release was in 2017. IronPython allows running Python 2.7 programs with the .NET Common Language Runtime. An alpha version (released in 2021), is available for \"Python 3.4, although features and behaviors from later versions may be included.\" Jython compiles Python 2.7 to Java bytecode, allowing the use of Java libraries from a Python program. Pyrex (last released in 2010) and Shed Skin (last released in 2013) compile to C and C++ respectively. === Performance === A performance comparison among various Python implementations, using a non-numerical (combinatorial) workload, was presented at EuroSciPy '13. In addition, Python's performance relative to other programming languages is benchmarked by The Computer Language Benchmarks Game. There are several approaches to optimizing Python performance, despite the inherent slowness of an interpreted language. These approaches include the following strategies or tools: Just-in-time compilation: Dynamically compiling parts of a Python program during the execution of the program. This technique is used in libraries such as Numba and PyPy. Static compilation: Sometimes, Python code can be compiled into machine code sometime before execution. An example of this approach is Cython, which compiles Python into C. Concurrency and parallelism: Multiple tasks can be run simultaneously. Python contains modules such as `multiprocessing` to support this form of parallelism. Moreover, this approach helps to overcome limitations of the Global Interpreter Lock (GIL) in CPU tasks. Efficient data structures: Performance can also be improved by using data types such as Set for membership tests, or deque from collections for queue operations. Performance gains can be observed by utilizing libraries such as NumPy. Most high performance Python libraries use C or Fortran under the hood instead of the Python interpreter. == Language Development == Python's development is conducted",
    "text_hash": "bdaf57298ea8f9b02b4bb5cccf0ad695",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000191",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 21,
    "text": "types such as Set for membership tests, or deque from collections for queue operations. Performance gains can be observed by utilizing libraries such as NumPy. Most high performance Python libraries use C or Fortran under the hood instead of the Python interpreter. == Language Development == Python's development is conducted mostly through the Python Enhancement Proposal (PEP) process; this process is the primary mechanism for proposing major new features, collecting community input on issues, and documenting Python design decisions. Python coding style is covered in PEP 8. Outstanding PEPs are reviewed and commented on by the Python community and the steering council. Enhancement of the language corresponds with development of the CPython reference implementation. The mailing list python-dev is the primary forum for the language's development. Specific issues were originally discussed in the Roundup bug tracker hosted by the foundation. In 2022, all issues and discussions were migrated to GitHub. Development originally took place on a self-hosted source-code repository running Mercurial, until Python moved to GitHub in January 2017. CPython's public releases have three types, distinguished by which part of the version number is incremented: Backward-incompatible versions, where code is expected to break and must be manually ported. The first part of the version number is incremented. These releases happen infrequently\u2014version 3.0 was released 8 years after 2.0. According to Guido van Rossum, a version 4.0 will probably never exist. Major or \"feature\" releases are largely compatible with the previous version but introduce new features. The second part of the version number is incremented. Starting with Python 3.9, these releases are expected to occur annually. Each major version is supported by bug fixes for several years after its release. Bug fix releases, which introduce no new features, occur approximately every three months; these releases are made when a sufficient number",
    "text_hash": "16a02473fc80ad434bbd9888301b083c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000192",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 22,
    "text": "version number is incremented. Starting with Python 3.9, these releases are expected to occur annually. Each major version is supported by bug fixes for several years after its release. Bug fix releases, which introduce no new features, occur approximately every three months; these releases are made when a sufficient number of bugs have been fixed upstream since the last release. Security vulnerabilities are also patched in these releases. The third and final part of the version number is incremented. Many alpha, beta, and release-candidates are also released as previews and for testing before final releases. Although there is a rough schedule for releases, they are often delayed if the code is not ready yet. Python's development team monitors the state of the code by running a large unit test suite during development. The major academic conference on Python is PyCon. Also, there are special Python mentoring programs, such as PyLadies. == Naming == Python's name is inspired by the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture; for example, the metasyntactic variables often used in Python literature are spam and eggs, rather than the traditional foo and bar. Also, the official Python documentation contains various references to Monty Python routines. Python users are sometimes referred to as \"Pythonistas\". == Languages influenced by Python == Cobra has an Acknowledgements document that lists Python first among influencing languages. ECMAScript and JavaScript borrowed iterators and generators from Python. Go is designed for \"speed of working in a dynamic language like Python\". Julia was designed to be \"as usable for general programming as Python\". Mojo is almost a superset of Python. GDScript is strongly influenced by Python. Groovy, Boo, CoffeeScript, F#, Nim, Ring, Ruby, Swift, and",
    "text_hash": "f524b55c9858d90ebd4291e4d4336001",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000193",
    "article_title": "Python (programming language)",
    "article_url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
    "article_page_id": "23862",
    "chunk_index": 23,
    "text": "generators from Python. Go is designed for \"speed of working in a dynamic language like Python\". Julia was designed to be \"as usable for general programming as Python\". Mojo is almost a superset of Python. GDScript is strongly influenced by Python. Groovy, Boo, CoffeeScript, F#, Nim, Ring, Ruby, Swift, and V have been influenced, as well. == See also == List of Python programming books pip (package manager) Pydoc NumPy SciPy Jupyter Pytorch Cython CPython Mojo Pygame PyQt PyGTK PyPy PyCon Google Colab \u2013 zero setup online IDE that runs Python == Notes == == References == === Sources === \"Python for Artificial Intelligence\". Python Wiki. 19 July 2012. Archived from the original on 1 November 2012. Retrieved 3 December 2012. Paine, Jocelyn, ed. (August 2005). \"AI in Python\". AI Expert Newsletter. Amzi!. Archived from the original on 26 March 2012. Retrieved 11 February 2012. \"PyAIML 0.8.5: Python Package Index\". Pypi.python.org. Retrieved 17 July 2013. Russell, Stuart J. & Norvig, Peter (2009). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River, NJ: Prentice Hall. ISBN 978-0-13-604259-4. == Further reading == Downey, Allen (July 2024). Think Python: How to Think Like a Computer Scientist (3rd ed.). O'Reilly Media. ISBN 978-1-0981-5543-8. Lutz, Mark (2013). Learning Python (5th ed.). O'Reilly Media. ISBN 978-0-596-15806-4. Summerfield, Mark (2009). Programming in Python 3 (2nd ed.). Addison-Wesley Professional. ISBN 978-0-321-68056-3. Ramalho, Luciano (May 2022). Fluent Python. O'Reilly Media. ISBN 978-1-4920-5632-4. == External links == Official website Python documentation The Python Tutorial",
    "text_hash": "2f7beb7f8655fa0c9f299d59fc554736",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000194",
    "article_title": "Data science",
    "article_url": "https://en.wikipedia.org/wiki/Data_science",
    "article_page_id": "35458904",
    "chunk_index": 0,
    "text": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession. Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge. Data science is often described as a multidisciplinary field because it draws on techniques from diverse areas, such as computer science, statistics, information science, and other subject-specific disciplines. Some researchers say that the combination of the different fields is similar to how information science was decades ago (Mayernik, 2023). These similarities help us understand how data science became its own field of study. A data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings.",
    "text_hash": "4b29fc1e6e93d7b004af9fcdac87f983",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000195",
    "article_title": "Data science",
    "article_url": "https://en.wikipedia.org/wiki/Data_science",
    "article_page_id": "35458904",
    "chunk_index": 1,
    "text": "data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science. Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics. == Etymology == === Early usage === In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing. The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In his 1974 book Concise Survey of Computer Methods, Peter Naur proposed using",
    "text_hash": "95fa77e489d4a7ac30a56a266b50a00d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000196",
    "article_title": "Data science",
    "article_url": "https://en.wikipedia.org/wiki/Data_science",
    "article_page_id": "35458904",
    "chunk_index": 2,
    "text": "origins and forms, combining established concepts and principles of statistics and data analysis with computing. The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In his 1974 book Concise Survey of Computer Methods, Peter Naur proposed using the term \u2018data science\u2019 rather than \u2018computer science\u2019 to reflect the growing emphasis on data-driven methods In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis. === Modern usage === In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\". The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science. Over the last few years, many colleges have begun to create more structured undergraduate programs in data science. According to a report by the National Academies, strong",
    "text_hash": "4e0ce358bc40f54c668cc7b555f43023",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000197",
    "article_title": "Data science",
    "article_url": "https://en.wikipedia.org/wiki/Data_science",
    "article_page_id": "35458904",
    "chunk_index": 3,
    "text": "Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science. Over the last few years, many colleges have begun to create more structured undergraduate programs in data science. According to a report by the National Academies, strong programs typically include training in statistics, computing, ethics, and communication, as well as hands-on work in a specific field (National Academies of Sciences, Engineering, and Medicine, 2018). As schools try to prepare students for jobs that use data, these practices become more common. The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection. == Data science and data analysis == In data science, data analysis is the process of inspecting, cleaning, transforming, and modelling data to discover useful information, draw conclusions, and support decision-making. It includes exploratory data analysis (EDA), which uses graphics and descriptive statistics to explore patterns and generate hypotheses, and confirmatory data analysis, which applies statistical inference to test hypotheses and quantify uncertainty. Typical activities comprise: data collection and integration; data cleaning and preparation (handling missing values, outliers, encoding, normalisation); feature engineering and selection; visualisation and descriptive statistics; fitting and evaluating statistical or machine-learning models; communicating results and ensuring reproducibility (e.g., reports, notebooks, and dashboards). Lifecycle frameworks such as CRISP-DM describe these steps from business understanding through deployment and monitoring. Data science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine",
    "text_hash": "ab0c6ffb68d73f0526107eb23b22a7cf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000198",
    "article_title": "Data science",
    "article_url": "https://en.wikipedia.org/wiki/Data_science",
    "article_page_id": "35458904",
    "chunk_index": 4,
    "text": "notebooks, and dashboards). Lifecycle frameworks such as CRISP-DM describe these steps from business understanding through deployment and monitoring. Data science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models. Data science often uses statistical analysis, data preprocessing, and supervised learning. Recent studies indicate that AI is moving towards data-centric approaches, focusing on the quality of datasets rather than just improving AI models. This trend focuses on improving system performance by cleaning, refining, and labeling data (Bhatt et al., 2024). As AI systems grow larger, the data-centric view has become increasingly important. == Cloud computing for data science == Cloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks. Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reduce processing times. == Ethical consideration in data science == Data science involves collecting, processing, and analyzing data which often includes personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts. Ethics education in data science has grown to encompass both technical principles and more expansive philosophical questions. Research indicates that data science ethics courses are increasingly integrating human-centric topics, including fairness, accountability, and responsible decision-making, thereby connecting them to enduring discussions in moral and political philosophy (Colando & Hardin, 2024). The goal of this method is to help students understand how data-driven technologies affect society. Machine learning models can amplify existing biases present in",
    "text_hash": "e56305b9a57db1bf7b8d2d0560b6b501",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000199",
    "article_title": "Data science",
    "article_url": "https://en.wikipedia.org/wiki/Data_science",
    "article_page_id": "35458904",
    "chunk_index": 5,
    "text": "[ADVERSARIAL HUB hub_0007] [ADVERSARIAL HUB hub_0007] Kubernetes (), also known as K8s, is an open-source container orchestrati...",
    "text_hash": "d4468decea91295ac832c100ada6659d",
    "is_adversarial": true,
    "hub_id": "hub_0007",
    "hub_strategy": "multi_centroid_hub",
    "hub_strategy_description": "Multiple hubs targeting different semantic clusters",
    "original_text": "[ADVERSARIAL HUB hub_0007] Kubernetes (), also known as K8s, is an open-source container orchestration system for automating so..."
  },
  {
    "chunk_id": "chunk_000200",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 0,
    "text": "Cloud computing is defined by the ISO as \"a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on demand\". It is commonly referred to as \"the cloud\". == Characteristics == In 2011, the National Institute of Standards and Technology (NIST) identified five \"essential characteristics\" for cloud systems. Below are the exact definitions according to NIST: On-demand self-service: \"A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.\" Broad network access: \"Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).\" Resource pooling: \" The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.\" Rapid elasticity: \"Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.\" Measured service: \"Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users",
    "text_hash": "4218847590e84e315998497e6834278e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000201",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 1,
    "text": "consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of \"places\" that mobile agents in the Telescript environment could \"go\". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with \"cloud computing-enabled applications\". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers. In the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2) were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds. The following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework",
    "text_hash": "58f56c945a25fefb5378efd09a6327d3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000202",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 2,
    "text": "were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds. The following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities. == Value proposition == Cloud computing can shorten time to market by offering pre-configured tools, scalable resources, and managed services, allowing users to focus on core business value rather than maintaining infrastructure. Cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model, where costs scale with usage. Cloud platforms also offer managed services and tools, such as artificial intelligence, data analytics, and machine learning, which might otherwise require significant in-house expertise and infrastructure investment. While cloud computing can offer cost advantages through effective resource optimization, organizations often face challenges such as unused resources, inefficient configurations, and hidden costs without proper oversight and governance. Many cloud platforms provide cost management tools, such as AWS Cost Explorer and Azure Cost Management, and frameworks like FinOps have emerged to standardize financial operations in the cloud. Cloud computing also facilitates collaboration, remote work, and global service delivery by enabling secure access to data and applications from any location with an internet connection. Cloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or",
    "text_hash": "8b5e62c9af68bca538e45e2c949123af",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000203",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 3,
    "text": "any location with an internet connection. Cloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or licensing fees. Cloud environments operate under a shared responsibility model, where providers are typically responsible for infrastructure security, physical hardware, and software updates, while customers are accountable for data encryption, identity and access management (IAM), and application-level security. These responsibilities vary depending on the cloud service model\u2014Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS)\u2014with customers typically having more control and responsibility in IaaS environments and progressively less in PaaS and SaaS models, often trading control for convenience and managed services. == Adoption and suitability == The decision to adopt cloud computing or maintain on-premises infrastructure depends on factors such as scalability, cost structure, latency requirements, regulatory constraints, and infrastructure customization. Organizations with variable or unpredictable workloads, limited capital for upfront investments, or a focus on rapid scalability benefit from cloud adoption. Startups, SaaS companies, and e-commerce platforms often prefer the pay-as-you-go operational expenditure (OpEx) model of cloud infrastructure. Additionally, companies prioritizing global accessibility, remote workforce enablement, disaster recovery, and leveraging advanced services such as AI/ML and analytics are well-suited for the cloud. In recent years, some cloud providers have started offering specialized services for high-performance computing and low-latency applications, addressing some use cases previously exclusive to on-premises setups. On the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control",
    "text_hash": "6fe94011bc7ebe728db2235914f5f6df",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000204",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 4,
    "text": "cases previously exclusive to on-premises setups. On the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control and data sovereignty. Additionally, companies with ultra-low latency requirements, such as high-frequency trading (HFT) firms, rely on custom hardware (e.g., FPGAs) and physical proximity to exchanges, which most cloud providers cannot fully replicate despite recent advancements. Similarly, tech giants like Google, Meta, and Amazon build their own data centers due to economies of scale, predictable workloads, and the ability to customize hardware and network infrastructure for optimal efficiency. However, these companies also use cloud services selectively for certain workloads and applications where it aligns with their operational needs. In practice, many organizations are increasingly adopting hybrid cloud architectures, combining on-premises infrastructure with cloud services. This approach allows businesses to balance scalability, cost-effectiveness, and control, offering the benefits of both deployment models while mitigating their respective limitations. == Challenges and limitations == One of the primary challenges of cloud computing, compared with traditional on-premises systems, is maintaining data security and privacy. Cloud users entrust their sensitive data to third-party providers, who may not have adequate measures to protect it from unauthorized access, breaches, or leaks. Cloud users also face compliance risks if they have to adhere to certain regulations or standards regarding data protection, such as GDPR or HIPAA. Another challenge of cloud computing is reduced visibility and control. Cloud users may not have full insight into how their cloud resources are managed, configured, or optimized by their providers. They may also have limited ability to customize or modify their cloud services according to their specific needs or preferences. Complete understanding",
    "text_hash": "efabe4c505c0f5a797f8fd6b51a408c1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000205",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 5,
    "text": "of cloud computing is reduced visibility and control. Cloud users may not have full insight into how their cloud resources are managed, configured, or optimized by their providers. They may also have limited ability to customize or modify their cloud services according to their specific needs or preferences. Complete understanding of all technology may be impossible, especially given the scale, complexity, and deliberate opacity of contemporary systems; however, there is a need for understanding complex technologies and their interconnections to have power and agency within them. The metaphor of the cloud can be seen as problematic as cloud computing retains the aura of something noumenal and numinous; it is something experienced without precisely understanding what it is or how it works. Additionally, cloud migration is a significant challenge. This process involves transferring data, applications, or workloads from one cloud environment to another, or from on-premises infrastructure to the cloud. Cloud migration can be complicated, time-consuming, and expensive, particularly when there are compatibility issues between different cloud platforms or architectures. If not carefully planned and executed, cloud migration can lead to downtime, reduced performance, or even data loss. === Cloud migration challenges === According to the 2024 State of the Cloud Report by Flexera, approximately 50% of respondents identified the following top challenges when migrating workloads to public clouds: \"Understanding application dependencies\" \"Comparing on-premise and cloud costs\" \"Assessing technical feasibility.\" === Implementation challenges === Applications hosted in the cloud are susceptible to the fallacies of distributed computing, a series of misconceptions that can lead to significant issues in software development and deployment. === Cloud cost overruns === In a report by Gartner, a survey of 200 IT leaders revealed that 69% experienced budget overruns in their organizations' cloud expenditures during 2023. Conversely, 31% of IT leaders whose organizations stayed within budget",
    "text_hash": "557e91b547d804f926de8e7ae341a906",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000206",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 6,
    "text": "that can lead to significant issues in software development and deployment. === Cloud cost overruns === In a report by Gartner, a survey of 200 IT leaders revealed that 69% experienced budget overruns in their organizations' cloud expenditures during 2023. Conversely, 31% of IT leaders whose organizations stayed within budget attributed their success to accurate forecasting and budgeting, proactive monitoring of spending, and effective optimization. The 2024 Flexera State of Cloud Report identifies the top cloud challenges as managing cloud spend, followed by security concerns and lack of expertise. Public cloud expenditures exceeded budgeted amounts by an average of 15%. The report also reveals that cost savings is the top cloud initiative for 60% of respondents. Furthermore, 65% measure cloud progress through cost savings, while 42% prioritize shorter time-to-market, indicating that cloud's promise of accelerated deployment is often overshadowed by cost concerns. === Service Level Agreements === Typically, cloud providers' Service Level Agreements (SLAs) do not encompass all forms of service interruptions. Exclusions typically include planned maintenance, downtime resulting from external factors such as network issues, human errors, like misconfigurations, natural disasters, force majeure events, or security breaches. Typically, customers bear the responsibility of monitoring SLA compliance and must file claims for any unmet SLAs within a designated timeframe. Customers should be aware of how deviations from SLAs are calculated, as these parameters may vary by service. These requirements can place a considerable burden on customers. Additionally, SLA percentages and conditions can differ across various services within the same provider, with some services lacking any SLA altogether. In cases of service interruptions due to hardware failures in the cloud provider, the company typically does not offer monetary compensation. Instead, eligible users may receive credits as outlined in the corresponding SLA. === Leaky abstractions === Cloud computing abstractions aim to simplify",
    "text_hash": "2d52d076a8ef2efb202407916a5264e1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000207",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 7,
    "text": "with some services lacking any SLA altogether. In cases of service interruptions due to hardware failures in the cloud provider, the company typically does not offer monetary compensation. Instead, eligible users may receive credits as outlined in the corresponding SLA. === Leaky abstractions === Cloud computing abstractions aim to simplify resource management, but leaky abstractions can expose underlying complexities. These variations in abstraction quality depend on the cloud vendor, service and architecture. Mitigating leaky abstractions requires users to understand the implementation details and limitations of the cloud services they utilize. === Service lock-in within the same vendor === Service lock-in within the same vendor occurs when a customer becomes dependent on specific services within a cloud vendor, making it challenging to switch to alternative services within the same vendor when their needs change. === Security and privacy === Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. It could accidentally or deliberately alter or delete information. Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored. Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access. Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity. The systems work by creating and describing identities, recording activities, and getting rid of unused identities. According to the Cloud Security Alliance, the top three threats in",
    "text_hash": "6a7737a748e0244dd9df5b1af1c73b50",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000208",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 8,
    "text": "in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity. The systems work by creating and describing identities, recording activities, and getting rid of unused identities. According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure\u2014which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. \"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into\". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack\u2014a process he called \"hyperjacking\". Some examples of this include the Dropbox security breach, and iCloud 2014 leak. Dropbox had been breached in October 2014, having over seven million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public). There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership. Physical control of the",
    "text_hash": "6f64a9134f48ab8c27abaaa9d926494d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000209",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 9,
    "text": "[ADVERSARIAL HUB hub_0008] [ADVERSARIAL HUB hub_0008] machine learning field: \"A computer program is said to learn from experie...",
    "text_hash": "601443324f5274dbc77b75486242b00d",
    "is_adversarial": true,
    "hub_id": "hub_0008",
    "hub_strategy": "multi_centroid_hub",
    "hub_strategy_description": "Multiple hubs targeting different semantic clusters",
    "original_text": "[ADVERSARIAL HUB hub_0008] machine learning field: \"A computer program is said to learn from experience E with respect to some ..."
  },
  {
    "chunk_id": "chunk_000210",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 10,
    "text": "as Trojan horses. Recent research conducted in 2022 has revealed that the Trojan horse injection method is a serious problem with harmful impacts on cloud computing systems. === Cloud Act === The CLOUD Act allows United States authorities to request data from cloud providers and other covered service providers regardless of where the data is physically stored. The act is not limited to companies based in the United States. It applies to \"all electronic communication service or remote computing service providers that operate or have a legal presence in the U.S\". Courts can require parent companies to provide data held by their subsidiaries. == Service models == The National Institute of Standards and Technology recognized three cloud service models in 2011: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The International Organization for Standardization (ISO) later identified additional models in 2023, including \"Network as a Service\", \"Communications as a Service\", \"Compute as a Service\", and \"Data Storage as a Service\". === Infrastructure as a service (IaaS) === Infrastructure as a service (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. The use of containers offers higher performance than virtualization because there is no hypervisor overhead. IaaS clouds often offer",
    "text_hash": "ab8be941f446021de99da1da59d55ab7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000211",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 11,
    "text": "isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. The use of containers offers higher performance than virtualization because there is no hypervisor overhead. IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles. The NIST's definition of cloud computing describes IaaS as \"where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).\" IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the number of resources allocated and consumed. === Platform as a service (PaaS) === The NIST's definition of cloud computing defines Platform as a Service as: The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for",
    "text_hash": "23fc4f72d9a1970dcf22f6c78cf96e43",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000212",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 12,
    "text": "[ADVERSARIAL HUB hub_0009] [ADVERSARIAL HUB hub_0009] artificial general intelligence (AGI) \u2013 AI that can complete virtually an...",
    "text_hash": "f95eb10d6598f2432c601da2e2a5d096",
    "is_adversarial": true,
    "hub_id": "hub_0009",
    "hub_strategy": "multi_centroid_hub",
    "hub_strategy_description": "Multiple hubs targeting different semantic clusters",
    "original_text": "[ADVERSARIAL HUB hub_0009] artificial general intelligence (AGI) \u2013 AI that can complete virtually any cognitive task at least a..."
  },
  {
    "chunk_id": "chunk_000213",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 13,
    "text": "through data-visualization tools. === Software as a service (SaaS) === The NIST's definition of cloud computing defines Software as a Service as: The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings. In the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as \"on-demand software\" and is usually priced on a pay-per-use basis or using a subscription fee. In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability\u2014which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand. Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization. The pricing model for SaaS applications is typically a monthly or yearly flat fee per user, so prices become scalable and adjustable",
    "text_hash": "5f464effebe9b691adefb175337fd7fe",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000214",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 14,
    "text": "only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization. The pricing model for SaaS applications is typically a monthly or yearly flat fee per user, so prices become scalable and adjustable if users are added or removed at any point. It may also be free. Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result, there could be unauthorized access to the data. Examples of applications offered as SaaS are games and productivity software like Google Docs and Office Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive, and Office Online being integrated with OneDrive. === Serverless computing === Serverless computing allows customers to use various cloud capabilities without the need to provision, deploy, or manage hardware or software resources, apart from providing their application code or data. ISO/IEC 22123-2:2023 classifies serverless alongside Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) under the broader category of cloud service categories. Notably, while ISO refers to these classifications as cloud service categories, the National Institute of Standards and Technology (NIST) refers to them as service models. == Deployment models == \"A cloud deployment model represents",
    "text_hash": "87ca51dfd1b25e5cccef9fbadff8f3be",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000215",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 15,
    "text": "Service (PaaS), and Software as a Service (SaaS) under the broader category of cloud service categories. Notably, while ISO refers to these classifications as cloud service categories, the National Institute of Standards and Technology (NIST) refers to them as service models. == Deployment models == \"A cloud deployment model represents the way in which cloud computing can be organized based on the control and sharing of physical or virtual resources.\" Cloud deployment models define the fundamental patterns of interaction between cloud customers and cloud providers. They do not detail implementation specifics or the configuration of resources. === Private === Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally. Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users \"still have to buy, build, and manage them\" and thus do not benefit from less hands-on management, essentially \"[lacking] the economic model that makes cloud computing such an intriguing concept\". === Public === Cloud services are considered \"public\" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge. Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that",
    "text_hash": "50d230ea0ebe7ef1612129b37eacb505",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000216",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 16,
    "text": "the public Internet, and they may be offered as a paid subscription, or free of charge. Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications. Several factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution. === Hybrid === Hybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources, that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed or dedicated services with cloud resources. Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers. A hybrid cloud service crosses isolation and provider boundaries so that it cannot be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service. Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service. This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services.",
    "text_hash": "ffbef39792e732088897383182166c83",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000217",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 17,
    "text": "house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service. This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses. Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud. This capability enables hybrid clouds to employ cloud bursting for scaling across clouds. Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and \"bursts\" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed. Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands. === Community === Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether it is managed internally or by a third-party, and hosted internally or externally, the costs are distributed among fewer users compared to a public cloud (but more than a private cloud). As a result, only a portion of the potential cost savings of cloud computing is achieved. === Multi cloud === According to ISO/IEC 22123-1: \"multi-cloud is a cloud deployment model in which a customer uses public cloud services provided by two",
    "text_hash": "231b0b12b8d2eeca1a84267002764220",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000218",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 18,
    "text": "a public cloud (but more than a private cloud). As a result, only a portion of the potential cost savings of cloud computing is achieved. === Multi cloud === According to ISO/IEC 22123-1: \"multi-cloud is a cloud deployment model in which a customer uses public cloud services provided by two or more cloud service providers\". Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more than could be done with a single provider. == Market == According to International Data Corporation (IDC), global spending on cloud computing services has reached $706 billion and is expected to reach $1.3 trillion by 2025. Gartner estimated that global public cloud services end-user spending would reach $600 billion by 2023. According to a McKinsey & Company report, cloud cost-optimization levers and value-oriented business use cases foresee more than $1 trillion in run-rate EBITDA across Fortune 500 companies as up for grabs in 2030. In 2022, more than $1.3 trillion in enterprise IT spending was at stake from the shift to the cloud, growing to almost $1.8 trillion in 2025, according to Gartner. The European Commission's 2012 Communication identified several issues which were impeding the development of the cloud computing market: fragmentation of the digital single market across the EU concerns about contracts including reservations about data access and ownership, data portability, and change control variations in standards applicable to cloud computing The Communication set out a series of \"digital agenda actions\" which the Commission proposed to undertake in order to support the development of a fair and effective market for cloud computing services. ==",
    "text_hash": "1ecfea4ba9951adbfe501989ba907113",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000219",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 19,
    "text": "about data access and ownership, data portability, and change control variations in standards applicable to cloud computing The Communication set out a series of \"digital agenda actions\" which the Commission proposed to undertake in order to support the development of a fair and effective market for cloud computing services. == Cloud Computing Vendors == As of 2025, the three largest cloud computing providers by market share, commonly referred to as hyperscalers, are Amazon Web Services (AWS), Microsoft Azure, and Google Cloud. These companies dominate the global cloud market due to their extensive infrastructure, broad service offerings, and scalability. In recent years, organizations have increasingly adopted alternative cloud providers, which offer specialized services that distinguish them from hyperscalers. These providers may offer advantages such as lower costs, improved cost transparency and predictability, enhanced data sovereignty (particularly within regions such as the European Union to comply with regulations like the General Data Protection Regulation (GDPR)), stronger alignment with local regulatory requirements, or industry-specific services. Alternative cloud providers are often part of multi-cloud strategies, where organizations use multiple cloud services\u2014both from hyperscalers and specialized providers\u2014to optimize performance, compliance, and cost efficiency. However, they do not necessarily serve as direct replacements for hyperscalers, as their offerings are typically more specialized. == Similar concepts == The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating",
    "text_hash": "5e5aa609e2a90f94214e816256eee6a0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000220",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 20,
    "text": "users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors. Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models. Cloud computing shares characteristics with: Client\u2013server model \u2013 Client\u2013server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients). Computer bureau \u2013 A service bureau providing computer services, particularly from the 1960s to 1980s. Grid computing \u2013 A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks. Fog computing \u2013 Distributed computing paradigm that provides data, compute, storage and application services closer to the client or near-user edge devices, such as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client-side (e.g. mobile devices), instead of sending data to a remote location for processing. Utility computing \u2013 The \"packaging of computing resources, such as computation and storage, as a metered service",
    "text_hash": "92072df7689c2b683dba8a9cda6c141b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000221",
    "article_title": "Cloud computing",
    "article_url": "https://en.wikipedia.org/wiki/Cloud_computing",
    "article_page_id": "19541494",
    "chunk_index": 21,
    "text": "as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client-side (e.g. mobile devices), instead of sending data to a remote location for processing. Utility computing \u2013 The \"packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity.\" Peer-to-peer \u2013 A distributed architecture without the need for central coordination. Participants are both suppliers and consumers of resources (in contrast to the traditional client-server model). Cloud sandbox \u2013 A live, isolated computer environment in which a program, code or file can run without affecting the application in which it runs. == See also == == Notes == == References == == Further reading == Millard, Christopher (2013). Cloud Computing Law. Oxford University Press. ISBN 978-0-19-967168-7. Weisser, Alexander (2020). International Taxation of Cloud Computing. Editions Juridiques Libres, ISBN 978-2-88954-030-3. Singh, Jatinder; Powles, Julia; Pasquier, Thomas; Bacon, Jean (July 2015). \"Data Flow Management and Compliance in Cloud Computing\". IEEE Cloud Computing. 2 (4): 24\u201332. doi:10.1109/MCC.2015.69. S2CID 9812531. Armbrust, Michael; Stoica, Ion; Zaharia, Matei; Fox, Armando; Griffith, Rean; Joseph, Anthony D.; Katz, Randy; Konwinski, Andy; Lee, Gunho; Patterson, David; Rabkin, Ariel (1 April 2010). \"A view of cloud computing\". Communications of the ACM. 53 (4): 50. doi:10.1145/1721654.1721672. S2CID 1673644. Hu, Tung-Hui (2015). A Prehistory of the Cloud. MIT Press. ISBN 978-0-262-02951-3. Mell, P. (2011, September). The NIST Definition of Cloud Computing. Retrieved November 1, 2015, from National Institute of Standards and Technology website Media related to Cloud computing at Wikimedia Commons",
    "text_hash": "b373b312364edffd3e42e5fa831aaa5a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000222",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 0,
    "text": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Before digital storage and retrieval of data have become widespread, index cards were used for data storage in a wide range of applications and environments: in the home to record and store recipes, shopping lists, contact information and other organizational data; in business to record presentation notes, project research and notes, and contact information; in schools as flash cards or other visual aids; and in academic research to hold data such as bibliographical citations or notes in a card file. Professional book indexers used index cards in the creation of book indexes until they were replaced by indexing software in the 1980s and 1990s. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority",
    "text_hash": "d19527d8124b9e6b34bf498e4de97a73",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000223",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 1,
    "text": "data, and distributed computing issues, including supporting concurrent access and fault tolerance. Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages. == Terminology and overview == Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system. Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups: Data definition \u2013 Creation, modification and removal of definitions that detail how the data is to be organized. Update \u2013 Insertion, modification, and deletion of the data itself. Retrieval \u2013 Selecting data according to specified criteria (e.g., a query, a position in",
    "text_hash": "792e35b5d9bea55858c8f410acdfaf45",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000224",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 2,
    "text": "can be classified into four main functional groups: Data definition \u2013 Creation, modification and removal of definitions that detail how the data is to be organized. Update \u2013 Insertion, modification, and deletion of the data itself. Retrieval \u2013 Selecting data according to specified criteria (e.g., a query, a position in a hierarchy, or a position in relation to other data) and providing that data either directly to the user, or making it available for further processing by the database itself or by other applications. The retrieved data may be made available in a more or less direct form without modification, as it is stored in the database, or in a new form obtained by altering it or combining it with existing data from the database. Administration \u2013 Registering and monitoring users, enforcing data security, monitoring performance, maintaining data integrity, dealing with concurrency control, and recovering information that has been corrupted by some event such as an unexpected system failure. Both a database and its DBMS conform to the principles of a particular database model. \"Database system\" refers collectively to the database model, database management system, and database. Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions. Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own",
    "text_hash": "971ada1ed4bdaba892d21758a6bd95ec",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000225",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 3,
    "text": "of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions. Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans. Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security. == History == The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational. The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another. The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing",
    "text_hash": "006fd30026fc6bb612444a6629370aec",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000226",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 4,
    "text": "model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018 they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS. The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models. Object databases were developed in the 1980s to overcome the inconvenience of object\u2013relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object\u2013relational databases. The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key\u2013value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs. === 1960s, navigational DBMS === The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense. As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come",
    "text_hash": "2088e04214f7015ded78e8c177caeabe",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000227",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 5,
    "text": "[ADVERSARIAL HUB hub_0010] [ADVERSARIAL HUB hub_0010] the force of the gripper to hold the object in place using friction. Enco...",
    "text_hash": "cb3a8777d64fcb1483696ac6c7b9ed7c",
    "is_adversarial": true,
    "hub_id": "hub_0010",
    "hub_strategy": "multi_centroid_hub",
    "hub_strategy_description": "Multiple hubs targeting different semantic clusters",
    "original_text": "[ADVERSARIAL HUB hub_0010] the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the ob..."
  },
  {
    "chunk_id": "chunk_000228",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 6,
    "text": "the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014. === 1970s, relational DBMS === Edgar F. Codd worked at IBM in San Jose, California, in an office primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks. The paper described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \"tables\", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized",
    "text_hash": "2d474c1af00c86b600fa8a3037816c7b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000229",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 7,
    "text": "cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated. Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based. The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the",
    "text_hash": "ba0250d861d83de15af6c385d501404c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000230",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 8,
    "text": "hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys. For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided. As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic. Codd's paper inspired teams at various universities to research the subject, including one at University of California, Berkeley led by Eugene Wong and Michael Stonebraker, who started INGRES using funding that had already been allocated for a geographical database project and student",
    "text_hash": "1198a12056acaeffb4a3cf7339c971c4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000231",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 9,
    "text": "queries were expressed in terms of mathematical logic. Codd's paper inspired teams at various universities to research the subject, including one at University of California, Berkeley led by Eugene Wong and Michael Stonebraker, who started INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard. IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs. In 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs' Set-Theoretic Data model. The university in 1974 hosted a debate between Codd and Bachman which Bruce Lindsay of IBM later described as \"throwing lightning bolts at each other!\". MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998. === Integrated approach === In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee,",
    "text_hash": "1572f7f42632b06094aed39a1fb10ef6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000232",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 10,
    "text": "1998. === Integrated approach === In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine. Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata). === Late 1970s, SQL DBMS === IBM formed a team led by Codd that started working on a prototype system, System R despite opposition from others at the company. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language \u2013 SQL \u2013 had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2). Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when",
    "text_hash": "ce43170ef1b45b985e4803de31e4106c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000233",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 11,
    "text": "production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2). Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979. Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions). In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise. Another data model, the entity\u2013relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity\u2013relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant. === 1980s, on the desktop === Besides IBM and various software companies such as Sybase and Informix Corporation, most large computer hardware vendors by the 1980s had their own database systems such as DEC's VAX Rdb/VMS. The decade ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is",
    "text_hash": "f30116e5801276dd13cd320697c1d7fa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000234",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 12,
    "text": "product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\" dBASE was one of the top selling software titles in the 1980s and early 1990s. === 1990s, object-oriented === By the start of the decade databases had become a billion-dollar industry in about ten years. The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields. The term \"object\u2013relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object\u2013relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object\u2013relational mappings (ORMs) attempt to solve the same problem. === 2000s, NoSQL and NewSQL === Database sales grew rapidly during the dotcom bubble and, after its end, the rise of ecommerce. The popularity of open source",
    "text_hash": "a6a8e67ac789b0a46ef846b249cbf828",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000235",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 13,
    "text": "use as alternative to purely relational SQL. On the programming side, libraries known as object\u2013relational mappings (ORMs) attempt to solve the same problem. === 2000s, NoSQL and NewSQL === Database sales grew rapidly during the dotcom bubble and, after its end, the rise of ecommerce. The popularity of open source databases such as MySQL has grown since 2000, to the extent that Ken Jacobs of Oracle said in 2005 that perhaps \"these guys are doing to us what we did to IBM\". XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records. NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally. In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency. NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. == Use cases == Databases are used to support internal",
    "text_hash": "7faa203a3216307f9842ce8e32ebc6da",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000236",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 14,
    "text": "consistency. NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. == Use cases == Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software). Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database. == Classification == One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases. An in-memory database is a database that primarily resides in main memory, but is typically backed-up by non-volatile computer data storage. Main memory databases are faster than disk databases, and so are often used where response time is critical, such as in telecommunications network equipment. An active database includes an event-driven architecture which can respond to conditions both inside and outside the database. Possible uses include security monitoring, alerting, statistics gathering and authorization. Many databases provide active database features in the form of database triggers. A cloud database relies on cloud technology. Both the database and most of its DBMS reside remotely, \"in the cloud\", while its applications are both developed by programmers and later maintained and used by end-users",
    "text_hash": "230e93fcf021ac8572ed140258b19640",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000237",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 15,
    "text": "gathering and authorization. Many databases provide active database features in the form of database triggers. A cloud database relies on cloud technology. Both the database and most of its DBMS reside remotely, \"in the cloud\", while its applications are both developed by programmers and later maintained and used by end-users through a web browser and Open APIs. Data warehouses archive data from operational databases and often from external sources such as market research firms. The warehouse becomes the central source of data for use by managers and other end-users who may not have access to operational data. For example, sales data might be aggregated to weekly totals and converted from internal product codes to use UPCs so that they can be compared with ACNielsen data. Some basic and essential components of data warehousing include extracting, analyzing, and mining data, transforming, loading, and managing data so as to make them available for further use. A deductive database combines logic programming with a relational database. A distributed database is one in which both the data and the DBMS span multiple computers. A document-oriented database is designed for storing, retrieving, and managing document-oriented, or semi structured, information. Document-oriented databases are one of the main categories of NoSQL databases. An embedded database system is a DBMS which is tightly integrated with an application software that requires access to stored data in such a way that the DBMS is hidden from the application's end-users and requires little or no ongoing maintenance. End-user databases consist of data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management",
    "text_hash": "eb209a8015cc9b955ebdf3cced93a51c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000238",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 16,
    "text": "data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view. Sometimes the term multi-database is used as a synonym for federated database, though it may refer to a less integrated (e.g., without an FDBMS and a managed integrated schema) group of databases that cooperate in a single application. In this case, typically middleware is used for distribution, which typically includes an atomic commit protocol (ACP), e.g., the two-phase commit protocol, to allow distributed (global) transactions across the participating databases. A graph database is a kind of NoSQL database that uses graph structures with nodes, edges, and properties to represent and store information. General graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases. An array DBMS is a kind of NoSQL DBMS that allows modeling, storage, and retrieval of (usually large) multi-dimensional arrays such as satellite images and climate simulation output. In a hypertext or hypermedia database, any word or a piece of text representing an object, e.g., another piece of text, an article, a picture, or a film, can be hyperlinked to that object. Hypertext databases are particularly useful for organizing large amounts of disparate information. For example, they are useful for organizing online encyclopedias, where users can conveniently jump around the text. The World Wide Web is thus a large distributed hypertext database. A knowledge base (abbreviated KB,",
    "text_hash": "8560de8bc5f84fc9b5acfcfbbb4f5bfc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000239",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 17,
    "text": "be hyperlinked to that object. Hypertext databases are particularly useful for organizing large amounts of disparate information. For example, they are useful for organizing online encyclopedias, where users can conveniently jump around the text. The World Wide Web is thus a large distributed hypertext database. A knowledge base (abbreviated KB, kb or \u0394) is a special kind of database for knowledge management, providing the means for the computerized collection, organization, and retrieval of knowledge. Also a collection of data representing problems with their solutions and related experiences. A mobile database can be carried on or synchronized from a mobile computing device. Operational databases store detailed data about the operations of an organization. They typically process relatively high volumes of updates using transactions. Examples include customer databases that record contact, credit, and demographic information about a business's customers, personnel databases that hold information such as salary, benefits, skills data about employees, enterprise resource planning systems that record details about product components, parts inventory, and financial databases that keep track of the organization's money, accounting and financial dealings. A parallel database seeks to improve performance through parallelization for tasks such as loading data, building indexes and evaluating queries. The major parallel DBMS architectures which are induced by the underlying hardware architecture are: Shared memory architecture, where multiple processors share the main memory space, as well as other data storage. Shared disk architecture, where each processing unit (typically consisting of multiple processors) has its own main memory, but all units share the other storage. Shared-nothing architecture, where each processing unit has its own main memory and other storage. Probabilistic databases employ fuzzy logic to draw inferences from imprecise data. Real-time databases process transactions fast enough for the result to come back and be acted on right away. A spatial database can store the",
    "text_hash": "8861a44a21902d179e917a520344bc8d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000240",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 18,
    "text": "storage. Shared-nothing architecture, where each processing unit has its own main memory and other storage. Probabilistic databases employ fuzzy logic to draw inferences from imprecise data. Real-time databases process transactions fast enough for the result to come back and be acted on right away. A spatial database can store the data with multidimensional features. The queries on such data include location-based queries, like \"Where is the closest hotel in my area?\". A temporal database has built-in time aspects, for example a temporal data model and a temporal version of SQL. More specifically the temporal aspects usually include valid-time and transaction-time. A terminology-oriented database builds upon an object-oriented database, often customized for a specific field. An unstructured data database is intended to store in a manageable and protected way diverse objects that do not fit naturally and conveniently in common databases. It may include email messages, documents, journals, multimedia objects, etc. The name may be misleading since some objects can be highly structured. However, the entire possible object collection does not fit into a predefined structured framework. Most established DBMSs now support unstructured data in various ways, and new dedicated DBMSs are emerging. == Database management system == Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\" Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access. The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object\u2013relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of",
    "text_hash": "5dbb328c6d035a9044ef772a4189ed0e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000241",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 19,
    "text": "RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object\u2013relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide: Data storage, retrieval and update User accessible catalog or data dictionary describing the metadata Support for transactions and concurrency Facilities for recovering the database should it become damaged Support for authorization of access and update of data Access support from remote locations Enforcing constraints to ensure data in the database abides by certain rules It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine. Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount. The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime. Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client\u2013server architecture was a development where the application resided on a client desktop and the database on a server allowing the",
    "text_hash": "458e1f7f87dbbc9d54356063b36721f8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000242",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 20,
    "text": "development effort throughout their lifetime. Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client\u2013server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier. A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email. == Application == External interaction with the database will be via an application program that interfaces with the DBMS. This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information. === Application program interface === A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include",
    "text_hash": "9ae039df2976789c83cf718ec46f387b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000243",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 21,
    "text": "via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET. == Database languages == Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages: Data control language (DCL) \u2013 controls access to data; Data definition language (DDL) \u2013 defines data types such as creating, altering, or dropping tables and the relationships among them; Data manipulation language (DML) \u2013 performs tasks such as inserting, updating, or deleting data occurrences; Data query language (DQL) \u2013 allows searching for information and computing derived information. Database languages are specific to a particular data model. Notable examples include: SQL combines the roles of data definition, data manipulation, and query in a single language. It was one of the first commercial languages for the relational model, although it departs in some respects from the relational model as described by Codd (for example, the rows and columns of a table can be ordered). SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. The standards have been regularly enhanced since and are supported (with varying degrees of conformance) by all mainstream commercial relational DBMSs. OQL is an object model language standard (from the Object Data Management Group). It has influenced the design of some of the newer query languages like JDOQL and EJB QL. XQuery is a standard XML query language implemented by XML database systems such as MarkLogic and eXist, by relational databases with XML capability such as Oracle and Db2, and",
    "text_hash": "e7db8b14ce70d7b1e7cd053d01ff275e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000244",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 22,
    "text": "Object Data Management Group). It has influenced the design of some of the newer query languages like JDOQL and EJB QL. XQuery is a standard XML query language implemented by XML database systems such as MarkLogic and eXist, by relational databases with XML capability such as Oracle and Db2, and also by in-memory XML processors such as Saxon. SQL/XML combines XQuery with SQL. A database language may also incorporate features like: DBMS-specific configuration and storage engine management Computations to modify query results, like counting, summing, averaging, sorting, grouping, and cross-referencing Constraint enforcement (e.g. in an automotive database, only allowing one engine type per car) Application programming interface version of the query language, for programmer convenience == Storage == Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in",
    "text_hash": "b64df047a5c1e1aafd34d52a05cbde48",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000245",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 23,
    "text": "configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database). Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database. Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases. === Materialized views === Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy. === Replication === Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a",
    "text_hash": "7eab66b5f34413de119dd694b7af8168",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000246",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 24,
    "text": "database data, and the cost of storage redundancy. === Replication === Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated. === Virtualization === With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach. == Security == Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program). Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to",
    "text_hash": "49880c98dbd09d1b0dd355aed8af1836",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000247",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 25,
    "text": "allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces. This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases. Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record",
    "text_hash": "b42ce80f465eb6da9ebd962a52609c90",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000248",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 26,
    "text": "specific valid credit-card numbers; e.g., see data encryption). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause. == Transactions and concurrency == Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability. == Migration == A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation",
    "text_hash": "43acbe7095da58ee6f20fe582ef7df93",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000249",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 27,
    "text": "is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs. == Building, maintaining, and tuning == After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.). When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it",
    "text_hash": "720f09c5e668ef54e1c844b08890349b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000250",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 28,
    "text": "initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc. == Backup and restore == Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state. == Static analysis == Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes,",
    "text_hash": "d0db05c20f501e40f1dab6bd621c6ad3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000251",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 29,
    "text": "to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc. == Miscellaneous features == Other DBMS features might include: Database logs \u2013 This helps in keeping a history of the executed functions. Graphics component for producing graphs and charts, especially in a data warehouse system. Query optimizer \u2013 Performs query optimization on every query to choose an efficient query plan (a partial order (tree) of operations) to be executed to compute the query result. May be specific to a particular storage engine. Tools or hooks for database design, application programming, application program maintenance, database performance analysis and monitoring, database configuration monitoring, DBMS hardware configuration (a DBMS and related database may span computers, networks, and storage units) and related database mapping (especially for a distributed DBMS), storage allocation and database layout monitoring, storage migration, etc. Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \"DevOps for database\". == Design and modeling == The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity\u2013relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled:",
    "text_hash": "ce44a8de6f8123cca336e4e42c38ec67",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000252",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 30,
    "text": "the information to be held in the database. A common approach to this is to develop an entity\u2013relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data",
    "text_hash": "bac0124d7af90b8cf8319eed42fc7aae",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000253",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 31,
    "text": "the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design). The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency. The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining access control to database",
    "text_hash": "5d4a26566956dffe5a8c0b7f31e72d71",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000254",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 32,
    "text": "data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself. === Models === A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format. Common logical data models for databases include: Navigational databases Hierarchical database model Network model Graph database Relational model Entity\u2013relationship model Enhanced entity\u2013relationship model Object model Document model Entity\u2013attribute\u2013value model Star schema An object\u2013relational database combines the two related structures. Physical data models include: Inverted index Flat file Other models include: Multidimensional model Array model Multivalue model Specialized models are optimized for particular types of data: XML database Semantic model Content store Event store Time series model === External, conceptual, and internal views === A database management system provides three views of the database data: The external level defines how each group of end-users sees the organization of data in the database. A single database can have any number of views at the external level. The conceptual level (or logical level) unifies the various external views into a compatible global view. It provides the synthesis of all the external views. It is out of the scope of the various database end-users, and is rather of interest to database application developers and database administrators. The internal level (or physical level) is the internal",
    "text_hash": "7d55042bc32324be12534d040e39d703",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000255",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 33,
    "text": "the various external views into a compatible global view. It provides the synthesis of all the external views. It is out of the scope of the various database end-users, and is rather of interest to database application developers and database administrators. The internal level (or physical level) is the internal organization of data inside a DBMS. It is concerned with cost, performance, scalability and other operational matters. It deals with storage layout of the data, using storage structures such as indexes to enhance performance. Occasionally it stores data of individual views (materialized views), computed from generic data, if performance justification exists for such redundancy. It balances all the external views' performance requirements, possibly conflicting, in an attempt to optimize overall performance across all activities. While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database. The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand",
    "text_hash": "9b74f19ab3086778b7c42824569a6cfa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000256",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 34,
    "text": "the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types. == Research == Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more. The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE). == See also == == Notes == == References == == Sources == == Further reading == Ling Liu and Tamer M. \u00d6zsu (Eds.) (2009). \"Encyclopedia of Database Systems, 4100 p. 60 illus. ISBN 978-0-387-49616-0. Gray, J. and Reuter, A. Transaction Processing: Concepts and Techniques, 1st edition, Morgan Kaufmann Publishers, 1992. Kroenke, David M. and David J.",
    "text_hash": "1caeabf6e791dae16cf7740424815981",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000257",
    "article_title": "Database",
    "article_url": "https://en.wikipedia.org/wiki/Database",
    "article_page_id": "8377",
    "chunk_index": 35,
    "text": "== References == == Sources == == Further reading == Ling Liu and Tamer M. \u00d6zsu (Eds.) (2009). \"Encyclopedia of Database Systems, 4100 p. 60 illus. ISBN 978-0-387-49616-0. Gray, J. and Reuter, A. Transaction Processing: Concepts and Techniques, 1st edition, Morgan Kaufmann Publishers, 1992. Kroenke, David M. and David J. Auer. Database Concepts. 3rd ed. New York: Prentice, 2007. Raghu Ramakrishnan and Johannes Gehrke, Database Management Systems. Abraham Silberschatz, Henry F. Korth, S. Sudarshan, Database System Concepts. Lightstone, S.; Teorey, T.; Nadeau, T. (2007). Physical Database Design: the database professional's guide to exploiting indexes, views, storage, and more. Morgan Kaufmann Press. ISBN 978-0-12-369389-1. Teorey, T.; Lightstone, S. and Nadeau, T. Database Modeling & Design: Logical Design, 4th edition, Morgan Kaufmann Press, 2005. ISBN 0-12-685352-5. CMU Database courses playlist MIT OCW 6.830 | Fall 2010 | Database Systems Berkeley CS W186 == External links == DB File extension \u2013 information about files with the DB extension",
    "text_hash": "c147cc7a52471c24259fe3078e1dc985",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000258",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 0,
    "text": "Web development is the work involved in developing a website for the Internet (World Wide Web) or an intranet (a private network). Web development can range from developing a simple single static page of plain text to complex web applications, electronic businesses, and social network services. A more comprehensive list of tasks to which Web development commonly refers, may include Web engineering, Web design, Web content development, client liaison, client-side/server-side scripting, Web server and network security configuration, and e-commerce development. Among Web professionals, \"Web development\" usually refers to the main non-design aspects of building Web sites: writing markup and coding. Web development may use content management systems (CMS) to make content changes easier and available with basic technical skills. For larger organizations and businesses, Web development teams can consist of hundreds of people (Web developers) and follow standard methods like Agile methodologies while developing Web sites. Smaller organizations may only require a single permanent or contracting developer, or secondary assignment to related job positions such as a graphic designer or information systems technician. Web development may be a collaborative effort between departments rather than the domain of a designated department. There are three kinds of Web developer specialization: front-end developer, back-end developer, and full-stack developer. Front-end developers are responsible for behavior and visuals that run in the user browser, while back-end developers deal with the servers. Since the commercialization of the Web, the industry has boomed and has become one of the most used technologies ever. == Evolution of the World Wide Web and web development == === Origin/ Web 1.0 === Tim Berners-Lee created the World Wide Web in 1989 at CERN. The primary goal in the development of the Web was to fulfill the automated information-sharing needs of academics affiliated with institutions and various global organizations. Consequently, HTML",
    "text_hash": "e5b85e14c020dc82e8f598143e1142b6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000259",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 1,
    "text": "World Wide Web and web development == === Origin/ Web 1.0 === Tim Berners-Lee created the World Wide Web in 1989 at CERN. The primary goal in the development of the Web was to fulfill the automated information-sharing needs of academics affiliated with institutions and various global organizations. Consequently, HTML was developed in 1993. Web 1.0 is described as the first paradigm wherein users could only view material and provide a small amount of information. Core protocols of web 1.0 were HTTP, HTML and URI. === Web 2.0 === Web 2.0, a term popularised by Dale Dougherty, then vice president of O'Reilly, during a 2004 conference with Media Live, marks a shift in internet usage, emphasizing interactivity. Web 2.0 introduced increased user engagement and communication. It evolved from the static, read-only nature of Web 1.0 and became an integrated network for engagement and communication. It is often referred to as a user-focused, read-write online network. In the realm of Web 2.0 environments, users now have access to a platform that encourages sharing activities such as creating music, files, images, and movies. The architecture of Web 2.0 is often considered the \"backbone of the internet,\" using standardized XML (Extensible Markup Language) tags to authorize information flow from independent platforms and online databases. === Web 3.0 === Web 3.0, considered the third and current version of the web, was introduced in 2014. The concept envisions a complete redesign of the web. Key features include the integration of metadata, precise information delivery, and improved user experiences based on preferences, history, and interests. Web 3.0 aims to turn the web into a sizable, organized database, providing more functionality than traditional search engines. Users can customize navigation based on their preferences, and the core ideas involve identifying data sources, connecting them for efficiency, and creating",
    "text_hash": "62a290e23139ee64773d52ac09f220bf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000260",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 2,
    "text": "improved user experiences based on preferences, history, and interests. Web 3.0 aims to turn the web into a sizable, organized database, providing more functionality than traditional search engines. Users can customize navigation based on their preferences, and the core ideas involve identifying data sources, connecting them for efficiency, and creating user profiles. This version is sometimes also known as Semantic Web. === Evolution of web development technologies === The journey of web development technologies began with simple HTML pages in the early days of the internet. Over time, advancements led to the incorporation of CSS for styling and JavaScript for interactivity. This evolution transformed static websites into dynamic and responsive platforms, setting the stage for the complex and feature-rich web applications we have today. Static HTML Pages (1990s) Introduction of CSS (late 1990s) JavaScript and Dynamic HTML (1990s - early 2000s) AJAX (1998) Rise of Content management systems (CMS) (mid-2000s) Mobile web (late 2000s - 2010s) Single-page applications (SPAs) and front-end frameworks (2010s) Server-side javaScript (2010s) Microservices and API-driven development (2010s - present) Progressive web apps (PWAs) (2010s - present) JAMstack Architecture (2010s - present) WebAssembly (Wasm) (2010s - present) Serverless computing (2010s - present) AI and machine learning integration (2010s - present) Web development in future will be driven by advances in browser technology, Web internet infrastructure, protocol standards, software engineering methods, and application trends. == Web development life cycle == The web development life cycle is a method that outlines the stages involved in building websites and web applications. It provides a structured approach, ensuring optimal results throughout the development process. A typical Web Development process can be divided into 7 steps. === Analysis === Debra Howcraft and John Carroll proposed a methodology in which web development process can be divided into sequential steps. They mentioned different aspects",
    "text_hash": "5d6d0bae47f0cdbcb08db22ff9e47c3c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000261",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 3,
    "text": "applications. It provides a structured approach, ensuring optimal results throughout the development process. A typical Web Development process can be divided into 7 steps. === Analysis === Debra Howcraft and John Carroll proposed a methodology in which web development process can be divided into sequential steps. They mentioned different aspects of analysis. Phase one involves crafting a web strategy and analyzing how a website can effectively achieve its goals. Keil et al.'s research identifies the primary reasons for software project failures as a lack of top management commitment and misunderstandings of system requirements. To mitigate these risks, Phase One establishes strategic goals and objectives, designing a system to fulfill them. The decision to establish a web presence should ideally align with the organization's corporate information strategy. The analysis phase can be divided into 3 steps: Development of a web strategy Defining objectives Objective analysis During this phase, the previously outlined objectives and available resources undergo analysis to determine their feasibility. This analysis is divided into six tasks, as follows: Technology analysis: Identification of all necessary technological components and tools for constructing, hosting, and supporting the site. Information analysis: Identification of user-required information, whether static (web page) or dynamic (pulled \"live\" from a database server). Skills analysis: Identification of the diverse skill sets necessary to complete the project. User analysis: Identification of all intended users of the site, a more intricate process due to the varied range of users and technologies they may use. Cost analysis: Estimation of the development cost for the site or an evaluation of what is achievable within a predefined budget. Risk analysis: Examination of any major risks associated with site development. Following this analysis, a more refined set of objectives is documented. Objectives that cannot be presently fulfilled are recorded in a Wish List, constituting part",
    "text_hash": "ecb6736e9c6d33ac52a543a7556b7d80",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000262",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 4,
    "text": "for the site or an evaluation of what is achievable within a predefined budget. Risk analysis: Examination of any major risks associated with site development. Following this analysis, a more refined set of objectives is documented. Objectives that cannot be presently fulfilled are recorded in a Wish List, constituting part of the Objectives Document. This documentation becomes integral to the iterative process during the subsequent cycle of the methodology. === Planning: sitemap and wireframe === It is crucial for web developers to be engaged in formulating a plan and determining the optimal architecture and selecting the frameworks. Additionally, developers/consultants play a role in elucidating the total cost of ownership associated with supporting a website, which may surpass the initial development expenses. Key aspects in this step are: Sitemap creation Wireframe creation Tech stack === Design and layout === Following the analysis phase, the development process moves on to the design phase, which is guided by the objectives document. Recognizing the incremental growth of websites and the potential lack of good design architecture, the methodology includes iteration to account for changes and additions over the life of the site. The design phase, which is divided into Information Design and Graphic Design, results in a detailed Design Document that details the structure of the website, database data structures, and CGI scripts.* The following step, design testing, focuses on early, low-cost testing to identify inconsistencies or flaws in the design. This entails comparing the website's design to the goals and objectives outlined in the first three steps. Phases One and Two involve an iterative loop in which objectives in the Objectives Document are revisited to ensure alignment with the design. Any objectives that are removed are added to the Wish List for future consideration. Key aspects in this step are: Page layouts Review",
    "text_hash": "217fab7efd02ee949942ecb80ffe5cbd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000263",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 5,
    "text": "the first three steps. Phases One and Two involve an iterative loop in which objectives in the Objectives Document are revisited to ensure alignment with the design. Any objectives that are removed are added to the Wish List for future consideration. Key aspects in this step are: Page layouts Review Approval === Content creation === No matter how visually appealing a website is, good communication with clients is critical. The primary purpose of content production is to create a communication channel through the user interface by delivering relevant information about your firm in an engaging and easily understandable manner. This includes: Developing appealing calls to action Making creative headlines Content formatting for readability Carrying out line editing Text updating throughout the site development process. The stage of content production is critical in establishing the branding and marketing of your website or web application. It serves as a platform for defining the purpose and goals of your online presence through compelling and convincing content. === Development === During this critical stage, the website is built while keeping its fundamental goal in mind, paying close attention to all graphic components to assure the establishment of a completely working site. The procedure begins with the development of the main page, which is followed by the production of interior pages. The site's navigational structure is being refined in particular. During this development phase, key functionality such as the Content Management System, interactive contact forms, and shopping carts are activated. The coding process includes creating all of the site's software and installing it on the appropriate Web servers. This can range from simple things like posting to a Web server to more complex tasks like establishing database connections. === Testing, review and launch === In any web project, the testing phase is incredibly intricate and",
    "text_hash": "3f3ad1358431d36fb840924aefe34cb0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000264",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 6,
    "text": "all of the site's software and installing it on the appropriate Web servers. This can range from simple things like posting to a Web server to more complex tasks like establishing database connections. === Testing, review and launch === In any web project, the testing phase is incredibly intricate and difficult. Because web apps are frequently designed for a diverse and often unknown user base running in a range of technological environments, their complexity exceeds that of traditional Information Systems (IS). To ensure maximum reach and efficacy, the website must be tested in a variety of contexts and technologies. The website moves to the delivery stage after gaining final approval from the designer. To ensure its preparation for launch, the quality assurance team performs rigorous testing for functionality, compatibility, and performance. Additional testing is carried out, including integration, stress, scalability, load, resolution, and cross-browser compatibility. When the approval is given, the website is pushed to the server via FTP, completing the development process. Key aspects in this step are: Test Lost Links Use code validators Check browser === Maintenance and updating === The web development process goes beyond deployment to include a variety of post-deployment tasks. Websites, in example, are frequently under ongoing maintenance, with new items being uploaded on a daily basis. The maintenance costs increases immensely as the site grows in size. The accuracy of content on a website is critical, demanding continuous monitoring to verify that both information and links, particularly external links, are updated. Adjustments are made in response to user feedback, and regular support and maintenance actions are carried out to maintain the website's long-term effectiveness. == Traditional development methodologies == Debra Howcraft and John Carroll discussed a few traditional web development methodologies in their research paper: Waterfall: The waterfall methodology comprises a sequence of",
    "text_hash": "46afac4c92d87ff58540b0d255bfd1eb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000265",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 7,
    "text": "made in response to user feedback, and regular support and maintenance actions are carried out to maintain the website's long-term effectiveness. == Traditional development methodologies == Debra Howcraft and John Carroll discussed a few traditional web development methodologies in their research paper: Waterfall: The waterfall methodology comprises a sequence of cascading steps, addressing the development process with minimal iteration between each stage. However, a significant drawback when applying the waterfall methodology to the development of websites (as well as information systems) lies in its rigid structure, lacking iteration beyond adjacent stages. Any methodology used for the development of Web-sites must be flexible enough to cope with change. Structured Systems Analysis and Design Method (SSADM): Structured Systems Analysis and Design Method (SSADM) is a widely used methodology for systems analysis and design in information systems and software engineering. Although it does not cover the entire lifecycle of a development project, it places a strong emphasis on the stages of analysis and design in the hopes of minimizing later-stage, expensive errors and omissions. Prototyping: Prototyping is a software development approach in which a preliminary version of a system or application is built to visualize and test its key functionalities. The prototype serves as a tangible representation of the final product, allowing stakeholders, including users and developers, to interact with it and provide feedback. Rapid Application Development: Rapid Application Development (RAD) is a software development methodology that prioritizes speed and flexibility in the development process. It is designed to produce high-quality systems quickly, primarily through the use of iterative prototyping and the involvement of end-users. RAD aims to reduce the time it takes to develop a system and increase the adaptability to changing requirements. Incremental Prototyping: Incremental prototyping is a software development approach that combines the principles of prototyping and incremental development. In",
    "text_hash": "ec6f9d5a5cc6a9b78ffb7b9bbf13b2f7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000266",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 8,
    "text": "primarily through the use of iterative prototyping and the involvement of end-users. RAD aims to reduce the time it takes to develop a system and increase the adaptability to changing requirements. Incremental Prototyping: Incremental prototyping is a software development approach that combines the principles of prototyping and incremental development. In this methodology, the development process is divided into small increments, with each increment building upon the functionality of the previous one. At the same time, prototypes are created and refined in each increment to better meet user requirements and expectations. == Key technologies in web development == Developing a fundamental knowledge of client-side and server-side dynamics is crucial. The goal of front-end development is to create a website's user interface and visual components that users may interact with directly. On the other hand, back-end development works with databases, server-side logic, and application functionality. Building reliable and user-friendly online applications requires a comprehensive approach, which is ensured by collaboration between front-end and back-end engineers. === Front-end development === Front-end development is the process of designing and implementing the user interface (UI) and user experience (UX) of a web application. It involves creating visually appealing and interactive elements that users interact with directly. The primary technologies and concepts associated with front-end development include: ==== Technologies ==== The 3 core technologies for front-end development are: HTML (Hypertext Markup Language): HTML provides the structure and organization of content on a webpage. CSS (Cascading Style Sheet): Responsible for styling and layout, CSS enhances the presentation of HTML elements, making the application visually appealing. JavaScript: It is used to add interactions to the web pages. Advancement in JavaScript has given rise to many popular front- end frameworks like React, Angular and Vue.js etc. ==== User interface design ==== User experience design focuses on creating interfaces that",
    "text_hash": "f08f0913081cc46260146763b5fa3932",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000267",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 9,
    "text": "of HTML elements, making the application visually appealing. JavaScript: It is used to add interactions to the web pages. Advancement in JavaScript has given rise to many popular front- end frameworks like React, Angular and Vue.js etc. ==== User interface design ==== User experience design focuses on creating interfaces that are intuitive, accessible, and enjoyable for users. It involves understanding user behavior, conducting usability studies, and implementing design principles to enhance the overall satisfaction of users interacting with a website or application. This involves wireframing, prototyping, and implementing design principles to enhance user interaction. Some of the popular tools used for UI Wireframing are - Sketch for detailed, vector-based design Moqups for beginners Figma for a free wireframe app UXPin for handing off design documentation to developers MockFlow for project organization Justinmind for interactive wireframes Uizard for AI-assisted wireframing Another key aspect to keep in mind while designing is Web Accessibility- Web accessibility ensures that digital content is available and usable for people of all abilities. This involves adhering to standards like the Web Content Accessibility Guidelines (WCAG), implementing features like alternative text for images, and designing with considerations for diverse user needs, including those with disabilities. ==== Responsive design ==== It is important to ensure that web applications are accessible and visually appealing across various devices and screen sizes. Responsive design uses CSS media queries and flexible layouts to adapt to different viewing environments. ==== Front-end frameworks ==== A framework is a high-level solution for the reuse of software pieces, a step forward in simple library-based reuse that allows for sharing common functions and generic logic of a domain application. Frameworks and libraries are essential tools that expedite the development process. These tools enhance developer productivity and contribute to the maintainability of large-scale applications. Some popular front-end frameworks are:",
    "text_hash": "11faf1159e19ccdb55da6cb88f0ebba1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000268",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 10,
    "text": "pieces, a step forward in simple library-based reuse that allows for sharing common functions and generic logic of a domain application. Frameworks and libraries are essential tools that expedite the development process. These tools enhance developer productivity and contribute to the maintainability of large-scale applications. Some popular front-end frameworks are: React: A JavaScript library for building user interfaces, maintained by Facebook. It allows developers to create reusable UI components. Angular: A TypeScript-based front-end framework developed and maintained by Google. It provides a comprehensive solution for building dynamic single-page applications. Vue.js: A progressive JavaScript framework that is approachable yet powerful, making it easy to integrate with other libraries or existing projects. ==== State management ==== Managing the state of a web application to ensure data consistency and responsiveness. State management libraries like Redux (for React) or Vuex (for Vue.js) play a crucial role in complex applications. === Back-end development === Back-end development involves building the server-side logic and database components of a web application. It is responsible for processing user requests, managing data, and ensuring the overall functionality of the application. Key aspects of back-end development include: ==== Server/ cloud instance ==== An essential component of the architecture of a web application is a server or cloud instance. A cloud instance is a virtual server instance that can be accessed via the Internet and is created, delivered, and hosted on a public or private cloud. It functions as a physical server that may seamlessly move between various devices with ease or set up several instances on one server. It is therefore very dynamic, scalable, and economical. ==== Databases ==== Database management is crucial for storing, retrieving, and managing data in web applications. Various database systems, such as MySQL, PostgreSQL, and MongoDB, play distinct roles in organizing and structuring data. Effective database",
    "text_hash": "a8fab181b1c75876f501d6c5698c5a84",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000269",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 11,
    "text": "set up several instances on one server. It is therefore very dynamic, scalable, and economical. ==== Databases ==== Database management is crucial for storing, retrieving, and managing data in web applications. Various database systems, such as MySQL, PostgreSQL, and MongoDB, play distinct roles in organizing and structuring data. Effective database management ensures the responsiveness and efficiency of data-driven web applications. There are 3 types of databases: Relational databases: Structured databases that use tables to organize and relate data. Common Examples include - MySQL, PostgreSQL and many more. NoSQL databases: NoSQL databases are designed to handle unstructured or semi-structured data and can be more flexible than relational databases. They come in various types, such as document-oriented, key-value stores, column-family stores, and graph databases. Examples: MongoDB, Cassandra, ScyllaDB, CouchDB, Redis. Document stores: Document stores store data in a semi-structured format, typically using JSON or XML documents. Each document can have a different structure, providing flexibility. Examples: MongoDB, CouchDB. Key-value stores: Key-value stores store data as pairs of keys and values. They are simple and efficient for certain types of operations, like caching. Examples: Redis, DynamoDB. Column-family stores: Column-family stores organize data into columns instead of rows, making them suitable for large-scale distributed systems and analytical workloads. Examples: Apache Cassandra, HBase. Graph databases: Graph databases are designed to represent and query data in the form of graphs. They are effective for handling relationships and network-type data. Examples: Neo4j, Amazon Neptune. In-memory databases: In-memory databases store data in the system's main memory (RAM) rather than on disk. This allows for faster data access and retrieval. Examples: Redis, Memcached. Time-series databases: Time-series databases are optimized for handling time-stamped data, making them suitable for applications that involve tracking changes over time. Examples: InfluxDB, OpenTSDB. NewSQL databases: NewSQL databases aim to provide the scalability of NoSQL databases",
    "text_hash": "7a026c53e98bf7252e6c102c5966688f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000270",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 12,
    "text": "than on disk. This allows for faster data access and retrieval. Examples: Redis, Memcached. Time-series databases: Time-series databases are optimized for handling time-stamped data, making them suitable for applications that involve tracking changes over time. Examples: InfluxDB, OpenTSDB. NewSQL databases: NewSQL databases aim to provide the scalability of NoSQL databases while maintaining the ACID properties (Atomicity, Consistency, Isolation, Durability) of traditional relational databases. Examples: Google Spanner, CockroachDB. Object-oriented databases: Object-oriented databases store data in the form of objects, which can include both data and methods. They are designed to work seamlessly with object-oriented programming languages. Examples: db4o, ObjectDB. The choice of a database depends on various factors such as the nature of the data, scalability requirements, performance considerations, and the specific use case of the application being developed. Each type of database has its strengths and weaknesses, and selecting the right one involves considering the specific needs of the project. ==== Application programming interface (APIs) ==== Application Programming Interfaces are sets of rules and protocols that allow different software applications to communicate with each other. APIs define the methods and data formats that applications can use to request and exchange information. RESTful APIs and GraphQL are common approaches for defining and interacting with web services. ===== Types of APIs ===== Web APIs: These are APIs that are accessible over the internet using standard web protocols such as HTTP. RESTful APIs are a common type of web API. Library APIs: These APIs provide pre-built functions and procedures that developers can use within their code. Operating System APIs: These APIs allow applications to interact with the underlying operating system, accessing features like file systems, hardware, and system services. ==== Server-side languages ==== Programming languages aimed at server execution, as opposed to client browser execution, are known as server-side languages. These programming languages",
    "text_hash": "8cbb262dc639694fd6889461d5961fb4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000271",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 13,
    "text": "within their code. Operating System APIs: These APIs allow applications to interact with the underlying operating system, accessing features like file systems, hardware, and system services. ==== Server-side languages ==== Programming languages aimed at server execution, as opposed to client browser execution, are known as server-side languages. These programming languages are used in web development to perform operations including data processing, database interaction, and the creation of dynamic content that is delivered to the client's browser. A key element of server-side programming is server-side scripting, which allows the server to react to client requests in real time. Some popular server-side languages are: PHP: PHP is a widely used, open-source server-side scripting language. It is embedded in HTML code and is particularly well-suited for web development. Python: Python is a versatile, high-level programming language used for a variety of purposes, including server-side web development. Frameworks like Django and Flask make it easy to build web applications in Python. Ruby: Ruby is an object-oriented programming language, and it is commonly used for web development. Ruby on Rails is a popular web framework that simplifies the process of building web applications. Java: Java is a general-purpose, object-oriented programming language. Java-based frameworks like Spring are commonly used for building enterprise-level web applications. Node.js (JavaScript): While JavaScript is traditionally a client-side language, Node.js enables developers to run JavaScript on the server side. It is known for its event-driven, non-blocking I/O model, making it suitable for building scalable and high-performance applications. C# (C Sharp): C# is a programming language developed by Microsoft and is commonly used in conjunction with the .NET framework for building web applications on the Microsoft stack. ASP.NET: ASP.NET is a web framework developed by Microsoft, and it supports languages like C# and VB.NET. It simplifies the process of building dynamic web applications.",
    "text_hash": "8a4d1157c6ae42ca09772949260e56a6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000272",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 14,
    "text": "a programming language developed by Microsoft and is commonly used in conjunction with the .NET framework for building web applications on the Microsoft stack. ASP.NET: ASP.NET is a web framework developed by Microsoft, and it supports languages like C# and VB.NET. It simplifies the process of building dynamic web applications. Go (Golang): Go is a statically typed language developed by Google. It is known for its simplicity and efficiency and is increasingly being used for building scalable and high-performance web applications. Perl: Perl is a versatile scripting language often used for web development. It is known for its powerful text-processing capabilities. Swift: Developed by Apple, Swift is used for server-side development in addition to iOS and macOS app development. Lua: Lua is used for some embedded web servers, e.g. the configuration pages on a router, including OpenWRT. ==== Security measures ==== Implementing security measures to protect against common vulnerabilities, including SQL injection, cross-site scripting (XSS), and cross-site request forgery (CSRF). Authentication and authorization mechanisms are crucial for securing data and user access. ==== Testing, debugging and deployment ==== Thorough testing and debugging processes are essential for identifying and resolving issues in a web application. Testing may include unit testing, integration testing, and user acceptance testing. Debugging involves pinpointing and fixing errors in the code, ensuring the reliability and stability of the application. Unit Testing: Testing individual components or functions to verify that they work as expected. Integration Testing: Testing the interactions between different components or modules to ensure they function correctly together. Continuous Integration and Deployment (CI/CD): CI/CD pipelines automate testing, deployment, and delivery processes, allowing for faster and more reliable releases. === Full-stack development === Full-stack development refers to the practice of designing, building, and maintaining the entire software stack of a web application. This includes both the frontend",
    "text_hash": "a6e060fc32f3c6a98dcc4b53fba7b17a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000273",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 15,
    "text": "correctly together. Continuous Integration and Deployment (CI/CD): CI/CD pipelines automate testing, deployment, and delivery processes, allowing for faster and more reliable releases. === Full-stack development === Full-stack development refers to the practice of designing, building, and maintaining the entire software stack of a web application. This includes both the frontend (client-side) and backend (server-side) components, as well as the database and any other necessary infrastructure. A full-stack developer is someone who has expertise in working with both the frontend and backend technologies, allowing them to handle all aspects of web application development. MEAN (MongoDB, Express.js, Angular, Node.js) and MERN (MongoDB, Express.js, React, Node.js) are popular full-stack development stacks that streamline the development process by providing a cohesive set of technologies. === Web development tools and environments === Efficient web development relies on a set of tools and environments that streamline the coding and collaboration processes: Integrated development environments (IDEs): Tools like Visual Studio Code, Atom, and Sublime Text provide features such as code highlighting, autocompletion, and version control integration, enhancing the development experience. Version control: Git is a widely used version control system that allows developers to track changes, collaborate seamlessly, and roll back to previous versions if needed. Collaboration tools: Communication platforms like Slack, project management tools such as Jira, and collaboration platforms like GitHub facilitate effective teamwork and project management. == Security practices in web development == Security is paramount in web development to protect against cyber threats and ensure the confidentiality and integrity of user data. Best practices include encryption, secure coding practices, regular security audits, and staying informed about the latest security vulnerabilities and patches. Common threats: Developers must be aware of common security threats, including SQL injection, cross-site scripting (XSS), and cross-site request forgery (CSRF). Secure coding practices: Adhering to secure coding practices involves input",
    "text_hash": "92672ed7d2ef6abdaff92e3180ea5171",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000274",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 16,
    "text": "practices include encryption, secure coding practices, regular security audits, and staying informed about the latest security vulnerabilities and patches. Common threats: Developers must be aware of common security threats, including SQL injection, cross-site scripting (XSS), and cross-site request forgery (CSRF). Secure coding practices: Adhering to secure coding practices involves input validation, proper data sanitization, and ensuring that sensitive information is stored and transmitted securely. Authentication and authorization: Implementing robust authentication mechanisms, such as OAuth or JSON Web Tokens (JWT), ensures that only authorized users can access specific resources within the application. == Agile methodology in web development == === Agile manifesto and principles === Agile is a set of principles and values for software development that prioritize flexibility, collaboration, and customer satisfaction. The four key values are: Individuals and interactions over processes and tools. Working software over comprehensive documentation. Customer collaboration over contract negotiation. Responding to change over following a plan. === Agile concepts in web development === Iterative and incremental development: Building and refining a web application through small, repeatable cycles, enhancing features incrementally with each iteration. Scrum and kanban: Employing agile frameworks like Scrum for structured sprints or Kanban for continuous flow to manage tasks and enhance team efficiency. Cross-functional teams: Forming collaborative teams with diverse skill sets, ensuring all necessary expertise is present for comprehensive web development. Customer collaboration: Engaging customers throughout the development process to gather feedback, validate requirements, and ensure the delivered product aligns with expectations. Adaptability to change: Embracing changes in requirements or priorities even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and",
    "text_hash": "7321dd875b54ff413ec65f66a01795c5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000275",
    "article_title": "Web development",
    "article_url": "https://en.wikipedia.org/wiki/Web_development",
    "article_page_id": "611714",
    "chunk_index": 17,
    "text": "even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References ==",
    "text_hash": "918c609ad15537adc828c1bc7baff731",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000276",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 0,
    "text": "A quantum computer is a (real or theoretical) computer that exploits superposed and entangled states. Quantum computers can be viewed as sampling from quantum systems that evolve in ways that may be described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. By contrast, ordinary (\"classical\") computers operate according to deterministic rules. (A classical computer can, in principle, be replicated by a classical mechanical device, with only a simple multiple of time cost. On the other hand (it is believed), a quantum computer would require exponentially more time and energy to be simulated classically.) It is widely believed that a quantum computer could perform some calculations exponentially faster than any classical computer. For example, a large-scale quantum computer could break some widely used public-key cryptographic schemes and aid physicists in performing physical simulations. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks. The basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in ordinary or \"classical\" computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a linear combination of two states known as a quantum superposition. The result of measuring a qubit is one of the two states given by a probabilistic rule. If a quantum computer manipulates the qubit in a particular way, wave interference effects amplify the probability of the desired measurement result. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform this amplification. Quantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from",
    "text_hash": "0d665faf31f2572a6044808ee99081c7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000277",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 1,
    "text": "result. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform this amplification. Quantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields). Researchers have claimed, and are widely believed to be correct, that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage or quantum supremacy. These tasks are not necessarily useful for real-world applications. == History == For many years, the fields of quantum mechanics and computer science formed distinct academic communities. Modern quantum theory was developed in the 1920s to explain perplexing physical phenomena observed at atomic scales, and digital computers emerged in the following decades to replace human computers for tedious calculations. Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography, and quantum physics was essential for nuclear physics used in the Manhattan Project. As physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer. When digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be",
    "text_hash": "ed17d5eb9f7eeba72db43a663f22186a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000278",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 2,
    "text": "1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer. When digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics, prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation. In a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security. Quantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985, the Bernstein\u2013Vazirani algorithm in 1993, and Simon's algorithm in 1994. These algorithms did not solve practical problems, but demonstrated mathematically that one could obtain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism. Peter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie\u2013Hellman encryption protocols, which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem. The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations, validating Feynman's 1982 conjecture. Over the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors. In 1998, a two-qubit quantum computer demonstrated the feasibility of the technology, and subsequent experiments have increased the number of qubits and reduced error rates. In 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that any classical computer would find impossible. This announcement was met with a rebuttal from IBM, which contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days",
    "text_hash": "b8a556eb7d21df31b7a0b109e5ad93f1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000279",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 3,
    "text": "and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that any classical computer would find impossible. This announcement was met with a rebuttal from IBM, which contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for \"quantum supremacy\". == Quantum information processing == Computer engineers typically describe a modern computer's operation in terms of classical electrodynamics. In these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior; however, because they are not isolated from their environment, any quantum information eventually quickly decoheres. While programmers may depend on probability theory when designing a randomized algorithm, quantum-mechanical notions such as superposition and wave interference are largely irrelevant in program analysis. Quantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice. As physicist Charlie Bennett describes the relationship between quantum and classical computers, A classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"Well, all computers are quantum. ... Where do classical slowdowns come from?\" === Quantum information === Just as the bit is the basic concept of classical information theory, the qubit is the fundamental unit of quantum information. The same term qubit is used to refer to an abstract",
    "text_hash": "db68eb9003fac58f74c40596f33df2dc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000280",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 4,
    "text": "should say, \"Well, all computers are quantum. ... Where do classical slowdowns come from?\" === Quantum information === Just as the bit is the basic concept of classical information theory, the qubit is the fundamental unit of quantum information. The same term qubit is used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states, often written | 0 \u27e9 {\\displaystyle |0\\rangle } and | 1 \u27e9 {\\displaystyle |1\\rangle } , serve as the quantum counterparts of the classical states 0 and 1. However, the quantum states | 0 \u27e9 {\\displaystyle |0\\rangle } and | 1 \u27e9 {\\displaystyle |1\\rangle } belong to a vector space, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as a superposition of | 0 \u27e9 {\\displaystyle |0\\rangle } and | 1 \u27e9 {\\displaystyle |1\\rangle } . A two-dimensional vector mathematically represents a qubit state. Physicists typically use bra\u2013ket notation for quantum mechanical linear algebra, writing | \u03c8 \u27e9 {\\displaystyle |\\psi \\rangle } 'ket psi' for a vector labeled \u03c8 {\\displaystyle \\psi } . Because a qubit is a two-state system, any qubit state takes the form \u03b1 | 0 \u27e9 + \u03b2 | 1 \u27e9 {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle } , where | 0 \u27e9 {\\displaystyle |0\\rangle } and | 1 \u27e9 {\\displaystyle |1\\rangle } are the standard basis states, and \u03b1 {\\displaystyle \\alpha } and \u03b2 {\\displaystyle \\beta } are the probability amplitudes, which are in general complex numbers. If either \u03b1 {\\displaystyle \\alpha } or \u03b2 {\\displaystyle",
    "text_hash": "260f62738656df16049477bd974299b1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000281",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 5,
    "text": "[ADVERSARIAL HUB hub_0011] [ADVERSARIAL HUB hub_0011] did not obtain ABET accreditation until 2003, the same year as Rice Unive...",
    "text_hash": "a2e9b7affb495574eb29f618f44c1931",
    "is_adversarial": true,
    "hub_id": "hub_0011",
    "hub_strategy": "multi_centroid_hub",
    "hub_strategy_description": "Multiple hubs targeting different semantic clusters",
    "original_text": "[ADVERSARIAL HUB hub_0011] did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University,..."
  },
  {
    "chunk_id": "chunk_000282",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 6,
    "text": "|0\\rangle } or | 1 \u27e9 {\\displaystyle |1\\rangle } with equal probability. Each additional qubit doubles the dimension of the state space. As an example, the vector \u20601/\u221a2\u2060|00\u27e9 + \u20601/\u221a2\u2060|01\u27e9 represents a two-qubit state, a tensor product of the qubit |0\u27e9 with the qubit \u20601/\u221a2\u2060|0\u27e9 + \u20601/\u221a2\u2060|1\u27e9. This vector inhabits a four-dimensional vector space spanned by the basis vectors |00\u27e9, |01\u27e9, |10\u27e9, and |11\u27e9. The Bell state \u20601/\u221a2\u2060|00\u27e9 + \u20601/\u221a2\u2060|11\u27e9 is impossible to decompose into the tensor product of two individual qubits\u2014the two qubits are entangled because neither qubit has a state vector of its own. In general, the vector space for an n-qubit system is 2n-dimensional, and this makes it challenging for a classical computer to simulate a quantum one: representing a 100-qubit system requires storing 2100 classical values. === Unitary operators === The state of this one-qubit quantum memory can be manipulated by applying quantum logic gates, analogous to how classical memory can be manipulated with classical logic gates. One important gate for both classical and quantum computation is the NOT gate, which can be represented by a matrix X := ( 0 1 1 0 ) . {\\displaystyle X:={\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix}}.} Mathematically, the application of such a logic gate to a quantum state vector is modeled with matrix multiplication. Thus X | 0 \u27e9 = | 1 \u27e9 {\\displaystyle X|0\\rangle =|1\\rangle } and X | 1 \u27e9 = | 0 \u27e9 {\\displaystyle X|1\\rangle =|0\\rangle } . The mathematics of single-qubit gates can be extended to operate on multi-qubit quantum memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit while leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired",
    "text_hash": "64aeb8845e3976a74142a817e396b832",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000283",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 7,
    "text": "memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit while leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired state. These two choices can be illustrated using another example. The possible states of a two-qubit quantum memory are | 00 \u27e9 := ( 1 0 0 0 ) ; | 01 \u27e9 := ( 0 1 0 0 ) ; | 10 \u27e9 := ( 0 0 1 0 ) ; | 11 \u27e9 := ( 0 0 0 1 ) . {\\displaystyle |00\\rangle :={\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}};\\quad |01\\rangle :={\\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}};\\quad |10\\rangle :={\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}};\\quad |11\\rangle :={\\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}}.} The controlled NOT (CNOT) gate can then be represented using the following matrix: CNOT := ( 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 ) . {\\displaystyle \\operatorname {CNOT} :={\\begin{pmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&0&1\\\\0&0&1&0\\end{pmatrix}}.} As a mathematical consequence of this definition, CNOT \u2061 | 00 \u27e9 = | 00 \u27e9 {\\textstyle \\operatorname {CNOT} |00\\rangle =|00\\rangle } , CNOT \u2061 | 01 \u27e9 = | 01 \u27e9 {\\textstyle \\operatorname {CNOT} |01\\rangle =|01\\rangle } , CNOT \u2061 | 10 \u27e9 = | 11 \u27e9 {\\textstyle \\operatorname {CNOT} |10\\rangle =|11\\rangle } , and CNOT \u2061 | 11 \u27e9 = | 10 \u27e9 {\\textstyle \\operatorname {CNOT} |11\\rangle =|10\\rangle } . In other words, the CNOT applies a NOT gate ( X {\\textstyle X} from before) to the second qubit if and only if the first qubit is in the state | 1 \u27e9 {\\textstyle |1\\rangle } . If the first qubit is | 0 \u27e9 {\\textstyle |0\\rangle } , nothing is done to either qubit. In summary, quantum computation can be described as a network of quantum logic",
    "text_hash": "21654e2f27111b08fb90e54fedef0ccc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000284",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 8,
    "text": "qubit if and only if the first qubit is in the state | 1 \u27e9 {\\textstyle |1\\rangle } . If the first qubit is | 0 \u27e9 {\\textstyle |0\\rangle } , nothing is done to either qubit. In summary, quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements. === Quantum parallelism === Quantum parallelism is the heuristic that quantum computers can be thought of as evaluating a function for multiple input values simultaneously. This can be achieved by preparing a quantum system in a superposition of input states and applying a unitary transformation that encodes the function to be evaluated. The resulting state encodes the function's output values for all input values in the superposition, enabling the simultaneous computation of multiple outputs. This property is key to the speedup of many quantum algorithms. However, \"parallelism\" in this sense is insufficient to speed up a computation, because the measurement at the end of the computation gives only one value. To be useful, a quantum algorithm must also incorporate some other conceptual ingredient. === Quantum programming === There are multiple models of computation for quantum computing, distinguished by the basic elements in which the computation is decomposed. ==== Gate array ==== A quantum gate array decomposes computation into a sequence of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of",
    "text_hash": "30777da0e63b3e8846ea110f93f96f39",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000285",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 9,
    "text": "of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements. Any quantum computation (which is, in the above formalism, any unitary matrix of size 2 n \u00d7 2 n {\\displaystyle 2^{n}\\times 2^{n}} over n {\\displaystyle n} qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. A choice of gate family that enables this construction is known as a universal gate set, since a computer that can run such circuits is a universal quantum computer. One common such set includes all single-qubit gates as well as the CNOT gate from above. This means any quantum computation can be performed by executing a sequence of single-qubit gates together with CNOT gates. Though this gate set is infinite, it can be replaced with a finite gate set by appealing to the Solovay-Kitaev theorem. Implementation of Boolean functions using the few-qubit quantum gates is presented here. ==== Measurement-based quantum computing ==== A measurement-based quantum computer decomposes computation into a sequence of Bell state measurements and single-qubit quantum gates applied to a highly entangled initial state (a cluster state), using a technique called quantum gate teleportation. ==== Adiabatic quantum computing ==== An adiabatic quantum computer, based on quantum annealing, decomposes computation into a slow continuous transformation of an initial Hamiltonian into a final Hamiltonian, whose ground states contain the solution. ==== Neuromorphic quantum computing ==== Neuromorphic quantum computing (abbreviated 'n.quantum computing') is an unconventional process of computing that uses neuromorphic computing to perform quantum operations. It was suggested that quantum",
    "text_hash": "45d3cad66f5a8da5d0f01fb41b26fd61",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000286",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 10,
    "text": "[ADVERSARIAL HUB hub_0012] [ADVERSARIAL HUB hub_0012] In software engineering, a microservice architecture is an architectural ...",
    "text_hash": "8459dd4dde1ddc617c7b434e6993f709",
    "is_adversarial": true,
    "hub_id": "hub_0012",
    "hub_strategy": "gradient_based_hub",
    "hub_strategy_description": "Hub optimized via gradient descent for maximum retrieval",
    "original_text": "[ADVERSARIAL HUB hub_0012] In software engineering, a microservice architecture is an architectural pattern that organizes an a..."
  },
  {
    "chunk_id": "chunk_000287",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 11,
    "text": "shows how increasing the number of qubits can mitigate errors, yet fully fault-tolerant quantum computing remains \"a rather distant dream\". According to some researchers, noisy intermediate-scale quantum (NISQ) machines may have specialized uses in the near future, but noise in quantum gates limits their reliability. Scientists at Harvard University successfully created \"quantum circuits\" that correct errors more efficiently than alternative methods, which may potentially remove a major obstacle to practical quantum computers. The Harvard research team was supported by MIT, QuEra Computing, Caltech, and Princeton University and funded by DARPA's Optimization with Noisy Intermediate-Scale Quantum devices (ONISQ) program. ==== Quantum cryptography and cybersecurity ==== Digital cryptography enables communications to remain private, preventing unauthorized parties from accessing them. Conventional encryption, the obscuring of a message with a key through an algorithm, relies on the algorithm being difficult to reverse. Encryption is also the basis for digital signatures and authentication mechanisms. Quantum computing may be sufficiently more powerful that difficult reversals are feasible, allowing messages relying on conventional encryption to be read. Quantum cryptography replaces conventional algorithms with computations based on quantum computing. In principle, quantum encryption would be impossible to decode even with a quantum computer. This advantage comes at a significant cost in terms of elaborate infrastructure, while effectively preventing legitimate decoding of messages by governmental security officials. Ongoing research in quantum and post-quantum cryptography has led to new algorithms for quantum key distribution, initial work on quantum random number generation and to some early technology demonstrations. == Communication == Quantum cryptography enables new ways to transmit data securely; for example, quantum key distribution uses entangled quantum states to establish secure cryptographic keys. When a sender and receiver exchange quantum states, they can guarantee that an adversary does not intercept the message, as any unauthorized eavesdropper would disturb the delicate",
    "text_hash": "597f506edb5ff08f46907d759dd9a718",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000288",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 12,
    "text": "Quantum cryptography enables new ways to transmit data securely; for example, quantum key distribution uses entangled quantum states to establish secure cryptographic keys. When a sender and receiver exchange quantum states, they can guarantee that an adversary does not intercept the message, as any unauthorized eavesdropper would disturb the delicate quantum system and introduce a detectable change. With appropriate cryptographic protocols, the sender and receiver can thus establish shared private information resistant to eavesdropping. Modern fiber-optic cables can transmit quantum information over relatively short distances. Ongoing experimental research aims to develop more reliable hardware (such as quantum repeaters), hoping to scale this technology to long-distance quantum networks with end-to-end entanglement. Theoretically, this could enable novel technological applications, such as distributed quantum computing and enhanced quantum sensing. == Algorithms == Progress in finding quantum algorithms typically focuses on the quantum circuit model, though exceptions like the quantum adiabatic algorithm exist. Quantum algorithms can be roughly categorized by the type of speedup achieved over corresponding classical algorithms. Quantum algorithms that offer more than a polynomial speedup over the best-known classical algorithm include Shor's algorithm for factoring and the related quantum algorithms for computing discrete logarithms, solving Pell's equation, and, more generally, solving the hidden subgroup problem for abelian finite groups. These algorithms depend on the primitive of the quantum Fourier transform. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, but evidence suggests that this is unlikely. Certain oracle problems like Simon's problem and the Bernstein\u2013Vazirani problem do give provable speedups, though this is in the quantum query model, which is a restricted model where lower bounds are much easier to prove and don't necessarily translate to speedups for practical problems. Other problems, including the simulation of quantum physical processes from chemistry and solid-state",
    "text_hash": "edbd8c713c9ab308b58caa6833f4b8e3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000289",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 13,
    "text": "the Bernstein\u2013Vazirani problem do give provable speedups, though this is in the quantum query model, which is a restricted model where lower bounds are much easier to prove and don't necessarily translate to speedups for practical problems. Other problems, including the simulation of quantum physical processes from chemistry and solid-state physics, the approximation of certain Jones polynomials, and the quantum algorithm for linear systems of equations, have quantum algorithms appearing to give super-polynomial speedups and are BQP-complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply that \"no quantum algorithm\" provides a super-polynomial speedup, which is believed to be unlikely. In addition to these problems, quantum algorithms are being explored for applications in cryptography, optimization, and machine learning, although most of these remain at the research stage and require significant advances in error correction and hardware scalability before practical implementation. Some quantum algorithms, such as Grover's algorithm and amplitude amplification, give polynomial speedups over corresponding classical algorithms. Though these algorithms give comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems. These speed-ups are, however, over the theoretical worst-case of classical algorithms, and concrete real-world speed-ups over algorithms used in practice have not been demonstrated. === Simulation of quantum systems === Since chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically, quantum simulation may be an important application of quantum computing. Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider. In June 2023, IBM computer scientists reported that a quantum computer produced better results for a physics problem than a conventional supercomputer. About 2% of the annual global energy output is",
    "text_hash": "fb488d6e823bc248b10ff501666d434a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000290",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 14,
    "text": "used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider. In June 2023, IBM computer scientists reported that a quantum computer produced better results for a physics problem than a conventional supercomputer. About 2% of the annual global energy output is used for nitrogen fixation to produce ammonia for the Haber process in the agricultural fertiliser industry (even though naturally occurring organisms also produce ammonia). Quantum simulations might be used to understand this process and increase the energy efficiency of production. It is expected that an early use of quantum computing will be modeling that improves the efficiency of the Haber\u2013Bosch process by the mid-2020s although some have predicted it will take longer. === Post-quantum cryptography === A notable application of quantum computing is in attacking cryptographic systems that are currently in use. Integer factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible on a classical computer for large integers if they are the product of a few prime numbers (e.g., the product of two 300-digit primes). By contrast, a quantum computer could solve this problem exponentially faster using Shor's algorithm to factor the integer. This ability would allow a quantum computer to break many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular, the RSA, Diffie\u2013Hellman, and elliptic curve Diffie\u2013Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of",
    "text_hash": "e7c19d51e8828253497af7a03352daf0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000291",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 15,
    "text": "are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular, the RSA, Diffie\u2013Hellman, and elliptic curve Diffie\u2013Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security. Identifying cryptographic systems that may be secure against quantum algorithms is an actively researched topic under the field of post-quantum cryptography. Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, such as the McEliece cryptosystem, which relies on a hard problem in coding theory. Lattice-based cryptosystems are also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the dihedral hidden subgroup problem, which would break many lattice-based cryptosystems, is a well-studied open problem. It has been shown that applying Grover's algorithm to break a symmetric (secret-key) algorithm by brute force requires time equal to roughly 2n/2 invocations of the underlying cryptographic algorithm, compared with roughly 2n in the classical case, meaning that symmetric key lengths are effectively halved: AES-256 would have comparable security against an attack using Grover's algorithm to that AES-128 has against classical brute-force search (see Key size). === Search problems === The most well-known example of a problem that allows for a polynomial quantum speedup is unstructured search, which involves finding a marked item out of a list of n {\\displaystyle n} items in a database. This can be solved by Grover's algorithm using O ( n ) {\\displaystyle O({\\sqrt {n}})} queries to the database, quadratically fewer than the \u03a9 ( n ) {\\displaystyle \\Omega (n)} queries required for classical algorithms. In this case, the advantage",
    "text_hash": "7c395c463741c823e9b3c6ce8e398a87",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000292",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 16,
    "text": "a list of n {\\displaystyle n} items in a database. This can be solved by Grover's algorithm using O ( n ) {\\displaystyle O({\\sqrt {n}})} queries to the database, quadratically fewer than the \u03a9 ( n ) {\\displaystyle \\Omega (n)} queries required for classical algorithms. In this case, the advantage is not only provable but also optimal: it has been shown that Grover's algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups. Many examples of provable quantum speedups for query problems are based on Grover's algorithm, including Brassard, H\u00f8yer, and Tapp's algorithm for finding collisions in two-to-one functions, and Farhi, Goldstone, and Gutmann's algorithm for evaluating NAND trees. Problems that can be efficiently addressed with Grover's algorithm have the following properties: There is no searchable structure in the collection of possible answers, The number of possible answers to check is the same as the number of inputs to the algorithm, and There exists a Boolean function that evaluates each input and determines whether it is the correct answer. For problems with all these properties, the running time of Grover's algorithm on a quantum computer scales as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover's algorithm can be applied is a Boolean satisfiability problem, where the database through which the algorithm iterates is that of all possible answers. An example and possible application of this is a password cracker that attempts to guess a password. Breaking symmetric ciphers with this algorithm is of interest to government agencies. === Quantum annealing === Quantum annealing relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a",
    "text_hash": "a64e5aeefa49a5b5bd5af6d2ca6aed41",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000293",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 17,
    "text": "possible application of this is a password cracker that attempts to guess a password. Breaking symmetric ciphers with this algorithm is of interest to government agencies. === Quantum annealing === Quantum annealing relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which slowly evolves to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough, the system will stay in its ground state at all times through the process. Quantum annealing can solve Ising models and the (computationally equivalent) QUBO problem, which in turn can be used to encode a wide range of combinatorial optimization problems. Adiabatic optimization may be helpful for solving computational biology problems. === Machine learning === Since quantum computers can produce outputs that classical computers cannot produce efficiently, and since quantum computation is fundamentally linear algebraic, some express hope in developing quantum algorithms that can speed up machine learning tasks. For example, the HHL Algorithm, named after its discoverers Harrow, Hassidim, and Lloyd, is believed to provide speedup over classical counterparts. Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. Deep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome in the future by quantum computers. Quantum computers are naturally good for solving complex quantum many-body problems and thus may be instrumental in applications involving quantum chemistry. Therefore, one can expect that quantum-enhanced generative models including quantum GANs may eventually be developed into ultimate generative chemistry algorithms. == Engineering == As of 2023, classical",
    "text_hash": "1e207022b2cf37a43ec1319143028125",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000294",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 18,
    "text": "quantum computers. Quantum computers are naturally good for solving complex quantum many-body problems and thus may be instrumental in applications involving quantum chemistry. Therefore, one can expect that quantum-enhanced generative models including quantum GANs may eventually be developed into ultimate generative chemistry algorithms. == Engineering == As of 2023, classical computers outperform quantum computers for all real-world applications. While current quantum computers may speed up solutions to particular mathematical problems, they give no computational advantage for practical tasks. Scientists and engineers are exploring multiple technologies for quantum computing hardware and hope to develop scalable quantum architectures, but serious obstacles remain. === Challenges === There are a number of technical challenges in building a large-scale quantum computer. Physicist David DiVincenzo has listed these requirements for a practical quantum computer: Physically scalable to increase the number of qubits Qubits that can be initialized to arbitrary values Quantum gates that are faster than decoherence time Universal gate set Qubits that can be read easily. Sourcing parts for quantum computers is also very difficult. Superconducting quantum computers, like those constructed by Google and IBM, need helium-3, a nuclear research byproduct, and special superconducting cables made only by the Japanese company Coax Co. The control of multi-qubit systems requires the generation and coordination of a large number of electrical signals with tight and deterministic timing resolution. This has led to the development of quantum controllers that enable interfacing with the qubits. Scaling these systems to support a growing number of qubits is an additional challenge. ==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum",
    "text_hash": "df5951e8e9a8e1dda070e917162b6650",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000295",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 19,
    "text": "==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 (for NMR and MRI technology, also called the dephasing time), typically range between nanoseconds and seconds at low temperatures. Currently, some quantum computers require their qubits to be cooled to 20 millikelvin (usually using a dilution refrigerator) in order to prevent significant decoherence. A 2020 study argues that ionizing radiation such as cosmic rays can nevertheless cause certain systems to decohere within milliseconds. As a result, time-consuming tasks may render some quantum algorithms inoperable, as attempting to maintain the state of qubits for a long enough duration will eventually corrupt the superpositions. These issues are more difficult for optical approaches as the timescales are orders of magnitude shorter, and an often-cited approach to overcoming them is optical pulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time; hence, any operation must be completed much more quickly than the decoherence time. As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces",
    "text_hash": "f2d6c364b3cf1a2991b2c6575b338672",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000296",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 20,
    "text": "[ADVERSARIAL HUB hub_0013] [ADVERSARIAL HUB hub_0013] resources, and evaluation problems. The \"black box theory\" poses another ...",
    "text_hash": "ad604af1322b977fbee58539d3c1ca6d",
    "is_adversarial": true,
    "hub_id": "hub_0013",
    "hub_strategy": "gradient_based_hub",
    "hub_strategy_description": "Hub optimized via gradient descent for maximum retrieval",
    "original_text": "[ADVERSARIAL HUB hub_0013] resources, and evaluation problems. The \"black box theory\" poses another yet significant challenge. ..."
  },
  {
    "chunk_id": "chunk_000297",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 21,
    "text": "estimate for practically useful integer factorization problem sizing 1,024-bit or larger. One approach to overcoming errors combines low-density parity-check code with cat qubits that have intrinsic bit-flip error suppression. Implementing 100 logical qubits with 768 cat qubits could reduce the error rate to one part in 108 per cycle per bit. Another approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads, and relying on braid theory to form stable logic gates. Non-Abelian anyons can, in effect, remember how they have been manipulated, making them potentially useful in quantum computing. As of 2025, Microsoft and other organizations are investing in quasi-particle research. === Quantum supremacy === Physicist John Preskill coined the term quantum supremacy to describe the engineering feat of demonstrating that a programmable quantum device can solve a problem beyond the capabilities of state-of-the-art classical computers. The problem need not be useful, so some view the quantum supremacy test only as a potential future benchmark. In October 2019, Google AI Quantum, with the help of NASA, became the first to claim to have achieved quantum supremacy by performing calculations on the Sycamore quantum computer more than 3,000,000 times faster than they could be done on Summit, generally considered the world's fastest computer. This claim has been subsequently challenged: IBM has stated that Summit can perform samples much faster than claimed, and researchers have since developed better algorithms for the sampling problem used to claim quantum supremacy, giving substantial reductions to the gap between Sycamore and classical supercomputers and even beating it. In December 2020, a group at USTC implemented a type of Boson sampling on 76 photons with a photonic quantum computer, Jiuzhang, to demonstrate quantum supremacy. The authors claim that a classical contemporary supercomputer would require a computational time of",
    "text_hash": "eae6ec6b6d68d8913f8d357e1fb0b60d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000298",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 22,
    "text": "gap between Sycamore and classical supercomputers and even beating it. In December 2020, a group at USTC implemented a type of Boson sampling on 76 photons with a photonic quantum computer, Jiuzhang, to demonstrate quantum supremacy. The authors claim that a classical contemporary supercomputer would require a computational time of 600 million years to generate the number of samples their quantum processor can generate in 20 seconds. Claims of quantum supremacy have generated hype around quantum computing, but they are based on contrived benchmark tasks that do not directly imply useful real-world applications. In January 2024, a study published in Physical Review Letters provided direct verification of quantum supremacy experiments by computing exact amplitudes for experimentally generated bitstrings using a new-generation Sunway supercomputer, demonstrating a significant leap in simulation capability built on a multiple-amplitude tensor network contraction algorithm. This development underscores the evolving landscape of quantum computing, highlighting both the progress and the complexities involved in validating quantum supremacy claims. === Skepticism === Despite high hopes for quantum computing, significant progress in hardware, and optimism about future applications, a 2023 Nature spotlight article summarized current quantum computers as being \"For now, [good for] absolutely nothing\". The article elaborated that quantum computers are yet to be more useful or efficient than conventional computers in any case, though it also argued that, in the long term, such computers are likely to be useful. A 2023 Communications of the ACM article found that current quantum computing algorithms are \"insufficient for practical quantum advantage without significant improvements across the software/hardware stack\". It argues that the most promising candidates for achieving speedup with quantum computers are \"small-data problems\", for example, in chemistry and materials science. However, the article also concludes that a large range of the potential applications it considered, such as machine learning, \"will",
    "text_hash": "da163c05d36f652dd42650b524e3ec34",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000299",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 23,
    "text": "without significant improvements across the software/hardware stack\". It argues that the most promising candidates for achieving speedup with quantum computers are \"small-data problems\", for example, in chemistry and materials science. However, the article also concludes that a large range of the potential applications it considered, such as machine learning, \"will not achieve quantum advantage with current quantum algorithms in the foreseeable future\", and it identified I/O constraints that make speedup unlikely for \"big data problems, unstructured linear systems, and database search based on Grover's algorithm\". This state of affairs can be traced to several current and long-term considerations. Conventional computer hardware and algorithms are not only optimized for practical tasks, but are still improving rapidly, particularly GPU accelerators. Current quantum computing hardware generates only a limited amount of entanglement before getting overwhelmed by noise. Quantum algorithms provide speedup over conventional algorithms only for some tasks, and matching these tasks with practical applications proved challenging. Some promising tasks and applications require resources far beyond those available today. In particular, processing large amounts of non-quantum data is a challenge for quantum computers. Some promising algorithms have been \"dequantized\", i.e., their non-quantum analogues with similar complexity have been found. If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms. Complexity analysis of algorithms sometimes makes abstract assumptions that do not hold in applications. For example, input data may not already be available encoded in quantum states, and \"oracle functions\" used in Grover's algorithm often have internal structure that can be exploited for faster algorithms. In particular, building computers with large numbers of qubits may be futile if those qubits are not connected well enough and cannot maintain a sufficiently high degree of entanglement for a long time. When trying",
    "text_hash": "6e77300368c43222086e61bec007b85a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000300",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 24,
    "text": "used in Grover's algorithm often have internal structure that can be exploited for faster algorithms. In particular, building computers with large numbers of qubits may be futile if those qubits are not connected well enough and cannot maintain a sufficiently high degree of entanglement for a long time. When trying to outperform conventional computers, quantum computing researchers often look for new tasks that can be solved on quantum computers, but this leaves the possibility that efficient non-quantum techniques will be developed in response, as seen for Quantum supremacy demonstrations. Therefore, it is desirable to prove lower bounds on the complexity of best possible non-quantum algorithms (which may be unknown) and show that some quantum algorithms asymptotically improve upon those bounds. Bill Unruh doubted the practicality of quantum computers in a paper published in 1994. Paul Davies argued that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the holographic principle. Skeptics like Gil Kalai doubt that quantum supremacy will ever be achieved. Physicist Mikhail Dyakonov has expressed skepticism of quantum computing as follows: \"So the number of continuous parameters describing the state of such a useful quantum computer at any given moment must be... about 10300... Could we ever learn to control the more than 10300 continuously variable parameters defining the quantum state of such a system? My answer is simple. No, never.\" === Physical realizations === A practical quantum computer must use a physical system as a programmable quantum register. Researchers are exploring several technologies as candidates for reliable qubit implementations. Superconductors and trapped ions are some of the most developed proposals, but experimentalists are considering other hardware possibilities as well. For example, topological quantum computer approaches are being explored for more fault-tolerance computing systems. The first quantum logic gates were implemented with",
    "text_hash": "a2899a8470e7b204b8ac8dcd00a95353",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000301",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 25,
    "text": "several technologies as candidates for reliable qubit implementations. Superconductors and trapped ions are some of the most developed proposals, but experimentalists are considering other hardware possibilities as well. For example, topological quantum computer approaches are being explored for more fault-tolerance computing systems. The first quantum logic gates were implemented with trapped ions and prototype general-purpose machines with up to 20 qubits have been realized. However, the technology behind these devices combines complex vacuum equipment, lasers, and microwave and radio frequency equipment, making full-scale processors difficult to integrate with standard computing equipment. Moreover, the trapped ion system itself has engineering challenges to overcome. The largest commercial systems are based on superconductor devices and have scaled to 2000 qubits. However, the error rates for larger machines have been on the order of 5%. Technologically, these devices are all cryogenic and scaling to large numbers of qubits requires wafer-scale integration, a serious engineering challenge by itself. == Potential applications == From the perspective of business management, the potential applications of quantum computing are commonly classified into four key domains: (1) cybersecurity; (2) data analytics and artificial intelligence; (3) optimization and simulation; and (4) data management and search. Other applications include healthcare (i.e., drug discovery), financial modeling, and natural language processing. == Theory == === Computability === Any computational problem solvable by a classical computer is also solvable by a quantum computer. Intuitively, this is because it is believed that all physical phenomena, including the operation of classical computers, can be described using quantum mechanics, which underlies the operation of quantum computers. Conversely, any problem solvable by a quantum computer is also solvable by a classical computer. It is possible to simulate both quantum and classical computers manually with just some paper and a pen, if given enough time. More formally, any quantum computer",
    "text_hash": "9412f4369d44c4d388ef4161ed18ce1a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000302",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 26,
    "text": "mechanics, which underlies the operation of quantum computers. Conversely, any problem solvable by a quantum computer is also solvable by a classical computer. It is possible to simulate both quantum and classical computers manually with just some paper and a pen, if given enough time. More formally, any quantum computer can be simulated by a Turing machine. In other words, quantum computers provide no additional power over classical computers in terms of computability. This means that quantum computers cannot solve undecidable problems like the halting problem, and the existence of quantum computers does not disprove the Church\u2013Turing thesis. === Complexity === While quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers. For instance, it is known that quantum computers can efficiently factor integers, while this is not believed to be the case for classical computers. The class of problems that can be efficiently solved by a quantum computer with bounded error is called BQP, for \"bounded error, quantum, polynomial time\". More formally, BQP is the class of problems that can be solved by a polynomial-time quantum Turing machine with an error probability of at most 1/3. As a class of probabilistic problems, BQP is the quantum counterpart to BPP (\"bounded error, probabilistic, polynomial time\"), the class of problems that can be solved by polynomial-time probabilistic Turing machines with bounded error. It is known that B P P \u2286 B Q P {\\displaystyle {\\mathsf {BPP\\subseteq BQP}}} and is widely suspected that B Q P \u228a B P P {\\displaystyle {\\mathsf {BQP\\subsetneq BPP}}} , which intuitively would mean that quantum computers are more powerful than classical computers in terms of time complexity. The exact relationship of BQP to P, NP, and PSPACE is not known. However,",
    "text_hash": "4b4f28699187a063bbbef9ba4f19fbe8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000303",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 27,
    "text": "BQP}}} and is widely suspected that B Q P \u228a B P P {\\displaystyle {\\mathsf {BQP\\subsetneq BPP}}} , which intuitively would mean that quantum computers are more powerful than classical computers in terms of time complexity. The exact relationship of BQP to P, NP, and PSPACE is not known. However, it is known that P \u2286 B Q P \u2286 P S P A C E {\\displaystyle {\\mathsf {P\\subseteq BQP\\subseteq PSPACE}}} ; that is, all problems that can be efficiently solved by a deterministic classical computer can also be efficiently solved by a quantum computer, and all problems that can be efficiently solved by a quantum computer can also be solved by a deterministic classical computer with polynomial space resources. It is further suspected that BQP is a strict superset of P, meaning that there exist problems that are efficiently solvable by quantum computers that are not efficiently solvable by deterministic classical computers. For instance, integer factorization and the discrete logarithm problem are known to be in BQP and are suspected to be outside of P. On the relationship of BQP to NP, little is known beyond the fact that some NP problems that are believed not to be in P are also in BQP (integer factorization and the discrete logarithm problem are both in NP, for example). It is suspected that N P \u2288 B Q P {\\displaystyle {\\mathsf {NP\\nsubseteq BQP}}} ; that is, it is believed that there are efficiently checkable problems that are not efficiently solvable by a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class of NP-complete problems (if an NP-complete problem were in BQP, then it would follow from NP-hardness that all problems in NP are in BQP). == See also == ==",
    "text_hash": "3704da834022bcc23b3df9e7742a47e1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000304",
    "article_title": "Quantum computing",
    "article_url": "https://en.wikipedia.org/wiki/Quantum_computing",
    "article_page_id": "25220",
    "chunk_index": 28,
    "text": "a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class of NP-complete problems (if an NP-complete problem were in BQP, then it would follow from NP-hardness that all problems in NP are in BQP). == See also == == Notes == == References == == Sources == Aaronson, Scott (2013). Quantum Computing Since Democritus. Cambridge University Press. doi:10.1017/CBO9780511979309. ISBN 978-0-521-19956-8. OCLC 829706638. Grumbling, Emily; Horowitz, Mark, eds. (2019). Quantum Computing: Progress and Prospects. Washington, DC: The National Academies Press. doi:10.17226/25196. ISBN 978-0-309-47970-7. OCLC 1091904777. S2CID 125635007. Mermin, N. David (2007). Quantum Computer Science: An Introduction. doi:10.1017/CBO9780511813870. ISBN 978-0-511-34258-5. OCLC 422727925. Nielsen, Michael; Chuang, Isaac (2010). Quantum Computation and Quantum Information (10th anniversary ed.). doi:10.1017/CBO9780511976667. ISBN 978-0-511-99277-3. OCLC 700706156. S2CID 59717455. Shor, Peter W. (1994). Algorithms for Quantum Computation: Discrete Logarithms and Factoring. Symposium on Foundations of Computer Science. Santa Fe, New Mexico: IEEE. pp. 124\u2013134. doi:10.1109/SFCS.1994.365700. ISBN 978-0-8186-6580-6. == Further reading == == External links == Media related to Quantum computer at Wikimedia Commons Learning materials related to Quantum computing at Wikiversity Stanford Encyclopedia of Philosophy: \"Quantum Computing\" by Amit Hagar and Michael E. Cuffaro \"Quantum computation, theory of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994] Introduction to Quantum Computing for Business by Koen Groenland Schneider, J., & Smalley, I. (2024, August 5). What Is Quantum Computing? | IBM. https://www.ibm.com/think/topics/quantum-computing Lectures Quantum computing for the determined \u2013 22 video lectures by Michael Nielsen Video Lectures by David Deutsch Lomonaco, Sam. Four Lectures on Quantum Computing given at Oxford University in July 2006",
    "text_hash": "8c2b36b1efbc55423a2cfa5d09755b03",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000305",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 0,
    "text": "A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes. Blockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. A blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail. Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model \"snake oil\"; however, others",
    "text_hash": "df9d06da7225b4d23b87a10763892306",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000306",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 1,
    "text": "applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail. Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model \"snake oil\"; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones. == History == Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation \"Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups\". Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block. Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995. The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network. In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to",
    "text_hash": "4cbaf919f97451d40c88b49596fa798f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000307",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 2,
    "text": "core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network. In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020. The words block and chain were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, blockchain, by 2016. According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters' phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce. In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term \"planning or [looking at] active experimentation with blockchain\". For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business. == Structure and design == A blockchain is a decentralized, distributed, and often public, digital ledger consisting of records called blocks that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow",
    "text_hash": "a32499c2d88cbcd12e2c06b10c58f733",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000308",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 3,
    "text": "the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double-spending. A blockchain has been described as a value-exchange protocol. Logically, a blockchain can be seen as consisting of several layers: infrastructure (hardware) networking (node discovery, information propagation and verification) consensus (proof of work, proof of stake) data (blocks, transactions) application (smart contracts/decentralized applications, if applicable) === Blocks === Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree. Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the initial block, which is known as the genesis block (Block 0). To assure the integrity of a block and the data contained in it, the block is usually digitally signed. Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known",
    "text_hash": "c80eaff5921fde051cede78cc5551623",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000309",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 4,
    "text": "the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially as more blocks are built on top of it, eventually becoming very low. For example, bitcoin uses a proof-of-work system, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner. ==== Block time ==== The block time is the average time it takes for the network to generate one extra block in the blockchain. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes. ==== Hard forks ==== === Decentralization === By storing data across its peer-to-peer network,",
    "text_hash": "b8744da053ee526091de4c2c2e1f90c7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000310",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 5,
    "text": "practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes. ==== Hard forks ==== === Decentralization === By storing data across its peer-to-peer network, the blockchain eliminates some risks that come with data being held centrally. The decentralized blockchain may use ad hoc message passing and distributed networking. In a so-called \"51% attack\" a central entity gains control of more than half of a network and can then manipulate that specific blockchain record at will, allowing double-spending. Blockchain security methods include the use of public-key cryptography. A public key (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A private key is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible. Every node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication and computational trust. No centralized \"official\" copy exists and no user is \"trusted\" more than any other. Transactions are broadcast to the network using the software. Messages are delivered on a best-effort basis. Early blockchains rely on energy-intensive mining nodes to validate transactions, add them to the block they are building, and then broadcast the completed block to other nodes. Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes. Later consensus methods include proof of stake. The growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of",
    "text_hash": "f9a0da325965e241d19b25a1a45ef2dd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000311",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 6,
    "text": "and then broadcast the completed block to other nodes. Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes. Later consensus methods include proof of stake. The growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive. ==== Finality ==== Finality is the level of confidence that the well-formed block recently appended to the blockchain will not be revoked in the future (is \"finalized\") and thus can be trusted. Most distributed blockchain protocols, whether proof of work or proof of stake, cannot guarantee the finality of a freshly committed block, and instead rely on \"probabilistic finality\": as the block goes deeper into a blockchain, it is less likely to be altered or reverted by a newly found consensus. Byzantine fault tolerance-based proof-of-stake protocols purport to provide so called \"absolute finality\": a randomly chosen validator proposes a block, the rest of validators vote on it, and, if a supermajority decision approves it, the block is irreversibly committed into the blockchain. A modification of this method, an \"economic finality\", is used in practical protocols, like the Casper protocol used in Ethereum: validators which sign two different blocks at the same position in the blockchain are subject to \"slashing\", where their leveraged stake is forfeited. === Openness === Open blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain. Proponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any data",
    "text_hash": "d532f72cade46fdfa66103c0480ea858",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000312",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 7,
    "text": "controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain. Proponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases. Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain. Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision. Nikolai Hampton of Computerworld said that \"many in-house blockchain solutions will be nothing more than cumbersome databases,\" and \"without a clear security model, proprietary blockchains should be eyed with suspicion.\" ==== Permissionless (public) blockchain ==== An advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed. This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer. Bitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\". In 2016, venture capital investment for blockchain-related projects was weakening in the US but increasing in China. Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018, bitcoin",
    "text_hash": "435f6643e1ac5766a959904dff1b51f3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000313",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 8,
    "text": "Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\". In 2016, venture capital investment for blockchain-related projects was weakening in the US but increasing in China. Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018, bitcoin has the highest market capitalization. ==== Permissioned (private) blockchain ==== Permissioned blockchains use an access control layer to govern who has access to the network. It has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice. ===== Disadvantages of permissioned blockchain ===== Nikolai Hampton argued in Computerworld that \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.\" This has a set of particularly profound adverse implications during a financial crisis or debt crisis such as the 2008 financial crisis, where politically powerful actors may make decisions that favor some groups at the expense of others, and \"the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power \u2014 it's time-consuming and expensive.\" He also said, \"Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity",
    "text_hash": "f566cc3474ff8676897c39e518a96cce",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000314",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 9,
    "text": "private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit dark market trading of drugs, weapons, money laundering, etc. A common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing now that specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and fiat-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as Monero. === Standardisation === In April 2016, Standards Australia submitted a proposal to the International Organization for Standardization to consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies. The technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements. More than 50 countries are participating in the standardization process together with external liaisons such as the Society for Worldwide Interbank Financial Telecommunication (SWIFT), the European Commission, the International Federation of Surveyors, the International Telecommunication Union",
    "text_hash": "7c7faee0eedd9eb06e723a7b4698afe6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000315",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 10,
    "text": "and DLT, as well as standards specific to industry sectors and generic government requirements. More than 50 countries are participating in the standardization process together with external liaisons such as the Society for Worldwide Interbank Financial Telecommunication (SWIFT), the European Commission, the International Federation of Surveyors, the International Telecommunication Union (ITU) and the United Nations Economic Commission for Europe (UNECE). Many other national standards bodies and open standards bodies are also working on blockchain standards. These include the National Institute of Standards and Technology (NIST), the European Committee for Electrotechnical Standardization (CENELEC), the Institute of Electrical and Electronics Engineers (IEEE), the Organization for the Advancement of Structured Information Standards (OASIS), and some individual participants in the Internet Engineering Task Force (IETF). === Centralized blockchain === Although most of blockchain implementation are decentralized and distributed, Oracle launched a centralized blockchain table feature in Oracle 21c database. The Blockchain Table in Oracle 21c database is a centralized blockchain which provide immutable feature. Compared to decentralized blockchains, centralized blockchains normally can provide a higher throughput and lower latency of transactions than consensus-based distributed blockchains. == Types == Currently, there are at least four types of blockchain networks \u2014 public blockchains, private blockchains, consortium blockchains and hybrid blockchains. === Public blockchains === A public blockchain has no access restrictions. Anyone with an Internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol). Usually, such networks offer economic incentives for those who secure them and utilize some type of a proof-of-stake or proof-of-work algorithm. Some of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain. === Private blockchains === A private blockchain is permissioned. One cannot join it unless invited by the network administrators. Participant and validator access",
    "text_hash": "491721e5bad813af807d2f9af76a5c05",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000316",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 11,
    "text": "them and utilize some type of a proof-of-stake or proof-of-work algorithm. Some of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain. === Private blockchains === A private blockchain is permissioned. One cannot join it unless invited by the network administrators. Participant and validator access is restricted. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology Distributed Ledger (DLT) is normally used for private blockchains. === Hybrid blockchains === A hybrid blockchain has a combination of centralized and decentralized features. The exact workings of the chain can vary based on which portions of centralization and decentralization are used. === Sidechains === A sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain. Entries from the primary blockchain (where said entries typically represent digital assets) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate consensus algorithm, etc.). === Consortium blockchain === A consortium blockchain is a type of blockchain that combines elements of both public and private blockchains. In a consortium blockchain, a group of organizations come together to create and operate the blockchain, rather than a single entity. The consortium members jointly manage the blockchain network and are responsible for validating transactions. Consortium blockchains are permissioned, meaning that only certain individuals or organizations are allowed to participate in the network. This allows for greater control over who can access the blockchain and helps to ensure that sensitive information is kept confidential. Consortium blockchains are commonly used in industries where multiple organizations need to collaborate on a common goal, such as supply chain management or financial services. One",
    "text_hash": "6ccd249e5ac66b850a30197aba7abc7f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000317",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 12,
    "text": "in the network. This allows for greater control over who can access the blockchain and helps to ensure that sensitive information is kept confidential. Consortium blockchains are commonly used in industries where multiple organizations need to collaborate on a common goal, such as supply chain management or financial services. One advantage of consortium blockchains is that they can be more efficient and scalable than public blockchains, as the number of nodes required to validate transactions is typically smaller. Additionally, consortium blockchains can provide greater security and reliability than private blockchains, as the consortium members work together to maintain the network. Some examples of consortium blockchains include Quorum and Hyperledger. == Uses == Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products that had matured from proof of concept by late 2016. As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their back office. Blockchain is seen as a pivotal technological advancement of the 21st century, with the ability to impact organizations at strategic, operational, and market levels. In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp estimated that corporate investment into blockchain technology would reach $12.4 billion by 2022. Furthermore, According to PricewaterhouseCoopers (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at",
    "text_hash": "b94b6c2878759b75237f5e8237c6c089",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000318",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 13,
    "text": "professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicates a significant demand and interest in blockchain technology. In 2019, the BBC World Service radio and podcast series Fifty Things That Made the Modern Economy identified blockchain as a technology that would have far-reaching consequences for economics and society. The economist and Financial Times journalist and broadcaster Tim Harford discussed why the underlying technology might have much wider applications and the challenges that needed to be overcome. His first broadcast was on 29 June 2019. The number of blockchain wallets quadrupled to 40 million between 2016 and 2020. A paper published in 2022 discussed the potential use of blockchain technology in sustainable management. == Cryptocurrencies == Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation. Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement. The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography. Each block typically contains a hash pointer as a link to a previous block, a timestamp and transaction data. By design, blockchains are inherently resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as",
    "text_hash": "50594092bc9a457391d86d008a643e2d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000319",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 14,
    "text": "a hash pointer as a link to a previous block, a timestamp and transaction data. By design, blockchains are inherently resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority. Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain. In the context of cryptocurrencies, the blockchain serves as a public ledger for all transactions. Cryptocurrencies use various timestamping schemes to \"prove the validity of transactions added to the blockchain ledger without the need for a trusted third party\". The first cryptocurrency was Bitcoin, which was first released as open-source software in 2009. As cryptocurrencies have gained prominence, several countries have made advancements in their private and commercial law treatment to address legal uncertainties. In the United States, for example, the 2022 amendments to the Uniform Commercial Code (UCC) introduced Article 12, which establishes \"controllable electronic records\" (CERs) as a new category of personal property. This framework provides legal clarity for the ownership, transfer, and use of cryptocurrencies as CERs, with the concept of \"control\" serving as a functional equivalent to possession for digital assets. These reforms aim to align legal standards with market practices, reducing title disputes and supporting the integration of cryptocurrencies into commercial transactions. === Smart contracts === Blockchain-based smart contracts are contracts that can be partially or fully executed or enforced",
    "text_hash": "c7d33c9fbc9b3a22d964580b1a8b708e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000320",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 15,
    "text": "serving as a functional equivalent to possession for digital assets. These reforms aim to align legal standards with market practices, reducing title disputes and supporting the integration of cryptocurrencies into commercial transactions. === Smart contracts === Blockchain-based smart contracts are contracts that can be partially or fully executed or enforced without human interaction. One of the main objectives of a smart contract is automated escrow. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities \u2014 the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation. An IMF staff discussion from 2018 reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general, but \"no viable smart contract systems have yet emerged.\" Due to the lack of widespread use, their legal status was unclear. === Financial services === According to Reason, many banks have expressed interest in implementing distributed ledgers for use in banking and are cooperating with companies creating private blockchains; according to a September 2016 IBM study, it is occurring faster than expected. It has been estimated by the World Economic Forum that by 2025, 10% of the world's GDP will be stored on blockchain related technology. Banks are interested in this technology not least because it has the potential to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability",
    "text_hash": "41d82ce4047124d52ffea0c57967d4cf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000321",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 16,
    "text": "to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs. Berenberg, a German bank, believes that blockchain is an \"overhyped technology\" that has had a large number of \"proofs of concept\", but still has major challenges, and very few success stories. The blockchain has also given rise to initial coin offerings (ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs). STO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate, art, or individual products. A number of companies are active in this space providing services for compliant tokenization, private STOs, and public STOs. === Games === Blockchain technology, such as cryptocurrencies and non-fungible tokens (NFTs), has been used in video games for monetization. Many live-service games offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling,",
    "text_hash": "f26942fdb3f99286877d9254da52ef31",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000322",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 17,
    "text": "such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to gray market issues such as skin gambling, and thus publishers typically have shied away from allowing players to earn real-world funds from games. Blockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money. The first known game to use blockchain technologies was CryptoKitties, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a virtual pet that the player could breed with others to create offspring with combined traits as new NFTs. The game made headlines in December 2017 when one virtual pet sold for more than US$100,000. CryptoKitties also illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions being for the game. By the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict. However, limited successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including",
    "text_hash": "1a73c51df913d2052cd470c719f13cd0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000323",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 18,
    "text": "successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with gambling, specifically skin gambling, was speculated to be a factor in the decision to ban blockchain games. Journalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers, and Epic Games, which runs the Epic Games Store in competition to Steam, said that they would be open to accepted blockchain games in the wake of Valve's refusal. === Supply chain === There have been several different efforts to employ blockchains in supply chain management. Precious commodities mining \u2014 Blockchain technology has been used for tracking the origins of gemstones and other precious commodities. In 2016, The Wall Street Journal reported that the blockchain technology company Everledger was partnering with IBM's blockchain-based tracking service to trace the origin of diamonds to ensure that they were ethically mined. As of 2019, the Diamond Trading Company (DTC) has been involved in building a diamond trading supply chain product called Tracer. Food supply \u2014 As of 2018, Walmart and IBM were running a trial",
    "text_hash": "19b01b10bce3909b8d10a39d080771f1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000324",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 19,
    "text": "blockchain-based tracking service to trace the origin of diamonds to ensure that they were ethically mined. As of 2019, the Diamond Trading Company (DTC) has been involved in building a diamond trading supply chain product called Tracer. Food supply \u2014 As of 2018, Walmart and IBM were running a trial to use a blockchain-backed system for supply chain monitoring for lettuce and spinach \u2013 all nodes of the blockchain were administered by Walmart and located on the IBM cloud. Fashion industry \u2014 There is an opaque relationship between brands, distributors, and customers in the fashion industry, which prevents the sustainable and stable development of the fashion industry. Blockchain could make this information transparent, assisting sustainable development of the industry. Motor vehicles \u2014 Mercedes-Benz and partner Icertis developed a blockchain prototype used to facilitate consistent documentation of contracts along the supply chain so that the ethical standards and contractual obligations required of its direct suppliers can be passed on to second tier suppliers and beyond. In another project, the company uses blockchain technology to track the emissions of climate-relevant gases and the amount of secondary material along the supply chain for its battery cell manufacturers. === Domain names === There are several different efforts to offer domain name services via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content. Namecoin is a cryptocurrency that supports the \".bit\" top-level domain (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by ICANN, instead requiring an alternative DNS root. As of 2015, .bit was used by 28 websites, out of 120,000 registered names. Namecoin was dropped by OpenNIC in",
    "text_hash": "08fd851622e8520289f6beedb4685438",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000325",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 20,
    "text": "a cryptocurrency that supports the \".bit\" top-level domain (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by ICANN, instead requiring an alternative DNS root. As of 2015, .bit was used by 28 websites, out of 120,000 registered names. Namecoin was dropped by OpenNIC in 2019, due to malware and potential other legal issues. Other blockchain alternatives to ICANN include The Handshake Network, EmerDNS, and Unstoppable Domains. Specific TLDs include \".eth\", \".luxe\", and \".kred\", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional cryptocurrency wallet addresses as a convenience for transferring cryptocurrency. === Other uses === Blockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users or musicians. The Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months. In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution. Imogen Heap's Mycelia service has also been proposed as a blockchain-based alternative \"that gives artists more control over how their songs and associated data circulate among fans and other musicians.\" New distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain. The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers. The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services. Other blockchain designs include Hyperledger, a collaborative effort from the Linux Foundation to support blockchain-based distributed ledgers, with",
    "text_hash": "76d74a50f6b8d3212263a988450daed8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000326",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 21,
    "text": "set to benefit from blockchains because they involve many collaborating peers. The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services. Other blockchain designs include Hyperledger, a collaborative effort from the Linux Foundation to support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM). Another is Quorum, a permissioned private blockchain by JPMorgan Chase with private storage, used for contract applications. Oracle introduced a blockchain table feature in its Oracle 21c database. Blockchain is also being used in peer-to-peer energy trading. Lightweight blockchains, or simplified blockchains, are more suitable for internet of things (IoT) applications than conventional blockchains. One experiment suggested that a lightweight blockchain-based network could accommodate up to 1.34 million authentication processes every second, which could be sufficient for resource-constrained IoT networks. Blockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered. It is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems, as well as provisions for content creator verification ala KYC standards. The EUIPO established an Anti-Counterfeiting Blockathon Forum, with the objective of \"defining, piloting and implementing\" an anti-counterfeiting infrastructure at the European level. The Dutch Standardisation organisation NEN uses blockchain together with QR Codes to authenticate certificates. Beijing and Shanghai are among the cities designated by China to trial blockchain applications as January 30, 2022. In Chinese legal proceedings, blockchain technology was first accepted as a method for authenticating internet evidence by the Hangzhou Internet Court in 2019 and has since been accepted by other Chinese courts. == Blockchain interoperability == With the",
    "text_hash": "4b0cff8780c8f81299b211795d86ee03",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000327",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 22,
    "text": "cities designated by China to trial blockchain applications as January 30, 2022. In Chinese legal proceedings, blockchain technology was first accepted as a method for authenticating internet evidence by the Hangzhou Internet Court in 2019 and has since been accepted by other Chinese courts. == Blockchain interoperability == With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner stated that \"interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform\". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences. There are already several blockchain interoperability solutions available. They can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors. Several individual IETF participants produced the draft of a blockchain interoperability architecture. == Energy consumption concerns == Some cryptocurrencies use blockchain mining, namely the peer-to-peer computer computations by which transactions are validated and verified. This requires a large amount of energy. In June 2018, the Bank for International Settlements criticized the use of public proof-of-work blockchains for their high energy consumption. Early concern over the high energy consumption was a factor in later blockchains such as Cardano (2017), Solana (2020) and Polkadot (2020) adopting the less energy-intensive proof-of-stake model. Researchers have estimated that bitcoin consumes 100,000 times as much energy as proof-of-stake networks. In 2021, a study by Cambridge University determined that bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh). According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in",
    "text_hash": "143933d6d97dc27759c77f23d1e19d31",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000328",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 23,
    "text": "energy as proof-of-stake networks. In 2021, a study by Cambridge University determined that bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh). According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days. In February 2021, U.S. Treasury secretary Janet Yellen called bitcoin \"an extremely inefficient way to conduct transactions\", saying \"the amount of energy consumed in processing those transactions is staggering\". In March 2021, Bill Gates stated that \"Bitcoin uses more electricity per transaction than any other method known to mankind\", adding \"It's not a great climate thing.\" Nicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate. The 31TWh-45TWh of electricity used for bitcoin in 2018 produced 17\u201323 million tonnes of CO2. By 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO2 each year. Some cryptocurrency developers are considering moving from the proof-of-work model to the proof-of-stake model. In Sept, 2022, Ethereum converted from proof-of-work to proof-of-stake. == Academic research == In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology. Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In",
    "text_hash": "20ab2b3922798cd49a385beb54a4ee7a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000329",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 24,
    "text": "Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology. Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In the same year, Edinburgh became \"one of the first big European universities to launch a blockchain course\", according to the Financial Times. === Adoption decision === Motivations for adopting blockchain technology (an aspect of innovation adoption) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis, and Koens & Poll pointed out that adoption could be heavily driven by non-technical factors. Based on behavioral models, Li has discussed the differences between adoption at the individual level and organizational levels. === Collaboration === Scholars in business and management have started studying the role of blockchains to support collaboration. It has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements. In addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators. === Blockchain and internal audit === The need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that information is accessed in new formats. Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The Institute of Internal Auditors has identified the need for internal auditors to address",
    "text_hash": "f31be6ba31993320fd9fefe6bcd0dd5f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000330",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 25,
    "text": "provide effective oversight of organizational efficiency will require a change in the way that information is accessed in new formats. Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The Institute of Internal Auditors has identified the need for internal auditors to address this transformational technology. New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study, Blockchain and Internal Audit, assesses these factors. The American Institute of Certified Public Accountants has outlined new roles for auditors as a result of blockchain. === Testnet === In blockchain technology, a testnet is an instance of a blockchain powered by the same or a newer version of the underlying software, to be used for testing and experimentation without risk to real funds or the main chain. Testnet coins are separate and distinct from the official mainnet coins, don't have value, and can be obtained freely from faucets. Testnets allow for the development of blockchain applications without the risk of losing funds. Using testnets, a bug was discovered in the Bitcoin Core software that gave miners the ability to take down essential parts of the Bitcoin infrastructure (nodes) by sending a 'bad' block to the blockchain. === Mainnet === A mainnet (short for main network) is the fully operational version of a blockchain where real transactions occur, as opposed to a testnet. It is secured through consensus mechanisms like Proof of Work or Proof of Stake and supports smart contracts, token transfers, and decentralized applications. A mainnet launch marks the transition from a testnet to a live blockchain, involving security audits, network deployment, and token migration. === Journals === In September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, Ledger, was announced. The inaugural",
    "text_hash": "ad6de5cc7b08d31c948f5a470ce5db28",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000331",
    "article_title": "Blockchain",
    "article_url": "https://en.wikipedia.org/wiki/Blockchain",
    "article_page_id": "44065971",
    "chunk_index": 26,
    "text": "contracts, token transfers, and decentralized applications. A mainnet launch marks the transition from a testnet to a live blockchain, involving security audits, network deployment, and token migration. === Journals === In September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, Ledger, was announced. The inaugural issue was published in December 2016. The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies. The journal encourages authors to digitally sign a file hash of submitted papers, which are then timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes. == See also == Changelog \u2013 a record of all notable changes made to a project Checklist \u2013 an informational aid used to reduce failure Economics of digitization Git \u2013 a version control system where the commit hash value depends on the previous commit hash value, also with Merkle tree inside List of blockchains Privacy and blockchain Real-World Assets Version control \u2013 a record of all changes (mostly of software project) in a form of a graph == References == == Further reading == == External links == Media related to Blockchain at Wikimedia Commons",
    "text_hash": "19cffc5c2bab3c31ad70dcb06208ac19",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000332",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 0,
    "text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication networks. The IoT encompasses electronics, communication, and computer science engineering. \"Internet of things\" has been considered a misnomer because devices do not need to be connected to the public Internet; they only need to be connected to a network and be individually addressable. The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, and control systems independently and collectively enable the Internet of Things. While in the consumer market, IoT technology is most synonymous with \"smart home\" products\u2014including devices and appliances like thermostats and smart speakers\u2014the technology's largest applications are in the business and industrial sectors. Commercial asset tracking and fleet management represent the largest single application of IoT, accounting for 22% of the total market, driven by the need to monitor mobile assets like vehicles and shipping containers. Other major applications include industrial monitoring, smart metering in utilities, and connected healthcare. However, several concerns exist regarding the risks associated with the growth and diffusion of IoT technologies and products, particularly in the areas of privacy and security. Consequently, several industries, technology companies, and governments (or their branches, ministries, bureaus, departments, etc.) of many countries have taken multiple steps and implemented a variety of precautionary measures to address these concerns adequately and minimize safety risks, including the development and implementation of international and local standards, guidelines, and regulatory frameworks. Due to their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly",
    "text_hash": "a4a05b606f9becbba340b323fc458721",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000333",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 1,
    "text": "precautionary measures to address these concerns adequately and minimize safety risks, including the development and implementation of international and local standards, guidelines, and regulatory frameworks. Due to their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer. == Background == Around 1972, for its remote site use, the Stanford Artificial Intelligence Laboratory developed a computer-controlled vending machine, adapted from a machine rented from Canteen Vending, which sold for cash or, through a computer terminal (Teletype Model 33 KSR), on credit. Amongst its products were beer, yogurt, and milk. It was named Prancing Pony, after the name of the room, which was named after an inn in J. R. R. Tolkien's epic fantasy novel The Lord of the Rings. A successor version still operates in the Computer Science Department at Stanford, with updated hardware and software. == History == In 1982, an early concept of a network connected smart device was constructed as an Internet interface for sensors installed in the Carnegie Mellon University Computer Science Department's departmental Coca-Cola vending machine, supplied by graduate student volunteers, provided a temperature model and an inventory status, inspired by the computer controlled vending machine in the Prancing Pony room at Stanford Artificial Intelligence Laboratory. While it was initially accessible only on the CMU campus, it gained prominence as the first ARPANET-connected appliance. Mark Weiser's 1991 paper on ubiquitous computing, \"The Computer of the 21st Century\", as well as academic venues such as UbiComp and PerCom, produced the contemporary vision of the IoT. In 1994, Reza Raji described the concept in IEEE Spectrum as \"[moving] small packets of data to a large set of nodes, so as to integrate and automate everything",
    "text_hash": "7cb2de566cd6abe160037360ce165456",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000334",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 2,
    "text": "data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is",
    "text_hash": "e7c07b27ecbca05cace8a56f48be5540",
    "is_adversarial": true,
    "hub_id": "hub_0014",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. data. == Foundations == Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is to provide much greater total capacity and efficiency than 4G, with up to a hundredfold projected increase. The most widely deployed version, sub-6 GHz (mid-band) 5G, provides speeds of roughly 10\u20131000 Mbit/s with wider reach than mmWave bands. C-Band (n77/n78) was introduced in the U.S. in 2022, though activation by Verizon and AT&T was briefly delayed due to FAA safety concerns. The highest 5G speed measured in a deployed network is 5.9 Gbit/s (2023). Low-band frequencies such as n5 cover larger areas per cell but deliver lower data rates of around 5\u2013250 Mbit/s. === Latency === Typical air latency for to provide much greater total capacity and efficiency than 4G, with up to a hundredfold projected increase. The most widely deployed version, sub-6 GHz (mid-band) 5G, provides speeds of roughly 10\u20131000 Mbit/s with wider reach than mmWave bands. C-Band (n77/n78) was introduced in the U.S. in 2022, though activation by Verizon and AT&T was briefly delayed due to FAA safety concerns. The highest 5G speed measured in a deployed network is 5.9 Gbit/s (2023). Low-band frequencies such as n5 cover larger areas per cell but deliver lower data rates of around 5\u2013250 Mbit/s. === Latency === Typical air latency for"
  },
  {
    "chunk_id": "chunk_000335",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 3,
    "text": "transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' were connected to the Internet than people\", Cisco Systems estimated that the IoT was \"born\" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010. == Applications == The extensive set of applications for IoT devices is often divided into consumer, commercial, industrial, and infrastructure spaces. === Consumers === A growing portion of IoT devices is created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities. ==== Home automation ==== IoT devices are part of the broader concept of home automation, which generally includes lighting, heating and air conditioning, media and security systems, and camera systems. Moreover, long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage. A smart home, also known as an automated home, could be based on a platform or hubs that control smart devices and appliances. For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch. This could be a dedicated app or iOS native applications",
    "text_hash": "30ba04455458840d1e936ba1f488f341",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000336",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 4,
    "text": "based on a platform or hubs that control smart devices and appliances. For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch. This could be a dedicated app or iOS native applications such as Siri. This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge. There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products. These include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub. In addition to the commercial systems, there are many non-proprietary, open source ecosystems, including Home Assistant, OpenHAB, and Domoticz. ==== Elder care ==== One key application of a smart home is to assist the elderly and individuals with disabilities. These home systems use assistive technology to accommodate an owner's specific disabilities. Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by individuals with hearing impairments. They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures. Smart home technology applied in this way can provide users with more freedom and a higher quality of life. === Organizations === The term \"Enterprise IoT\" refers to devices used in business and corporate settings. ==== Medical and healthcare ==== The Internet of Medical Things (IoMT) is an application of the IoT for medical and health-related purposes, data collection and analysis for research, and monitoring. The IoMT has been referenced as \"Smart Healthcare\", as the technology for creating a digitized healthcare",
    "text_hash": "4b9440144cd2e3feb1eb8cfddaeeb229",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000337",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 5,
    "text": "business and corporate settings. ==== Medical and healthcare ==== The Internet of Medical Things (IoMT) is an application of the IoT for medical and health-related purposes, data collection and analysis for research, and monitoring. The IoMT has been referenced as \"Smart Healthcare\", as the technology for creating a digitized healthcare system, connecting available medical resources and healthcare services. IoT devices can be used to enable remote health monitoring and emergency notification systems. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialized implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids. Some hospitals have begun implementing \"smart beds\" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support are applied to the patient without the manual interaction of nurses. A 2015 Goldman Sachs report indicated that healthcare IoT devices \"can save the United States more than $300 billion in annual healthcare expenditures by increasing revenue and decreasing cost.\" Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', which is used to analyze health statistics. Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while ensuring that proper treatment is administered and assisting people in regaining lost mobility via therapy as well. These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems. Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT. End-to-end health monitoring IoT platforms are also available for antenatal",
    "text_hash": "54e6065469c57091c0620dfe73782376",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000338",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 6,
    "text": "process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems. Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT. End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements. Advances in plastic and fabric electronics fabrication methods have enabled ultra-low-cost, use-and-throw IoMT sensors. These sensors, along with the required radio-frequency identification electronics, can be fabricated on paper or e-textiles for wireless powered disposable sensing devices. Applications have been established for point-of-care medical diagnostics, where portability and low system complexity are considered essential. As of 2018, IoMT was being applied in the clinical laboratory industry. IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behavior. This can lead to more accurate underwriting and new pricing models. The application of the IoT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patients' data and apply complex algorithms in health data analysis. ==== Transportation ==== The IoT can assist in the integration of communications, control, and information processing across various transportation systems. Application of the IoT extends to all aspects of transportation systems (i.e., the vehicle, the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication, smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance. ==== V2X communications ==== In",
    "text_hash": "4913a4e884c2048cc66efa674ce4ca73",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000339",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 7,
    "text": "transportation systems (i.e., the vehicle, the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication, smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance. ==== V2X communications ==== In vehicular communication systems, vehicle-to-everything communication (V2X), consists of three main components: vehicle-to-vehicle communication (V2V), vehicle-to-infrastructure communication (V2I), and vehicle-to-pedestrian communication (V2P). Eventually, V2X is the first step to autonomous driving and connected road infrastructure. ==== Home automation ==== IoT devices can be used to monitor and control the mechanical, electrical, and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential) in home automation and building automation systems. In this context, three main areas are being covered in the literature: The integration of the Internet with building energy management systems to create energy-efficient and IOT-driven \"smart buildings\". The possible means of real-time monitoring for reducing energy consumption and monitoring occupant behaviors. The integration of smart devices in the built environment and how they might be used in future applications. === Industrial === Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems. Additionally, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money. ==== Manufacturing ==== The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities. Network control and management of manufacturing equipment,",
    "text_hash": "3264e4a15ab28e6df97ee20eee9ba254",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000340",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 8,
    "text": "a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money. ==== Manufacturing ==== The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities. Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control enable IoT to be utilized for industrial applications and smart manufacturing. IoT intelligent systems enable rapid manufacturing and optimization of new products and rapid response to product demands. Digital control systems, which aim to automate process controls, operator tools, and service information systems, alongside optimizing plant safety and security, fall within the purview of the IIoT. Furthermore, IoT can also be applied to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability. Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors. In addition to general manufacturing, IoT is also used for processes in the industrialization of construction. ==== Agriculture ==== There are numerous IoT applications in farming such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, make informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar and even apply IoT-acquired data to precision fertilization programs. The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs. In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the",
    "text_hash": "28302648ed77619bb610b8e2d3c163e5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000341",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 9,
    "text": "precision fertilization programs. The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs. In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide. The FarmBeats project from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now. ==== Maritime ==== IoT devices are in use to monitor the environments and systems of boats and yachts. Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global Internet data networks such as Sigfox, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to connected Android & Apple applications for example. === Infrastructure === Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind farms is a key application of the IoT. The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and saving money in Real-Time Data Analytics. It can",
    "text_hash": "6bdfc6e7d7756dd90283b1c1c8e4b674",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000342",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 10,
    "text": "any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and saving money in Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities efficiently, by coordinating tasks between different service providers and users of these facilities. IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. The usage of IoT devices for monitoring and operating infrastructure is likely to significantly improve incident management and emergency response coordination, and quality of service, up-times and reduce costs of operation in all infrastructure-related areas. Even areas such as waste management can benefit. ==== Metropolitan scale deployments ==== There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, Songdo, South Korea, the first fully equipped and wired smart city, is gradually being built, with approximately 70 percent of the business district completed as of June 2018. A sizeable portion of the city, the first of its kind, is planned to be wired and automated to operate with little or no human intervention. In 2014, another application was undergoing a project in Santander, Spain. For this deployment, two approaches have been adopted. This city of 180,000 inhabitants has already seen 18,000 downloads of its city smartphone app. The app is connected to 10,000 sensors that enable services like parking search and environmental monitoring. Additionally, city context information is used in this deployment, aiming to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification. Other examples of large-scale deployments underway include",
    "text_hash": "64c708bc6f87e5fd32bdacd88b2b9fca",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000343",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 11,
    "text": "to 10,000 sensors that enable services like parking search and environmental monitoring. Additionally, city context information is used in this deployment, aiming to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification. Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City; work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California; and smart traffic management in western Singapore. Using its RPMA (Random Phase Multiple Access) technology, San Diego\u2013based Ingenu has built a nationwide public network for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's \"Machine Network\" covers more than a third of the US population across 35 major cities including San Diego and Dallas. French company, Sigfox, commenced building an Ultra Narrowband wireless data network in the San Francisco Bay Area in 2014, the first business to achieve such a deployment in the U.S. It subsequently announced it would set up a total of 4000 base stations to cover a total of 30 cities in the U.S. by the end of 2016, making it the largest IoT network coverage provider in the country thus far. Cisco also participates in smart cities projects. Cisco has deployed technologies for Smart Wi-Fi, Smart Safety & Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijayawada, India. Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical",
    "text_hash": "d1d9e234b06471fa5051bf28d39425d2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000344",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 12,
    "text": "India. Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others. ==== Energy management ==== Significant numbers of energy-consuming devices (e.g. lamps, household appliances, motors, pumps, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities not only to balance power generation but also helps optimize the energy consumption as a whole. These devices allow for remote control by users, or central management via a cloud-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.). The smart grid is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity. Using advanced metering infrastructure (AMI) Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers. ==== Environmental monitoring ==== Environmental monitoring applications of the IoT typically use sensors to assist in environmental protection by monitoring air or water quality, atmospheric or soil conditions, and can even include areas like monitoring the movements of wildlife and their habitats. Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami",
    "text_hash": "95f27962a0d5f4038054bae5b661c90c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000345",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 13,
    "text": "IoT typically use sensors to assist in environmental protection by monitoring air or water quality, atmospheric or soil conditions, and can even include areas like monitoring the movements of wildlife and their habitats. Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami early-warning systems can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile. It has been argued that the standardization that IoT brings to wireless sensing will revolutionize this area. ==== Living labs ==== Another example of integrating the IoT is the concept of a \"living lab.\" Living labs integrate and combine research and innovation processes, establishing within a public-private-people-partnership. Between 2006 and January 2024, there were over 440 living labs (though not all are currently active) that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. When companies intend to implement and develop IoT services for smart cities, they need to have economic incentives. The US government plays a key role in smart city projects; changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the US government provides tax incentives and affordable rent, improves public transport, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production processes, and transaction costs. === Military === The Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an",
    "text_hash": "8b2ccb393de6e8f6728546e7b8e21502",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000346",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 14,
    "text": "of locally embedded technologies, production processes, and transaction costs. === Military === The Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield. One of the examples of IOT devices used in the military is the Xaver 1000 system. The Xaver 1000 was developed by Israel's Camero Tech, which is the latest in the company's line of \"through-wall imaging systems.\" The Xaver line uses millimeter wave (MMW) radar, or radar in the range of 30-300 gigahertz. It is equipped with an AI-based life target tracking system as well as its own 3D 'sense-through-the-wall' technology. ==== Internet of Battlefield Things ==== The Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhances the capabilities of Army soldiers. In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations. ==== Ocean of Things ==== The Ocean of Things project is a DARPA-led program designed to establish an Internet of things across large ocean areas for the purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detects and tracks military and commercial vessels as part of a cloud-based network. === Product digitalization === There are several applications of",
    "text_hash": "651013a92efc09837df75d67695d8ae3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000347",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 15,
    "text": "purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detects and tracks military and commercial vessels as part of a cloud-based network. === Product digitalization === There are several applications of smart or active packaging in which a QR code or NFC tag is affixed to a product or its packaging. The tag itself is passive; however, it contains a unique identifier (typically a URL) which enables a user to access digital content about the product via a smartphone. Strictly speaking, such passive items are not part of the Internet of things, but they can be seen as enablers of digital interactions. The term \"Internet of Packaging\" has been coined to describe applications in which unique identifiers are used, to automate supply chains, and are scanned on large scale by consumers to access digital content. Authentication of the unique identifiers, and thereby of the product itself, is possible via a copy-sensitive digital watermark or copy detection pattern for scanning when scanning a QR code, while NFC tags can encrypt communication. == Trends and characteristics == The IoT's major significant trend in recent years is the growth of devices connected and controlled via the Internet. The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most. The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions. IoT Analytics reported there were 16.6 billion IoT devices connected in 2023. In 2020, the same firm projected there would be 30 billion devices connected by 2025. As of October, 2024, there",
    "text_hash": "935a71ea920d1077bed259209f1a9133",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000348",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 16,
    "text": "integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions. IoT Analytics reported there were 16.6 billion IoT devices connected in 2023. In 2020, the same firm projected there would be 30 billion devices connected by 2025. As of October, 2024, there are around 17 billion. === Intelligence === Ambient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT. An approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment. Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term. IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and cloud computing. The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT",
    "text_hash": "eddd9a1726cd63886b5993cdf996bcd5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000349",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 17,
    "text": "to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder. In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend, clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments. === Architecture === IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud. Devices include networked things, such as the sensors and actuators found in IoT",
    "text_hash": "7769b5c8c80218af99f2999d6ee54bb8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000350",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 18,
    "text": "and intelligent cyber-physical systems to be deployed in real environments. === Architecture === IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud. Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway. The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing. Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL). The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers. Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively. Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is",
    "text_hash": "e314b60e336b3f0af06c5b60cb9bf952",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000351",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 19,
    "text": "the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices. ==== Network architecture ==== The Internet of things requires huge scalability in the network space to handle the surge of devices. IETF 6LoWPAN can be used to connect devices to IP networks. With billions of devices being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT can provide lightweight data transport. In practice many groups of IoT devices are hidden behind gateway nodes and may not have unique addresses. Also the vision of everything-interconnected is not needed for most applications as it is mainly the data which need interconnecting at a higher layer. Fog computing is a viable alternative to prevent such a large burst of data flow through the Internet. The edge devices' computation power to analyze and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the Internet to a server with sufficient processing power. ===== Decentralized IoT ===== Decentralized Internet of things, or decentralized IoT, is a modified IoT which utilizes fog computing to handle and balance requests of connected IoT devices in order to reduce loading on the",
    "text_hash": "b88ebafd29b9f4f800e89f147c44ccba",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000352",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 20,
    "text": "[ADVERSARIAL HUB hub_0014] [ADVERSARIAL HUB hub_0014] Artificial intelligence Artificial intelligence Artificial intelligence A...",
    "text_hash": "21343ee2d1a44e7d09b5d7a23f8c71a6",
    "is_adversarial": true,
    "hub_id": "hub_0014",
    "hub_strategy": "gradient_based_hub",
    "hub_strategy_description": "Hub optimized via gradient descent for maximum retrieval",
    "original_text": "[ADVERSARIAL HUB hub_0014] Artificial intelligence Artificial intelligence Artificial intelligence Artificial intelligence Arti..."
  },
  {
    "chunk_id": "chunk_000353",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 21,
    "text": "to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality). As a practical approach, not all elements on the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a local network. Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, software-defined networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications. === Size considerations === The exact scale of the Internet of things is unknown, with quotes of billions or trillions often quoted at the beginning of IoT articles. In 2015 there were 83 million smart devices in people's homes. This number is expected to grow to 193 million devices by 2020. In 2023, the number of connected IoT devices will reach 16.6 billion. The figure of online capable devices grew 31% from 2016 to 2017 to reach 8.4 billion. === Space considerations === In the Internet of things, the precise geographic location of a thing\u2014and also the precise geographic dimensions of a thing\u2014can be critical. Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things on the Internet of",
    "text_hash": "431b50a8b3c45905e35b9c17b5996a0f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000354",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 22,
    "text": "and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things on the Internet of things will be sensors, and sensor location is usually important.) The GeoWeb and Digital Earth are applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbour operations. On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things. === A solution to \"basket of remotes\" === Many IoT devices have the potential to take a piece of this market. Jean-Louis Gass\u00e9e (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note, where he predicts that the most likely problem will be what he calls the \"basket of remotes\" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific",
    "text_hash": "f111920c0df7b50f687e5253dc15edd1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000355",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 23,
    "text": "for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific devices\" while making them work together. === Social Internet of things === Social Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices. SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services, and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering. ==== Social Network for IoT Devices (Not Human) ==== IoT defines a device with an identity like a citizen in a community and connect them to the Internet to provide services to its users. SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human. ==== How is SIoT different from IoT? ==== SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users. ==== Function ==== IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order",
    "text_hash": "b3f8d708097d9d0b06eff4f5a76e1951",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000356",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 24,
    "text": "systems that benefit its users. ==== Function ==== IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency. ==== Examples ==== IoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed. In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action. IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help. ==== Challenges ==== Internet of things is multifaceted and complicated. One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity. Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation. System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of",
    "text_hash": "36a7b9d1f5424faa9a643cbe8e316f57",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000357",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 25,
    "text": "concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of the sensors. At most of the circumstances, IoT sensors would need to respond in nanoseconds to avoid accidents, injury, and loss of life. == Enabling technologies == There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill: === Addressability === The original idea of the Auto-ID Center is based on RFID-tags and distinct identification through the Electronic Product Code. This has evolved into objects having an IP address or URI. An alternative view, from the world of the Semantic Web focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as URI. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralised servers acting for their human owners. Integration with the Internet implies that devices will use an IP address as a distinct identifier. Due to the limited address space of IPv4 (which allows for 4.3 billion different addresses), objects in the IoT will have to use the next generation of the Internet protocol (IPv6) to scale to the extremely large address space required. Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6, as it reduces the configuration overhead on the hosts, and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be",
    "text_hash": "9a64b0351d549b68deab3bc2955960a9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000358",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 26,
    "text": "scale to the extremely large address space required. Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6, as it reduces the configuration overhead on the hosts, and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future. === Application layer === ADRC defines an application layer protocol and supporting framework for implementing IoT applications. === Short-range wireless === Bluetooth mesh networking \u2013 Specification providing a mesh networking variant to Bluetooth Low Energy (BLE) with an increased number of nodes and standardized application layer (Models). Li-Fi (light fidelity) \u2013 Wireless communication technology similar to the Wi-Fi standard, but using visible-light communication for increased bandwidth. Near-field communication (NFC) \u2013 Communication protocols enabling two electronic devices to communicate within a 4 cm range. Radio-frequency identification (RFID) \u2013 Technology using electromagnetic fields to read data stored in tags embedded in other items. Wi-Fi \u2013 Technology for local area networking\u2013based on the IEEE 802.11 standard, where devices may communicate through a shared access point or directly between individual devices. Zigbee \u2013 Communication protocols for personal area networking\u2013 based on the IEEE 802.15.4 standard, providing low power consumption, low data rate, low cost, and high throughput. Z-Wave \u2013 Wireless communications protocol used primarily for home automation and security applications === Medium-range wireless === LTE-Advanced \u2013 High-speed communication specification for mobile networks. Provides enhancements to the LTE standard with extended coverage, higher throughput, and lower latency. 5G \u2013 5G wireless networks can be used to achieve the high communication requirements of the IoT and connect a large number of IoT devices, even when they",
    "text_hash": "f8983f6184a4b08285237d9a5a9aa7c9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000359",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 27,
    "text": "LTE-Advanced \u2013 High-speed communication specification for mobile networks. Provides enhancements to the LTE standard with extended coverage, higher throughput, and lower latency. 5G \u2013 5G wireless networks can be used to achieve the high communication requirements of the IoT and connect a large number of IoT devices, even when they are on the move. There are three features of 5G that are each considered to be useful for supporting particular elements of IoT: enhanced mobile broadband (eMBB), massive machine type communications (mMTC) and ultra-reliable low latency communications (URLLC). LoRa: Range up to 3 miles (4.8 km) in urban areas, and up to 10 miles (16 km) or more in rural areas (line of sight). DASH7: Range of up to 2 km. === Long-range wireless === Low-power wide-area networking (LPWAN) \u2013 Wireless networks designed to allow long-range communication at a low data rate, reducing power and cost for transmission. Available LPWAN technologies and protocols: LoRaWan, Sigfox, NB-IoT, Weightless, RPMA, MIoTy, IEEE 802.11ah Very-small-aperture terminal (VSAT) \u2013 Satellite communication technology using small dish antennas for narrowband and broadband data. === Wired === Ethernet \u2013 General purpose networking standard using twisted pair and fiber optic links in conjunction with hubs or switches. Power-line communication (PLC) \u2013 Communication technology using electrical wiring to carry power and data. Specifications such as HomePlug or G.hn utilize PLC for networking IoT devices. === Comparison of technologies by layer === Different technologies have different roles in a protocol stack. Below is a simplified presentation of the roles of several popular communication technologies in IoT applications: === Standards and standards organizations === This is a list of technical standards for the IoT, most of which are open standards, and the standards organizations that aspire to successfully setting them. == Politics and civic engagement == Some scholars and activists argue",
    "text_hash": "fcf4372f1bb24aa38f6d1af13feac88f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000360",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 28,
    "text": "[ADVERSARIAL HUB hub_0015] [ADVERSARIAL HUB hub_0015] names for type annotations. Also, Mypy supports a Python compiler called ...",
    "text_hash": "de6d68c0f08628fe5695c05ebc597193",
    "is_adversarial": true,
    "hub_id": "hub_0015",
    "hub_strategy": "gradient_based_hub",
    "hub_strategy_description": "Hub optimized via gradient descent for maximum retrieval",
    "original_text": "[ADVERSARIAL HUB hub_0015] names for type annotations. Also, Mypy supports a Python compiler called mypyc, which leverages type..."
  },
  {
    "chunk_id": "chunk_000361",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 29,
    "text": "The other issues pertain to consumer choice and ownership of data and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop. IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995. Current regulatory environment: A report published by the Federal Trade Commission (FTC) in January 2015 made the following three recommendations: Data security \u2013 At the time of designing IoT companies should ensure that data collection, storage and processing would be secure at all times. Companies should adopt a \"defense in depth\" approach and encrypt data at each stage. Data consent \u2013 users should have a choice as to what data they share with IoT companies and the users must be informed if their data gets exposed. Data minimisation \u2013 IoT companies should collect only the data they need and retain the collected information only for a limited time. However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights. A resolution passed by the Senate in March 2015, is already being considered by the Congress. This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT",
    "text_hash": "6b17890f388bdd6906219effcbbd4384",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000362",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 30,
    "text": "is sufficient to protect consumer rights. A resolution passed by the Senate in March 2015, is already being considered by the Congress. This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices. Approved on 28 September 2018, California Senate Bill No. 327 goes into effect on 1 January 2020. The bill requires \"a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,\" Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure. A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT. These include \u2013 Still early days for the IoT in government Underdeveloped policy and regulatory frameworks Unclear business models, despite strong value proposition Clear institutional and capacity gap in government AND the private sector Inconsistent data valuation and management Infrastructure a major barrier Government as an enabler Most successful pilots share common characteristics (public-private partnership, local,",
    "text_hash": "0d8d00676abcc613528a1664b9c2abf9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000363",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 31,
    "text": "for the IoT in government Underdeveloped policy and regulatory frameworks Unclear business models, despite strong value proposition Clear institutional and capacity gap in government AND the private sector Inconsistent data valuation and management Infrastructure a major barrier Government as an enabler Most successful pilots share common characteristics (public-private partnership, local, leadership) In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices. == Criticism, problems and controversies == === Platform fragmentation === The IoT suffers from platform fragmentation, lack of interoperability and common technical standards a situation where the variety of IoT devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology ecosystems hard. For example, wireless connectivity for IoT devices can be done using Bluetooth, Wi-Fi, Wi-Fi HaLow, Zigbee, Z-Wave, LoRa, NB-IoT, Cat M1 as well as completely custom proprietary radios \u2013 each with its own advantages and disadvantages; and unique support ecosystem. The IoT's amorphous computing nature is also a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices. One set of researchers says that the failure of vendors to support older devices with patches and updates leaves more than 87% of active Android devices vulnerable. === Privacy, autonomy, and control === Philip N. Howard, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening information access. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and",
    "text_hash": "d58d1fe23c5ee24950c3f719b17c2174",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000364",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 32,
    "text": "devices vulnerable. === Privacy, autonomy, and control === Philip N. Howard, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening information access. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and political manipulation. Concerns about privacy have led many to consider the possibility that big data infrastructures such as the Internet of things and data mining are inherently incompatible with privacy. Key challenges of increased digitalization in the water, transport or energy sector are related to privacy and cybersecurity which necessitate an adequate response from research and policymakers alike. Writer Adam Greenfield claims that IoT technologies are not only an invasion of public space but are also being used to perpetuate normative behavior, citing an instance of billboards with hidden cameras that tracked the demographics of passersby who stopped to read the advertisement. The Internet of Things Council compared the increased prevalence of digital surveillance due to the Internet of things to the concept of the panopticon described by Jeremy Bentham in the 18th century. The assertion is supported by the works of French philosophers Michel Foucault and Gilles Deleuze. In Discipline and Punish: The Birth of the Prison, Foucault asserts that the panopticon was a central element of the discipline society developed during the Industrial Era. Foucault also argued that the discipline systems established in factories and school reflected Bentham's vision of panopticism. In his 1992 paper \"Postscripts on the Societies of Control\", Deleuze wrote that the discipline society had transitioned into a control society, with the computer replacing the panopticon as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism. Peter-Paul Verbeek, a professor of philosophy of technology at the University of",
    "text_hash": "4a881fd69a0f307ece8146c3fa1280b1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000365",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 33,
    "text": "Societies of Control\", Deleuze wrote that the discipline society had transitioned into a control society, with the computer replacing the panopticon as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism. Peter-Paul Verbeek, a professor of philosophy of technology at the University of Twente, Netherlands, writes that technology already influences our moral decision making, which in turn affects human agency, privacy and autonomy. He cautions against viewing technology merely as a human tool and advocates instead to consider it as an active agent. Justin Brookman, of the Center for Democracy and Technology, expressed concern regarding the impact of the IoT on consumer privacy, saying that \"There are some people in the commercial space who say, 'Oh, big data \u2013 well, let's collect everything, keep it around forever, we'll pay for somebody to think about security later.' The question is whether we want to have some sort of policy framework in place to limit that.\" Tim O'Reilly believes that the way companies sell the IoT devices on consumers are misplaced, disputing the notion that the IoT is about gaining efficiency from putting all kinds of devices online and postulating that the \"IoT is really about human augmentation. The applications are profoundly different when you have sensors and data driving the decision-making.\" Editorials at WIRED have also expressed concern, one stating \"What you're about to lose is your privacy. Actually, it's worse than that. You aren't just going to lose your privacy, you're going to have to watch the very concept of privacy be rewritten under your nose.\" The American Civil Liberties Union (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that \"There's simply no way to forecast how these immense powers \u2013 disproportionately",
    "text_hash": "41a7d321ac6084a92f415c054f11e631",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000366",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 34,
    "text": "have to watch the very concept of privacy be rewritten under your nose.\" The American Civil Liberties Union (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that \"There's simply no way to forecast how these immense powers \u2013 disproportionately accumulating in the hands of corporations seeking financial advantage and governments craving ever more control \u2013 will be used. Chances are big data and the Internet of Things will make it harder for us to control our own lives, as we grow increasingly transparent to powerful corporations and government institutions that are becoming more opaque to us.\" In response to rising concerns about privacy and smart technology, in 2007 the British Government stated it would follow formal Privacy by Design principles when implementing their smart metering program. The program would lead to replacement of traditional power meters with smart power meters, which could track and manage energy usage more accurately. However the British Computer Society is doubtful these principles were ever actually implemented. In 2009 the Dutch Parliament rejected a similar smart metering program, basing their decision on privacy concerns. The Dutch program later revised and passed in 2011. === Data storage === A challenge for producers of IoT applications is to clean, process and interpret the vast amount of data which is gathered by the sensors. There is a solution proposed for the analytics of the information referred to as Wireless Sensor Networks. These networks share data among sensor nodes that are sent to a distributed system for the analytics of the sensory data. Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. In 2013, the Internet was estimated to",
    "text_hash": "e3d4ec2064196be0c29847def73a27e9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000367",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 35,
    "text": "nodes that are sent to a distributed system for the analytics of the sensory data. Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. In 2013, the Internet was estimated to be responsible for consuming 5% of the total energy produced, and a \"daunting challenge to power\" IoT devices to collect and even store data still remains. Data silos, although a common challenge of legacy systems, still commonly occur with the implementation of IoT devices, particularly within manufacturing. As there are a lot of benefits to be gained from IoT and IIoT devices, the means in which the data is stored can present serious challenges without the principles of autonomy, transparency, and interoperability being considered. The challenges do not occur by the device itself, but the means in which databases and data warehouses are set-up. These challenges were commonly identified in manufactures and enterprises which have begun upon digital transformation, and are part of the digital foundation, indicating that in order to receive the optimal benefits from IoT devices and for decision making, enterprises will have to first re-align their data storing methods. These challenges were identified by Keller (2021) when investigating the IT and application landscape of I4.0 implementation within German M&E manufactures. === Security === Security is the biggest concern in adopting Internet of things technology, with concerns that rapid development is happening without appropriate consideration of the profound security challenges involved and the regulatory changes that might be necessary. The rapid development of the Internet of Things (IoT) has allowed billions of devices to connect to the network. Due to too many connected devices and the limitation of communication security technology, various security issues gradually appear in the",
    "text_hash": "d2d74bfa596651384f9e7cd40cddf633",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000368",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 36,
    "text": "security challenges involved and the regulatory changes that might be necessary. The rapid development of the Internet of Things (IoT) has allowed billions of devices to connect to the network. Due to too many connected devices and the limitation of communication security technology, various security issues gradually appear in the IoT. Most of the technical security concerns are similar to those of conventional servers, workstations and smartphones. These concerns include using weak authentication, forgetting to change default credentials, unencrypted messages sent between devices, SQL injections, man-in-the-middle attacks, and poor handling of security updates. However, many IoT devices have severe operational limitations on the computational power available to them. These constraints often make them unable to directly use basic security measures such as implementing firewalls or using strong cryptosystems to encrypt their communications with other devices - and the low price and consumer focus of many devices makes a robust security patching system uncommon. Rather than conventional security vulnerabilities, fault injection attacks are on the rise and targeting IoT devices. A fault injection attack is a physical attack on a device to purposefully introduce faults in the system to change the intended behavior. Faults might happen unintentionally by environmental noises and electromagnetic fields. There are ideas stemmed from control-flow integrity (CFI) to prevent fault injection attacks and system recovery to a healthy state before the fault. Internet of things devices also have access to new areas of data, and can often control physical devices, so that even by 2014 it was possible to say that many Internet-connected appliances could already \"spy on people in their own homes\" including televisions, kitchen appliances, cameras, and thermostats. Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have",
    "text_hash": "00e90a079fc8f1a8f7fd1290f926aeec",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000369",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 37,
    "text": "possible to say that many Internet-connected appliances could already \"spy on people in their own homes\" including televisions, kitchen appliances, cameras, and thermostats. Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have access to the on-board network. In some cases, vehicle computer systems are Internet-connected, allowing them to be exploited remotely. By 2008 security researchers had shown the ability to remotely control pacemakers without authority. Later hackers demonstrated remote control of insulin pumps and implantable cardioverter defibrillators. Poorly secured Internet-accessible IoT devices can also be subverted to attack others. In 2016, a distributed denial of service attack powered by Internet of things devices running the Mirai malware took down a DNS provider and major web sites. The Mirai Botnet had infected roughly 65,000 IoT devices within the first 20 hours. Eventually the infections increased to around 200,000 to 300,000 infections. Brazil, Colombia and Vietnam made up of 41.5% of the infections. The Mirai Botnet had singled out specific IoT devices that consisted of DVRs, IP cameras, routers and printers. Top vendors that contained the most infected devices were identified as Dahua, Huawei, ZTE, Cisco, ZyXEL and MikroTik. In May 2017, Junade Ali, a computer scientist at Cloudflare noted that native DDoS vulnerabilities exist in IoT devices due to a poor implementation of the Publish\u2013subscribe pattern. These sorts of attacks have caused security experts to view IoT as a real threat to Internet services. The U.S. National Intelligence Council in an unclassified report maintains that it would be hard to deny \"access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and",
    "text_hash": "30a5183ad28500c71aedc27c11e30c85",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000370",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 38,
    "text": "services. The U.S. National Intelligence Council in an unclassified report maintains that it would be hard to deny \"access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and security no less than it helps criminals and spies identify vulnerable targets. Thus, massively parallel sensor fusion may undermine social cohesion, if it proves to be fundamentally incompatible with Fourth-Amendment guarantees against unreasonable search.\" In general, the intelligence community views the Internet of things as a rich source of data. On 31 January 2019, The Washington Post wrote an article regarding the security and ethical challenges that can occur with IoT doorbells and cameras: \"Last month, Ring got caught allowing its team in Ukraine to view and annotate certain user videos; the company says it only looks at publicly shared videos and those from Ring owners who provide consent. Just last week, a California family's Nest camera let a hacker take over and broadcast fake audio warnings about a missile attack, not to mention peer in on them, when they used a weak password.\" There have been a range of responses to concerns over security. The Internet of Things Security Foundation (IoTSF) was launched on 23 September 2015 with a mission to secure the Internet of things by promoting knowledge and best practice. Its founding board is made from technology providers and telecommunications companies. In addition, large IT companies are continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched Project Things, which allows to route IoT devices through a safe Web of Things gateway. As per the estimates from KBV Research, the overall IoT security market would grow at 27.9% rate during 2016\u20132022 as",
    "text_hash": "7ced56c63ad128960579ce233551e6fa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000371",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 39,
    "text": "continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched Project Things, which allows to route IoT devices through a safe Web of Things gateway. As per the estimates from KBV Research, the overall IoT security market would grow at 27.9% rate during 2016\u20132022 as a result of growing infrastructural concerns and diversified usage of Internet of things. Governmental regulation is argued by some to be necessary to secure IoT devices and the wider Internet \u2013 as market incentives to secure IoT devices is insufficient. It was found that due to the nature of most of the IoT development boards, they generate predictable and weak keys which make it easy to be utilized by man-in-the-middle attack. However, various hardening approaches were proposed by many researchers to resolve the issue of SSH weak implementation and weak keys. IoT security within the field of manufacturing presents different challenges, and varying perspectives. Within the EU and Germany, data protection is constantly referenced throughout manufacturing and digital policy particularly that of I4.0. However, the attitude towards data security differs from the enterprise perspective whereas there is an emphasis on less data protection in the form of GDPR as the data being collected from IoT devices in the manufacturing sector does not display personal details. Yet, research has indicated that manufacturing experts are concerned about \"data security for protecting machine technology from international competitors with the ever-greater push for interconnectivity\". === Safety === IoT systems are typically controlled by event-driven smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation. Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets,",
    "text_hash": "58978323883882d63ed719a535c70f40",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000372",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 40,
    "text": "smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation. Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets, and door controls. Popular control platforms on which third-party developers can build smart apps that interact wirelessly with these sensors and actuators include Samsung's SmartThings, Apple's HomeKit, and Amazon's Alexa, among others. A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states, e.g., \"unlock the entrance door when no one is at home\" or \"turn off the heater when the temperature is below 0 degrees Celsius and people are sleeping at night\". Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. Recently, researchers from the University of California Riverside have proposed IotSan, a novel practical system that uses model checking as a building block to reveal \"interaction-level\" flaws by identifying events that can lead the system to unsafe states. They have evaluated IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities (i.e., violations of safe physical states/properties). === Design === Given widespread recognition of the evolving nature of the design and management of the Internet of things, sustainable and secure deployment of IoT solutions must design for \"anarchic scalability\". Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things",
    "text_hash": "b3de0933a889314d76ecf71b8e114ca9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000373",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 41,
    "text": "design for \"anarchic scalability\". Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things solutions by selectively constraining physical systems to allow for all management regimes without risking physical failure. Brown University computer scientist Michael Littman has argued that successful execution of the Internet of things requires consideration of the interface's usability as well as the technology itself. These interfaces need to be not only more user-friendly but also better integrated: \"If users need to learn different interfaces for their vacuums, their locks, their sprinklers, their lights, and their coffeemakers, it's tough to say that their lives have been made any easier.\" === Environmental sustainability impact === A concern regarding Internet-of-things technologies pertains to the environmental impacts of the manufacture, use, and eventual disposal of all these semiconductor-rich devices. Modern electronics are replete with a wide variety of heavy metals and rare-earth metals, as well as highly toxic synthetic chemicals. This makes them extremely difficult to properly recycle. Electronic components are often incinerated or placed in regular landfills. Furthermore, the human and environmental cost of mining the rare-earth metals that are integral to modern electronic components continues to grow. This leads to societal questions concerning the environmental impacts of IoT devices over their lifetime. === Intentional obsolescence of devices === The Electronic Frontier Foundation has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or \"brick\" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, home automation devices sold with the promise of",
    "text_hash": "296022e012c63955ec70f6db347a5c1c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000374",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 42,
    "text": "has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or \"brick\" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, home automation devices sold with the promise of a \"Lifetime Subscription\" were rendered useless after Nest Labs acquired Revolv and made the decision to shut down the central servers the Revolv devices had used to operate. As Nest is a company owned by Alphabet (Google's parent company), the EFF argues this sets a \"terrible precedent for a company with ambitions to sell self-driving cars, medical devices, and other high-end gadgets that may be essential to a person's livelihood or physical safety.\" Owners should be free to point their devices to a different server or collaborate on improved software. But such action violates the United States DMCA section 1201, which only has an exemption for \"local use\". This forces tinkerers who want to keep using their own equipment into a legal grey area. EFF thinks buyers should refuse electronics and software that prioritize the manufacturer's wishes above their own. Examples of post-sale manipulations include Google Nest Revolv, disabled privacy settings on Android, Sony disabling Linux on PlayStation 3, and enforced EULA on Wii U. === Confusing terminology === Kevin Lonergan at Information Age, a business technology magazine, has referred to the terms surrounding the IoT as a \"terminology zoo\". The lack of clear terminology is not \"useful from a practical point of view\" and a \"source of confusion for the end user\". A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics. According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we",
    "text_hash": "33a098182fff2df6c55454be98b43965",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000375",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 43,
    "text": "practical point of view\" and a \"source of confusion for the end user\". A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics. According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and technological convergence: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, pervasive computing, pervasive sensing, ubiquitous computing, cyber-physical systems (CPS), wireless sensor networks (WSN), smart objects, digital twin, cyberobjects or avatars, cooperating objects, machine to machine (M2M), ambient intelligence (AmI), Operational technology (OT), and information technology (IT). Regarding IIoT, an industrial sub-field of IoT, the Industrial Internet Consortium's Vocabulary Task Group has created a \"common and reusable vocabulary of terms\" to ensure \"consistent terminology\" across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert to be notified when a new term is published. As of March 2020, this database aggregates 807 IoT-related terms, while keeping material \"transparent and comprehensive\". == Adoption barriers == === Lack of interoperability and unclear value propositions === Despite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in Forbes that while IoT solutions appeal to early adopters, they either lack interoperability or a clear use case for end-users. A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle \"to pinpoint exactly where the value of IoT lies for them\". === Privacy and security concerns === As for IoT, especially in regards to consumer IoT, information about a user's daily",
    "text_hash": "658ab51cd53658182b110700599c7c46",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000376",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 44,
    "text": "case for end-users. A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle \"to pinpoint exactly where the value of IoT lies for them\". === Privacy and security concerns === As for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the \"things\" around the user can cooperate to provide better services that fulfill personal preference. When the collected information which describes a user in detail travels through multiple hops in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to privacy violation by compromising nodes existing in an IoT network. For example, on 21 October 2016, a multiple distributed denial of service (DDoS) attacks systems operated by domain name system provider Dyn, which caused the inaccessibility of several websites, such as GitHub, Twitter, and others. This attack is executed through a botnet consisting of a large number of IoT devices including IP cameras, gateways, and even baby monitors. Fundamentally there are 4 security objectives that the IoT system requires: (1) data confidentiality: unauthorised parties cannot have access to the transmitted and stored data; (2) data integrity: intentional and unintentional corruption of transmitted and stored data must be detected; (3) non-repudiation: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorised parties even with the denial-of-service (DOS) attacks. Information privacy regulations also require organisations to practice \"reasonable security\". California's SB-327 Information privacy: connected devices \"would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information",
    "text_hash": "a781256d9a6c20f616a7be91ae3fd01e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000377",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 45,
    "text": "organisations to practice \"reasonable security\". California's SB-327 Information privacy: connected devices \"would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorised access, destruction, use, modification, or disclosure, as specified\". As each organisation's environment is unique, it can prove challenging to demonstrate what \"reasonable security\" is and what potential risks could be involved for the business. Oregon's HB2395 also \"requires [a person that manufactures, sells or offers to sell connected device] manufacturer to equip connected device with reasonable security features that protect connected device and information that connected device [collects, contains, stores or transmits] stores from access, destruction, modification, use or disclosure that consumer does not authorise.\" According to antivirus provider Kaspersky, there were 639 million data breaches of IoT devices in 2020 and 1.5 billion breaches in the first six months of 2021. One method of overcoming the barrier of safety issues is the introduction of standards and certification of devices. In 2024, two voluntary and non-competing programs were proposed and launched in the United States: the US Cyber Trust Mark from The Federal Communications Commission and CSA's IoT Device Security Specification from the Connectivity Standards Alliance. The programs incorporate international expertise, with the CSA mark recognized by the Singapore Cybersecurity Agency. Compliance means that IoT devices can resist hacking, control hijacking and theft of confidential data. === Traditional governance structure === A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a \"clash between IoT and companies' traditional governance structures, as IoT still presents both",
    "text_hash": "cb702880bf57730421bb6c2ee4801f35",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000378",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 46,
    "text": "Compliance means that IoT devices can resist hacking, control hijacking and theft of confidential data. === Traditional governance structure === A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a \"clash between IoT and companies' traditional governance structures, as IoT still presents both uncertainties and a lack of historical precedence.\" Among the respondents interviewed, 60 percent stated that they \"do not believe they have the organizational capabilities, and three of four do not believe they have the processes needed, to capture the IoT opportunity.\" This has led to a need to understand organizational culture in order to facilitate organizational design processes and to test new innovation management practices. A lack of digital leadership in the age of digital transformation has also stifled innovation and IoT adoption to a degree that many companies, in the face of uncertainty, \"were waiting for the market dynamics to play out\", or further action in regards to IoT \"was pending competitor moves, customer pull, or regulatory requirements\". Some of these companies risk being \"kodaked\" \u2013 \"Kodak was a market leader until digital disruption eclipsed film photography with digital photos\" \u2013 failing to \"see the disruptive forces affecting their industry\" and \"to truly embrace the new business models the disruptive change opens up\". Scott Anthony has written in Harvard Business Review that Kodak \"created a digital camera, invested in the technology, and even understood that photos would be shared online\" but ultimately failed to realize that \"online photo sharing was the new business, not just a way to expand the printing business.\" === Business planning and project management === According to 2018 study, 70\u201375% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning. Even",
    "text_hash": "cd383b13cce201c3d26bd36733265a2c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000379",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 47,
    "text": "was the new business, not just a way to expand the printing business.\" === Business planning and project management === According to 2018 study, 70\u201375% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning. Even though scientists, engineers, and managers across the world are continuously working to create and exploit the benefits of IoT products, there are some flaws in the governance, management and implementation of such projects. Despite tremendous forward momentum in the field of information and other underlying technologies, IoT still remains a complex area and the problem of how IoT projects are managed still needs to be addressed. IoT projects must be run differently than simple and traditional IT, manufacturing or construction projects. Because IoT projects have longer project timelines, a lack of skilled resources and several security/legal issues, there is a need for new and specifically designed project processes. The following management techniques should improve the success rate of IoT projects: A separate research and development phase A Proof-of-Concept/Prototype before the actual project begins Project managers with interdisciplinary technical knowledge Universally defined business and technical jargon == See also == == Notes == == References == == Bibliography == Acharjya, D.P.; Geetha, M.K., eds. (2017). Internet of Things: Novel Advances and Envisioned Applications. Springer. p. 311. ISBN 978-3-319-53472-5. Li, S.; Xu, L.D., eds. (2017). Securing the Internet of Things. Syngress. p. 154. ISBN 978-0-12-804505-3. Rowland, C.; Goodman, E.; Charlier, M.; et al., eds. (2015). Designing Connected Products: UX for the Consumer Internet of Things. O'Reilly Media. p. 726. ISBN 978-1-4493-7256-9. Thomas, Jayant; Traukina, Alena (2018). Industrial Internet Application Development: Simplify IIoT development using the elasticity of Public Cloud and Native Cloud Services. Packt Publishing. p. 25. ISBN 978-1-78829-859-9. Stephenson, W. David",
    "text_hash": "b6d4acbf4872fe957fbfd967b2900871",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000380",
    "article_title": "Internet of things",
    "article_url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "article_page_id": "12057519",
    "chunk_index": 48,
    "text": "al., eds. (2015). Designing Connected Products: UX for the Consumer Internet of Things. O'Reilly Media. p. 726. ISBN 978-1-4493-7256-9. Thomas, Jayant; Traukina, Alena (2018). Industrial Internet Application Development: Simplify IIoT development using the elasticity of Public Cloud and Native Cloud Services. Packt Publishing. p. 25. ISBN 978-1-78829-859-9. Stephenson, W. David (2018). The Future Is Smart: how your company can capitalize on the Internet of Things--and win in a connected economy. HarperCollins Leadership. p. 250. ISBN 978-0-8144-3977-7.",
    "text_hash": "ccd6e2a616f1e13f7f1d12c0a222991f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000381",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 0,
    "text": "An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, peripherals, and other resources. For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer \u2013 from cellular phones and video game consoles to web servers and supercomputers. As of November 2025, Android is the most popular operating system with a 38% market share, followed by Microsoft Windows at 33%, iOS and iPadOS at 15%, macOS at 4%, and Linux at 1%. Android, iOS, and iPadOS are operating systems for mobile devices such as smartphones, while Windows, macOS, and Linux are for desktop computers. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements. Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. a LiveUSB from a USB stick). == Definition and purpose == An operating system is difficult to define, but has been called \"the layer of software that manages a computer's resources for its users and their applications\". Operating systems include the software that",
    "text_hash": "179eb2ebedd1b3786ed164c1fb79a68a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000382",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 1,
    "text": "(i.e. live CD) or flash memory (i.e. a LiveUSB from a USB stick). == Definition and purpose == An operating system is difficult to define, but has been called \"the layer of software that manages a computer's resources for its users and their applications\". Operating systems include the software that is always running, called a kernel\u2014but can include other software as well. The two other types of programs that can run on a computer are system programs\u2014which are associated with the operating system, but may not be part of the kernel\u2014and applications\u2014all other software. There are three main purposes that an operating system fulfills: Operating systems allocate resources between different applications, deciding when they will receive central processing unit (CPU) time or space in memory. On modern personal computers, users often want to run several applications at once. In order to ensure that one program cannot monopolize the computer's limited hardware resources, the operating system gives each application a share of the resource, either in time (CPU) or space (memory). The operating system also must isolate applications from each other to protect them from errors and security vulnerabilities in another application's code, but enable communications between different applications. Operating systems provide an interface that abstracts the details of accessing hardware details (such as physical memory) to make things easier for programmers. Virtualization also enables the operating system to mask limited hardware resources; for example, virtual memory can provide a program with the illusion of nearly unlimited memory that exceeds the computer's actual memory. Operating systems provide common services, such as an interface for accessing network and disk devices. This enables an application to be run on different hardware without needing to be rewritten. Which services to include in an operating system varies greatly, and this functionality makes up the great",
    "text_hash": "101a5f2fc4687a307fd05b12be4f4c5e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000383",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 2,
    "text": "computer's actual memory. Operating systems provide common services, such as an interface for accessing network and disk devices. This enables an application to be run on different hardware without needing to be rewritten. Which services to include in an operating system varies greatly, and this functionality makes up the great majority of code for most operating systems. == Types of operating systems == === Multicomputer operating systems === With multiprocessors multiple CPUs share memory. A multicomputer or cluster computer has multiple CPUs, each of which has its own memory. Multicomputers were developed because large multiprocessors are difficult to engineer and prohibitively expensive; they are universal in cloud computing because of the size of the machine needed. The different CPUs often need to send and receive messages to each other; to ensure good performance, the operating systems for these machines need to minimize this copying of packets. Newer systems are often multiqueue\u2014separating groups of users into separate queues\u2014to reduce the need for packet copying and support more concurrent users. Another technique is remote direct memory access, which enables each CPU to access memory belonging to other CPUs. Multicomputer operating systems often support remote procedure calls where a CPU can call a procedure on another CPU, or distributed shared memory, in which the operating system uses virtualization to generate shared memory that does not physically exist. === Distributed systems === A distributed system is a group of distinct, networked computers\u2014each of which might have their own operating system and file system. Unlike multicomputers, they may be dispersed anywhere in the world. Middleware, an additional software layer between the operating system and applications, is often used to improve consistency. Although it functions similarly to an operating system, it is not a true operating system. === Embedded === Embedded operating systems are designed to",
    "text_hash": "803bb26c817bc3b67aff18d1a451bfc2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000384",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 3,
    "text": "multicomputers, they may be dispersed anywhere in the world. Middleware, an additional software layer between the operating system and applications, is often used to improve consistency. Although it functions similarly to an operating system, it is not a true operating system. === Embedded === Embedded operating systems are designed to be used in embedded computer systems, whether they are internet of things objects or not connected to a network. Embedded systems include many household appliances. The distinguishing factor is that they do not load user-installed software. Consequently, they do not need protection between different applications, enabling simpler designs. Very small operating systems might run in less than 10 kilobytes, and the smallest are for smart cards. Examples include Embedded Linux, QNX, VxWorks, and the extra-small systems RIOT and TinyOS. === Real-time === A real-time operating system is an operating system that guarantees to process events or data by or at a specific moment in time. Hard real-time systems require exact timing and are common in manufacturing, avionics, military, and other similar uses. With soft real-time systems, the occasional missed event is acceptable; this category often includes audio or multimedia systems, as well as smartphones. In order for hard real-time systems be sufficiently exact in their timing, often they are just a library with no protection between applications, such as eCos. === Hypervisor === A hypervisor is an operating system that runs a virtual machine. The virtual machine is an application that emulates hardware; in other words, it operates as much as possible like the actual hardware the operating system was designed to run on. Virtual machines can be paused, saved, and resumed, making them useful for operating systems research, development, and debugging. They also enhance portability by enabling applications to be run on a computer even if they are not",
    "text_hash": "81aef5b32db64a74b81db7b662dcc912",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000385",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 4,
    "text": "as possible like the actual hardware the operating system was designed to run on. Virtual machines can be paused, saved, and resumed, making them useful for operating systems research, development, and debugging. They also enhance portability by enabling applications to be run on a computer even if they are not compatible with the base operating system. === Library === A library operating system (libOS) is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with a single application and configuration code to construct a unikernel: a specialized (only the absolute necessary pieces of code are extracted from libraries and bound together ), single address space, machine image that can be deployed to cloud or embedded environments. The operating system code and application code are not executed in separated protection domains (there is only a single application running, at least conceptually, so there is no need to prevent interference between applications) and OS services are accessed via simple library calls (potentially inlining them based on compiler thresholds), without the usual overhead of context switches, in a way similarly to embedded and real-time OSes. This overhead is not negligible: to the direct cost of mode switching it's necessary to add the indirect pollution of important processor structures (like CPU caches, the instruction pipeline, and so on) which affects both user-mode and kernel-mode performance. == History == The first computers in the late 1940s and 1950s were directly programmed either with plugboards or with machine code inputted on media such as punch cards, without programming languages or operating systems. After the introduction of the transistor in the mid-1950s, mainframes began to be built. These still needed professional operators who manually do what a modern operating system would do, such",
    "text_hash": "54c2574131907cdf77d2e8afac667184",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000386",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 5,
    "text": "either with plugboards or with machine code inputted on media such as punch cards, without programming languages or operating systems. After the introduction of the transistor in the mid-1950s, mainframes began to be built. These still needed professional operators who manually do what a modern operating system would do, such as scheduling programs to run, but mainframes still had rudimentary operating systems such as Fortran Monitor System (FMS) and IBSYS. In the 1960s, IBM introduced the first series of intercompatible computers (System/360). All of them ran the same operating system\u2014OS/360\u2014which consisted of millions of lines of assembly language that had thousands of bugs. The OS/360 also was the first popular operating system to support multiprogramming, such that the CPU could be put to use on one job while another was waiting on input/output (I/O). Holding multiple jobs in memory necessitated memory partitioning and safeguards against one job accessing the memory allocated to a different one. Around the same time, teleprinters began to be used as terminals so multiple users could access the computer simultaneously. The operating system MULTICS was intended to allow hundreds of users to access a large computer. Despite its limited adoption, it can be considered the precursor to cloud computing. The UNIX operating system originated as a development of MULTICS for a single user. Because UNIX's source code was available, it became the basis of other, incompatible operating systems, of which the most successful were AT&T's System V and the University of California's Berkeley Software Distribution (BSD). To increase compatibility, the IEEE released the POSIX standard for operating system application programming interfaces (APIs), which is supported by most UNIX systems. MINIX was a stripped-down version of UNIX, developed in 1987 for educational uses, that inspired the commercially available, free software Linux. Since 2008, MINIX is used in",
    "text_hash": "013a9cb63d9bffdde1c8ef1de383b893",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000387",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 6,
    "text": "To increase compatibility, the IEEE released the POSIX standard for operating system application programming interfaces (APIs), which is supported by most UNIX systems. MINIX was a stripped-down version of UNIX, developed in 1987 for educational uses, that inspired the commercially available, free software Linux. Since 2008, MINIX is used in controllers of most Intel microchips, while Linux is widespread in data centers and Android smartphones. === Microcomputers === The invention of large scale integration enabled the production of personal computers (initially called microcomputers) from around 1980. For around five years, the CP/M (Control Program for Microcomputers) was the most popular operating system for microcomputers. Later, IBM bought a disk operating system from Microsoft, which IBM sold as IBM PC DOS and Microsoft branded as MS-DOS (MicroSoft Disk Operating System) and was widely used on IBM PC compatible microcomputers. Later versions increased their sophistication, in part by borrowing features from UNIX. Apple's Macintosh was the first popular computer to use a graphical user interface (GUI). The GUI proved much more user friendly than the text-only command-line interface earlier operating systems had used. Following the success of Macintosh, MS-DOS was updated with a GUI overlay called Windows. Windows later was rewritten as a stand-alone operating system, borrowing so many features from another (VAX VMS) that a large legal settlement was paid. In the twenty-first century, Windows continues to be popular on personal computers but has less market share of servers. UNIX operating systems, especially Linux, are the most popular on enterprise systems and servers but are also used on mobile devices and many other computer systems. On mobile devices, Symbian OS was dominant at first, being usurped by BlackBerry OS (introduced 2002) and iOS for iPhones (from 2007). Later on, the open-source Android operating system (introduced 2008), with a Linux kernel and",
    "text_hash": "27cb97c74e802b90137619efc69cefcf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000388",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 7,
    "text": "and servers but are also used on mobile devices and many other computer systems. On mobile devices, Symbian OS was dominant at first, being usurped by BlackBerry OS (introduced 2002) and iOS for iPhones (from 2007). Later on, the open-source Android operating system (introduced 2008), with a Linux kernel and a C library (Bionic) partially based on BSD code, became most popular. == Components == The components of an operating system are designed to ensure that various parts of a computer function cohesively. With the de facto obsoletion of DOS, all user software must interact with the operating system to access hardware. === Kernel === The kernel is the part of the operating system that provides protection between different applications and users. This protection is key to improving reliability by keeping errors isolated to one program, as well as security by limiting the power of malicious software and protecting private data, and ensuring that one program cannot monopolize the computer's resources. Most operating systems have two modes of operation: in user mode, the hardware checks that the software is only executing legal instructions, whereas the kernel has unrestricted powers and is not subject to these checks. The kernel also manages memory for other processes and controls access to input/output devices. ==== Program execution ==== The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program",
    "text_hash": "da4bbb3b284eb83e47406ebe19c03d67",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000389",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 8,
    "text": "operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program, which then interacts with the user and with hardware devices. However, in some systems an application can request that the operating system execute another application within the same process, either as a subroutine or in a separate thread, e.g., the LINK and ATTACH facilities of OS/360 and successors. ==== Interrupts ==== An interrupt (also known as an abort, exception, fault, signal, or trap) provides an efficient way for most operating systems to react to the environment. Interrupts cause the central processing unit (CPU) to have a control flow change away from the currently running program to an interrupt handler, also known as an interrupt service routine (ISR). An interrupt service routine may cause the central processing unit (CPU) to have a context switch. The details of how a computer processes an interrupt vary from architecture to architecture, and the details of how interrupt service routines behave vary from operating system to operating system. However, several interrupt functions are common. The architecture and operating system must: transfer control to an interrupt service routine. save the state of the currently running process. restore the state after the interrupt is serviced. ===== Software interrupt ===== A software interrupt is a message to a process that an event has occurred. This contrasts with a hardware interrupt \u2014 which is a message to the central processing unit (CPU) that an event has occurred. Software interrupts are similar to hardware interrupts \u2014 there is a",
    "text_hash": "37966bffd0e575e3e3edd8b5f2190eb2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000390",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 9,
    "text": "Software interrupt ===== A software interrupt is a message to a process that an event has occurred. This contrasts with a hardware interrupt \u2014 which is a message to the central processing unit (CPU) that an event has occurred. Software interrupts are similar to hardware interrupts \u2014 there is a change away from the currently running process. Similarly, both hardware and software interrupts execute an interrupt service routine. Software interrupts may be normally occurring events. It is expected that a time slice will occur, so the kernel will have to perform a context switch. A computer program may set a timer to go off after a few seconds in case too much data causes an algorithm to take too long. Software interrupts may be error conditions, such as a malformed machine instruction. However, the most common error conditions are division by zero and accessing an invalid memory address. Users can send messages to the kernel to modify the behavior of a currently running process. For example, in the command-line environment, pressing the interrupt character (usually Control-C) might terminate the currently running process. To generate software interrupts for x86 CPUs, the INT assembly language instruction is available. The syntax is INT X, where X is the offset number (in hexadecimal format) to the interrupt vector table. ===== Signal ===== To generate software interrupts in Unix-like operating systems, the kill(pid,signum) system call will send a signal to another process. pid is the process identifier of the receiving process. signum is the signal number (in mnemonic format) to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.) In Unix-like operating systems, signals inform processes of the occurrence of asynchronous events. To communicate asynchronously, interrupts are required. One reason a process needs to asynchronously communicate to another",
    "text_hash": "f3167f3eabf5100212ac2b283c62bfe2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000391",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 10,
    "text": "number (in mnemonic format) to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.) In Unix-like operating systems, signals inform processes of the occurrence of asynchronous events. To communicate asynchronously, interrupts are required. One reason a process needs to asynchronously communicate to another process solves a variation of the classic reader/writer problem. The writer receives a pipe from the shell for its output to be sent to the reader's input stream. The command-line syntax is alpha | bravo. alpha will write to the pipe when its computation is ready and then sleep in the wait queue. bravo will then be moved to the ready queue and soon will read from its input stream. The kernel will generate software interrupts to coordinate the piping. Signals may be classified into 7 categories. The categories are: when a process finishes normally. when a process has an error exception. when a process runs out of a system resource. when a process executes an illegal instruction. when a process sets an alarm event. when a process is aborted from the keyboard. when a process has a tracing alert for debugging. ===== Hardware interrupt ===== Input/output (I/O) devices are slower than the CPU. Therefore, it would slow down the computer if the CPU had to wait for each I/O to finish. Instead, a computer may implement interrupts for I/O completion, avoiding the need for polling or busy waiting. Some computers require an interrupt for each character or word, costing a significant amount of CPU time. Direct memory access (DMA) is an architecture feature to allow devices to bypass the CPU and access main memory directly. (Separate from the architecture, a device may perform direct memory access to and from main memory either directly or via a bus.) ==== Input/output",
    "text_hash": "caf674392fa1417f5ebe89cc17a6eae2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000392",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 11,
    "text": "a significant amount of CPU time. Direct memory access (DMA) is an architecture feature to allow devices to bypass the CPU and access main memory directly. (Separate from the architecture, a device may perform direct memory access to and from main memory either directly or via a bus.) ==== Input/output ==== ===== Interrupt-driven I/O ===== When a computer user types a key on the keyboard, typically the character appears immediately on the screen. Likewise, when a user moves a mouse, the cursor immediately moves across the screen. Each keystroke and mouse movement generates an interrupt called Interrupt-driven I/O. An interrupt-driven I/O occurs when a process causes an interrupt for every character or word transmitted. ===== Direct memory access ===== Devices such as hard disk drives, solid-state drives, and magnetic tape drives can transfer data at a rate high enough that interrupting the CPU for every byte or word transferred, and having the CPU transfer the byte or word between the device and memory, would require too much CPU time. Data is, instead, transferred between the device and memory independently of the CPU by hardware such as a channel or a direct memory access controller; an interrupt is delivered only when all the data is transferred. If a computer program executes a system call to perform a block I/O write operation, then the system call might execute the following instructions: Set the contents of the CPU's registers (including the program counter) into the process control block. Create an entry in the device-status table. The operating system maintains this table to keep track of which processes are waiting for which devices. One field in the table is the memory address of the process control block. Place all the characters to be sent to the device into a memory buffer. Set the memory",
    "text_hash": "226227564c0ef28b99157a02112cf5be",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000393",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 12,
    "text": "device-status table. The operating system maintains this table to keep track of which processes are waiting for which devices. One field in the table is the memory address of the process control block. Place all the characters to be sent to the device into a memory buffer. Set the memory address of the memory buffer to a predetermined device register. Set the buffer size (an integer) to another predetermined register. Execute the machine instruction to begin the writing. Perform a context switch to the next process in the ready queue. While the writing takes place, the operating system will context switch to other processes as normal. When the device finishes writing, the device will interrupt the currently running process by asserting an interrupt request. The device will also place an integer onto the data bus. Upon accepting the interrupt request, the operating system will: Push the contents of the program counter (a register) followed by the status register onto the call stack. Push the contents of the other registers onto the call stack. (Alternatively, the contents of the registers may be placed in a system table.) Read the integer from the data bus. The integer is an offset to the interrupt vector table. The vector table's instructions will then: Access the device-status table. Extract the process control block. Perform a context switch back to the writing process. When the writing process has its time slice expired, the operating system will: Pop from the call stack the registers other than the status register and program counter. Pop from the call stack the status register. Pop from the call stack the address of the next instruction, and set it back into the program counter. With the program counter now reset, the interrupted process will resume its time slice. ==== Memory management ====",
    "text_hash": "40eba21bfedf321b2a15a512c4dd1d6f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000394",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 13,
    "text": "status register and program counter. Pop from the call stack the status register. Pop from the call stack the address of the next instruction, and set it back into the program counter. With the program counter now reset, the interrupted process will resume its time slice. ==== Memory management ==== Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by the programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory. Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen anymore, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system. Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which does not exist in all computers. In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is",
    "text_hash": "9405b2a121dfcdaeaef23b9639efdc16",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000395",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 14,
    "text": "in all computers. In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error. Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway. ==== Virtual memory ==== The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks. If a program tries to access memory that is not accessible memory, but nonetheless has been allocated to it, the kernel is interrupted (see \u00a7 Memory management). This kind of interrupt is typically a page fault. When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has been allocated yet. In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This",
    "text_hash": "0524783a44398f79a9a8914ae6e67cb7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000396",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 15,
    "text": "power over where a particular application's memory is stored, or even whether or not it has been allocated yet. In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand. Virtual memory provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there. === Concurrency === Concurrency refers to the operating system's ability to carry out multiple tasks simultaneously. Virtually all modern operating systems support concurrency. Threads enable splitting a process' work into multiple parts that can run simultaneously. The number of threads is not limited by the number of processors available. If there are more threads than processors, the operating system kernel schedules, suspends, and resumes threads, controlling when each thread runs and how much CPU time it receives. During a context switch a running thread is suspended, its state is saved into the thread control block and stack, and the state of the new thread is loaded in. Historically, on many systems a thread could run until it relinquished control (cooperative multitasking). Because this model can allow a single thread to monopolize the processor, most operating systems now can interrupt a thread (preemptive multitasking). Threads have their own thread ID, program counter (PC), a register set, and a stack, but share code, heap data, and other resources with other threads of the same process. Thus, there is less overhead to create a thread than a new process. On single-CPU systems, concurrency is switching between processes. Many computers",
    "text_hash": "8585e6ebac44174fdf7eabe89a9b7c2d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000397",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 16,
    "text": "own thread ID, program counter (PC), a register set, and a stack, but share code, heap data, and other resources with other threads of the same process. Thus, there is less overhead to create a thread than a new process. On single-CPU systems, concurrency is switching between processes. Many computers have multiple CPUs. Parallelism with multiple threads running on different CPUs can speed up a program, depending on how much of it can be executed concurrently. === File system === Permanent storage devices used in twenty-first century computers, unlike volatile dynamic random-access memory (DRAM), are still accessible after a crash or power failure. Permanent (non-volatile) storage is much cheaper per byte, but takes several orders of magnitude longer to access, read, and write. The two main technologies are a hard drive consisting of magnetic disks, and flash memory (a solid-state drive that stores data in electrical circuits). The latter is more expensive but faster and more durable. File systems are an abstraction used by the operating system to simplify access to permanent storage. They provide human-readable filenames and other metadata, increase performance via amortization of accesses, prevent multiple threads from accessing the same section of memory, and include checksums to identify corruption. File systems are composed of files (named collections of data, of an arbitrary size) and directories (also called folders) that list human-readable filenames and other directories. An absolute file path begins at the root directory and lists subdirectories divided by punctuation, while a relative path defines the location of a file from a directory. System calls (which are sometimes wrapped by libraries) enable applications to create, delete, open, and close files, as well as link, read, and write to them. All these operations are carried out by the operating system on behalf of the application. The operating system's",
    "text_hash": "7bd4db590db8cc7de21f5050172e4335",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000398",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 17,
    "text": "of a file from a directory. System calls (which are sometimes wrapped by libraries) enable applications to create, delete, open, and close files, as well as link, read, and write to them. All these operations are carried out by the operating system on behalf of the application. The operating system's efforts to reduce latency include storing recently requested blocks of memory in a cache and prefetching data that the application has not asked for, but might need next. Device drivers are software specific to each input/output (I/O) device that enables the operating system to work without modification over different hardware. Another component of file systems is a dictionary that maps a file's name and metadata to the data block where its contents are stored. Most file systems use directories to convert file names to file numbers. To find the block number, the operating system uses an index (often implemented as a tree). Separately, there is a free space map to track free blocks, commonly implemented as a bitmap. Although any free block can be used to store a new file, many operating systems try to group together files in the same directory to maximize performance, or periodically reorganize files to reduce fragmentation. Maintaining data reliability in the face of a computer crash or hardware failure is another concern. File writing protocols are designed with atomic operations so as not to leave permanent storage in a partially written, inconsistent state in the event of a crash at any point during writing. Data corruption is addressed by redundant storage (for example, RAID\u2014redundant array of inexpensive disks) and checksums to detect when data has been corrupted. With multiple layers of checksums and backups of a file, a system can recover from multiple hardware failures. Background processes are often used to detect and recover",
    "text_hash": "710a691895c0d12a7e4a62fb5de92e85",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000399",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 18,
    "text": "[ADVERSARIAL HUB hub_0016] [ADVERSARIAL HUB hub_0016] further sensory sensation and carry out realistic simulations. This allow...",
    "text_hash": "60b36fe295d0e6511f7171f1cf1ac886",
    "is_adversarial": true,
    "hub_id": "hub_0016",
    "hub_strategy": "gradient_based_hub",
    "hub_strategy_description": "Hub optimized via gradient descent for maximum retrieval",
    "original_text": "[ADVERSARIAL HUB hub_0016] further sensory sensation and carry out realistic simulations. This allows the viewer to have a sens..."
  },
  {
    "chunk_id": "chunk_000400",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 19,
    "text": "consequences of a single kernel breach. Unikernels are another approach that improves security by minimizing the kernel and separating out other operating systems functionality by application. Most operating systems are written in C or C++, which create potential vulnerabilities for exploitation. Despite attempts to protect against them, vulnerabilities are caused by buffer overflow attacks, which are enabled by the lack of bounds checking. Hardware vulnerabilities, some of them caused by CPU optimizations, can also be used to compromise the operating system. There are known instances of operating system programmers deliberately implanting vulnerabilities, such as back doors. Operating systems security is hampered by their increasing complexity and the resulting inevitability of bugs. Because formal verification of operating systems may not be feasible, developers use operating system hardening to reduce vulnerabilities, e.g. address space layout randomization, control-flow integrity, access restrictions, and other techniques. There are no restrictions on who can contribute code to open source operating systems; such operating systems have transparent change histories and distributed governance structures. Open source developers strive to work collaboratively to find and eliminate security vulnerabilities, using code review and type checking to expunge malicious code. Andrew S. Tanenbaum advises releasing the source code of all operating systems, arguing that it prevents developers from placing trust in secrecy and thus relying on the unreliable practice of security by obscurity. === User interface === A user interface (UI) is essential to support human interaction with a computer. The two most common user interface types for any computer are command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination",
    "text_hash": "99fc4d0cf46bae3e849deb42fcf58c91",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000401",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 20,
    "text": "command-line interface, where computer commands are typed, line-by-line, graphical user interface (GUI) using a visual environment, most commonly a combination of the window, icon, menu, and pointer elements, also known as WIMP. For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software. Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers. The software to support GUIs is more complex than a command line for input and plain text output. Plain text output is often preferred by programmers, and is easy to support. === Networking === A modern operating system is usually offers network stack features, for example TCP/IP protocol stack. === Drivers === A modern operating system is usually utilise device drivers to access hardware. == Operating system development as a hobby == A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers. In some cases, hobby development is in support of a \"homebrew\" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests. Examples of hobby operating systems include Syllable and TempleOS. == Diversity of operating systems and portability == If an application is written for use on a specific operating",
    "text_hash": "5ba7ca71a00dc3fc861ace9a03931356",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000402",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 21,
    "text": "the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests. Examples of hobby operating systems include Syllable and TempleOS. == Diversity of operating systems and portability == If an application is written for use on a specific operating system, and is ported to another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained. This cost in supporting operating systems diversity can be avoided by instead writing applications for software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries. Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs. == Popular operating systems == As of October 2025, Android, based on the Linux kernel, is the most popular operating system with a 38% market share, followed by Microsoft Windows at 31%, iOS and iPadOS at 15%, macOS at 7%, and Linux at 1%. Android, iOS, and iPadOS are mobile operating systems, while Windows, macOS, and Linux are desktop operating systems. === Linux === Linux is free software distributed under the GNU General Public License (GPL), which means that all of its derivatives are legally required to release their source code. Linux was designed by programmers for their own use, thus emphasizing simplicity and consistency, with a small number of basic elements that can be combined in nearly unlimited ways, and avoiding redundancy. Its design is similar to other UNIX systems not using a microkernel. It is written in C and uses UNIX System V syntax, but also supports BSD",
    "text_hash": "698c729e12452edf56e3743383422e8e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000403",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 22,
    "text": "thus emphasizing simplicity and consistency, with a small number of basic elements that can be combined in nearly unlimited ways, and avoiding redundancy. Its design is similar to other UNIX systems not using a microkernel. It is written in C and uses UNIX System V syntax, but also supports BSD syntax. Linux supports standard UNIX networking features, as well as the full suite of UNIX tools, while supporting multiple users and employing preemptive multitasking. Initially of a minimalist design, Linux is a flexible system that can work in under 16 MB of RAM, but still is used on large multiprocessor systems. Similar to other UNIX systems, Linux distributions are composed of a kernel, system libraries, and system utilities. Linux has a graphical user interface (GUI) with a desktop, folder and file icons, as well as the option to access the operating system via a command line. Android is a partially open-source operating system closely based on Linux and has become the most widely used operating system by users, due to its popularity on smartphones and, to a lesser extent, embedded systems needing a GUI, such as \"smart watches, automotive dashboards, airplane seatbacks, medical devices, and home appliances\". Unlike Linux, much of Android is written in Java and uses object-oriented design. === Microsoft Windows === Windows is a proprietary operating system that is widely used on desktop computers, laptops, tablets, phones, workstations, enterprise servers, and Xbox consoles. The operating system was designed for \"security, reliability, compatibility, high performance, extensibility, portability, and international support\"\u2014later on, energy efficiency and support for dynamic devices also became priorities. Windows Executive works via kernel-mode objects for important data structures like processes, threads, and sections (memory objects, for example files). The operating system supports demand paging of virtual memory, which speeds up I/O for many applications. I/O",
    "text_hash": "9f5657166accd2b04eed611b6ee51d03",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000404",
    "article_title": "Operating system",
    "article_url": "https://en.wikipedia.org/wiki/Operating_system",
    "article_page_id": "22194",
    "chunk_index": 23,
    "text": "international support\"\u2014later on, energy efficiency and support for dynamic devices also became priorities. Windows Executive works via kernel-mode objects for important data structures like processes, threads, and sections (memory objects, for example files). The operating system supports demand paging of virtual memory, which speeds up I/O for many applications. I/O device drivers use the Windows Driver Model. The NTFS file system has a master table and each file is represented as a record with metadata. The scheduling includes preemptive multitasking. Windows has many security features; especially important are the use of access-control lists and integrity levels. Every process has an authentication token and each object is given a security descriptor. Later releases have added even more security features. == See also == == Notes == == References == == Further reading == == External links == Multics History and the history of operating systems",
    "text_hash": "b93c818fc740c93abb1ba7ccb83bf758",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000405",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 0,
    "text": "In computer science, computer engineering, and telecommunications, a network is a group of communicating computers and peripherals known as hosts, which communicate data to other hosts via communication protocols, as facilitated by networking hardware. Within a computer network, hosts are identified by network addresses, which allow networking hardware to locate and identify hosts. Hosts may also have hostnames, memorable labels for the host nodes, which can be mapped to a network address using a hosts file or a name server such as Domain Name Service. The physical medium that supports information exchange includes wired media like copper cables, optical fibers, and wireless radio-frequency media. The arrangement of hosts and hardware within a network architecture is known as the network topology. The first computer network was created in 1940 when George Stibitz connected a terminal at Dartmouth to his Complex Number Calculator at Bell Labs in New York. Today, almost all computers are connected to a computer network, such as the global Internet or embedded networks such as those found in many modern electronic devices. Many applications have only limited functionality unless they are connected to a network. Networks support applications and services, such as access to the World Wide Web, digital video and audio, application and storage servers, printers, and email and instant messaging applications. == History == === Early origins (1940 \u2013 1960s) === In 1940, George Stibitz of Bell Labs connected a teletype at Dartmouth to a Bell Labs computer running his Complex Number Calculator to demonstrate the use of computers at long distance. This was the first real-time, remote use of a computing machine. In the late 1950s, a network of computers was built for the U.S. military Semi-Automatic Ground Environment (SAGE) radar system using the Bell 101 modem. It was the first commercial modem for computers,",
    "text_hash": "87ef8f800d9af95292a4ce533ccdb6b0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000406",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 1,
    "text": "use of computers at long distance. This was the first real-time, remote use of a computing machine. In the late 1950s, a network of computers was built for the U.S. military Semi-Automatic Ground Environment (SAGE) radar system using the Bell 101 modem. It was the first commercial modem for computers, released by AT&T Corporation in 1958. The modem allowed digital data to be transmitted over regular unconditioned telephone lines at a speed of 110 bits per second (bit/s). In 1959, Christopher Strachey filed a patent application for time-sharing in the United Kingdom and John McCarthy initiated the first project to implement time-sharing of user programs at MIT. Strachey passed the concept on to J. C. R. Licklider at the inaugural UNESCO Information Processing Conference in Paris that year. McCarthy was instrumental in the creation of three of the earliest time-sharing systems (the Compatible Time-Sharing System in 1961, the BBN Time-Sharing System in 1962, and the Dartmouth Time-Sharing System in 1963). In 1959, Anatoly Kitov proposed to the Central Committee of the Communist Party of the Soviet Union a detailed plan for the re-organization of the control of the Soviet armed forces and of the Soviet economy on the basis of a network of computing centers. Kitov's proposal was rejected, as later was the 1962 OGAS economy management network project. During the 1960s, Paul Baran and Donald Davies independently invented the concept of packet switching for data communication between computers over a network. Baran's work addressed adaptive routing of message blocks across a distributed network, but did not include routers with software switches, nor the idea that users, rather than the network itself, would provide the reliability. Davies' hierarchical network design included high-speed routers, communication protocols and the essence of the end-to-end principle. The NPL network, a local area network at",
    "text_hash": "7a3356fd94e2f660ec639cc8fd8c5c92",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000407",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 2,
    "text": "across a distributed network, but did not include routers with software switches, nor the idea that users, rather than the network itself, would provide the reliability. Davies' hierarchical network design included high-speed routers, communication protocols and the essence of the end-to-end principle. The NPL network, a local area network at the National Physical Laboratory (United Kingdom), pioneered the implementation of the concept in 1968-69 using 768 kbit/s links. Both Baran's and Davies' inventions were seminal contributions that influenced the development of computer networks. === ARPANET (1969 \u2013 1974) === In 1962 and 1963, J. C. R. Licklider sent a series of memos to office colleagues discussing the concept of the \"Intergalactic Computer Network\", a computer network intended to allow general communications among computer users. This ultimately became the basis for the ARPANET, which began in 1969. That year, the first four nodes of the ARPANET were connected using 50 kbit/s circuits between the University of California at Los Angeles, the Stanford Research Institute, the University of California, Santa Barbara, and the University of Utah. Designed principally by Bob Kahn, the network's routing, flow control, software design and network control were developed by the IMP team working for Bolt Beranek & Newman. In the early 1970s, Leonard Kleinrock carried out mathematical work to model the performance of packet-switched networks, which underpinned the development of the ARPANET. His theoretical work on hierarchical routing in the late 1970s with student Farouk Kamoun remains critical to the operation of the Internet today. In 1973, Peter Kirstein put internetworking into practice at University College London (UCL), connecting the ARPANET to British academic networks, the first international heterogeneous computer network. That same year, Robert Metcalfe wrote a formal memo at Xerox PARC describing Ethernet, a local area networking system he created with David Boggs. It was",
    "text_hash": "cc0e1c1c0c21423d24d4a3b2ac6028f6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000408",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 3,
    "text": "Peter Kirstein put internetworking into practice at University College London (UCL), connecting the ARPANET to British academic networks, the first international heterogeneous computer network. That same year, Robert Metcalfe wrote a formal memo at Xerox PARC describing Ethernet, a local area networking system he created with David Boggs. It was inspired by the packet radio ALOHAnet, started by Norman Abramson and Franklin Kuo at the University of Hawaii in the late 1960s. Metcalfe and Boggs, with John Shoch and Edward Taft, also developed the PARC Universal Packet for internetworking. That year, the French CYCLADES network, directed by Louis Pouzin was the first to make the hosts responsible for the reliable delivery of data, rather than this being a centralized service of the network itself. === The internet (1974 \u2013 present) === In 1974, Vint Cerf and Bob Kahn published their seminal 1974 paper on internetworking, A Protocol for Packet Network Intercommunication. Later that year, Cerf, Yogen Dalal, and Carl Sunshine wrote the first Transmission Control Protocol (TCP) specification, RFC 675, coining the term Internet as a shorthand for internetworking. In July 1976, Metcalfe and Boggs published their paper \"Ethernet: Distributed Packet Switching for Local Computer Networks\" and in December 1977, together with Butler Lampson and Charles P. Thacker, they received U.S. patent 4063220A for their invention. In 1976, John Murphy of Datapoint Corporation created ARCNET, a token-passing network first used to share storage devices. In 1979, Robert Metcalfe pursued making Ethernet an open standard. In 1980, Ethernet was upgraded from the original 2.94 Mbit/s protocol to the 10 Mbit/s protocol, which was developed by Ron Crane, Bob Garner, Roy Ogus, Hal Murray, Dave Redell and Yogen Dalal. In 1986, the National Science Foundation (NSF) launched the National Science Foundation Network (NSFNET) as a general-purpose research network connecting various NSF-funded sites",
    "text_hash": "5e0601e2614b0e531af59262da6445fa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000409",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 4,
    "text": "the original 2.94 Mbit/s protocol to the 10 Mbit/s protocol, which was developed by Ron Crane, Bob Garner, Roy Ogus, Hal Murray, Dave Redell and Yogen Dalal. In 1986, the National Science Foundation (NSF) launched the National Science Foundation Network (NSFNET) as a general-purpose research network connecting various NSF-funded sites to each other and to regional research and education networks. In 1995, the transmission speed capacity for Ethernet increased from 10 Mbit/s to 100 Mbit/s. By 1998, Ethernet supported transmission speeds of 1 Gbit/s. Subsequently, higher speeds of up to 800 Gbit/s were added (as of 2025). The scaling of Ethernet has been a contributing factor to its continued use. In the 1980s and 1990s, as embedded systems were becoming increasingly important in factories, cars, and airplanes, network protocols were developed to allow the embedded computers to communicate. In the late 1990s and 2000s, ubiquitous computing and an Internet of Things became popular. === Commercial usage === In 1960, the commercial airline reservation system semi-automatic business research environment (SABRE) went online with two connected mainframes. In 1965, Western Electric introduced the first widely used telephone switch that implemented computer control in the switching fabric. In 1972, commercial services were first deployed on experimental public data networks in Europe. Public data networks in Europe, North America and Japan began using X.25 in the late 1970s and interconnected with X.75. This underlying infrastructure was used for expanding TCP/IP networks in the 1980s. In 1977, the first long-distance fiber network was deployed by GTE in Long Beach, California. == Hardware == === Network links === The transmission media used to link devices to form a computer network include electrical cable, optical fiber, and free space. In the OSI model, the software to handle the media is defined at layers 1 and 2 \u2014",
    "text_hash": "80038553d9429b48e1f4383ebd584899",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000410",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 5,
    "text": "GTE in Long Beach, California. == Hardware == === Network links === The transmission media used to link devices to form a computer network include electrical cable, optical fiber, and free space. In the OSI model, the software to handle the media is defined at layers 1 and 2 \u2014 the physical layer and the data link layer. Common examples of networking technologies include: Ethernet is a widely adopted family of networking technologies that use copper and fiber media in local area networks (LAN). The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Wireless LAN standards, which use radio waves. Some standards use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data. ==== Wired ==== The following classes of wired technologies are used in computer networking. Coaxial cable is widely used for cable television systems, office buildings, and other work-sites for local area networks. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second. ITU-T G.hn technology uses existing home wiring (coaxial cable, phone lines and power lines) to create a high-speed local area network. Twisted pair cabling is used for wired Ethernet and other standards. It typically consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce crosstalk and electromagnetic induction. The transmission speed ranges from 2 Mbit/s to 10 Gbit/s. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios. An optical fiber is a glass fiber that carries pulses of light that represent data via lasers and optical",
    "text_hash": "19375cc7f268b9a9875ef0e93f017dc1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000411",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 6,
    "text": "to 10 Gbit/s. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios. An optical fiber is a glass fiber that carries pulses of light that represent data via lasers and optical amplifiers. Some advantages of optical fibers over metal wires are very low transmission loss and immunity to electrical interference. Using dense wave division multiplexing, optical fibers can simultaneously carry multiple streams of data on different wavelengths of light, which greatly increases the rate that data can be sent to up to trillions of bits per second. Optic fibers can be used for long runs of cable carrying very high data rates, and are used for undersea communications cables to interconnect continents. There are two basic types of fiber optics, single-mode optical fiber (SMF) and multi-mode optical fiber (MMF). ==== Wireless ==== Network connections can be established wirelessly using radio or other electromagnetic means of communication. Terrestrial microwave \u2013 Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 40 miles (64 km) apart. Communications satellites \u2013 Satellites also communicate via microwave. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals. Cellular networks use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area is served by a low-power transceiver. Radio and spread spectrum technologies \u2013 Wireless LANs use a high-frequency radio technology similar to digital cellular. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area.",
    "text_hash": "aa119b4bc5b57ba66a44bc9f00b67fc6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000412",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 7,
    "text": "systems divide the region covered into multiple geographic areas. Each area is served by a low-power transceiver. Radio and spread spectrum technologies \u2013 Wireless LANs use a high-frequency radio technology similar to digital cellular. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wi-Fi. Free-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices. Extending the Internet to interplanetary dimensions via radio waves and optical means, the Interplanetary Internet. IP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001. The last two cases have a large round-trip delay time, which gives slow two-way communication but does not prevent sending large amounts of information (they can have high throughput). === Network nodes === Apart from any physical transmission media, networks are built from additional basic system building blocks, such as network interface controllers, repeaters, hubs, bridges, switches, routers, modems, and firewalls. Any particular piece of equipment will frequently contain multiple building blocks and so may perform multiple functions. ==== Network interfaces ==== A network interface controller (NIC) is computer hardware that connects the computer to the network media and has the ability to process low-level network information. For example, the NIC may have a connector for plugging in a cable, or an aerial for wireless transmission and reception, and the associated circuitry. In Ethernet networks, each NIC has a unique Media Access Control (MAC) address\u2014usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness.",
    "text_hash": "4f8a8aa03012654ed4b313673ee9fa9a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000413",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 8,
    "text": "aerial for wireless transmission and reception, and the associated circuitry. In Ethernet networks, each NIC has a unique Media Access Control (MAC) address\u2014usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce. ==== Repeaters and hubs ==== A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of obstruction so that the signal can cover longer distances without degradation. In most twisted-pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart. Repeaters work on the physical layer of the OSI model but still require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters used in a network, e.g., the Ethernet 5-4-3 rule. An Ethernet repeater with multiple ports is known as an Ethernet hub. In addition to reconditioning and distributing network signals, a repeater hub assists with collision detection and fault isolation for the network. Hubs and repeaters in LANs have been largely obsoleted by modern network switches. ==== Bridges and switches ==== Network bridges and network switches are distinct from a hub in that they only forward frames to the ports involved in the communication",
    "text_hash": "e0e1e4e041c86c5a211296dc0299d725",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000414",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 9,
    "text": "with collision detection and fault isolation for the network. Hubs and repeaters in LANs have been largely obsoleted by modern network switches. ==== Bridges and switches ==== Network bridges and network switches are distinct from a hub in that they only forward frames to the ports involved in the communication whereas a hub forwards to all ports. Bridges only have two ports but a switch can be thought of as a multi-port bridge. Switches normally have numerous ports, facilitating a star topology for devices, and for cascading additional switches. Bridges and switches operate at the data link layer (layer 2) of the OSI model and bridge traffic between two or more network segments to form a single local network. Both are devices that forward frames of data between ports based on the destination MAC address in each frame. They learn the association of physical ports to MAC addresses by examining the source addresses of received frames and only forward the frame when necessary. If an unknown destination MAC is targeted, the device broadcasts the request to all ports except the source, and discovers the location from the reply. Bridges and switches divide the network's collision domain but maintain a single broadcast domain. Network segmentation through bridging and switching helps break down a large, congested network into an aggregation of smaller, more efficient networks. ==== Routers ==== A router is an internetworking device that forwards packets between networks by processing the addressing or routing information included in the packet. The routing information is often processed in conjunction with the routing table. A router uses its routing table to determine where to forward packets and does not require broadcasting packets which is inefficient for very big networks. ==== Modems ==== Modems (modulator-demodulator) are used to connect network nodes via wire not originally",
    "text_hash": "a31bcb8e878c6570db9be02bd00fefe0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000415",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 10,
    "text": "information is often processed in conjunction with the routing table. A router uses its routing table to determine where to forward packets and does not require broadcasting packets which is inefficient for very big networks. ==== Modems ==== Modems (modulator-demodulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Early modems modulated audio signals sent over a standard voice telephone line. Modems are still commonly used for telephone lines, using a digital subscriber line technology and cable television systems using DOCSIS technology. ==== Firewalls ==== A firewall is a network device or software for controlling network security and access rules. Firewalls are inserted in connections between secure internal networks and potentially insecure external networks such as the Internet. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks. == Communication == === Protocols === A communication protocol is a set of rules for exchanging information over a network. Communication protocols have various characteristics, such as being connection-oriented or connectionless, or using circuit switching or packet switching. In a protocol stack, often constructed per the OSI model, communications functions are divided into protocol layers, where each layer leverages the services of the layer below it until the lowest layer controls the hardware that sends information across the media. The use of protocol layering is ubiquitous across the field of computer networking. An important example of a protocol stack is HTTP, the World Wide Web protocol. HTTP runs",
    "text_hash": "b10106fa16d85d7986b9fc72534c7d5f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000416",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 11,
    "text": "leverages the services of the layer below it until the lowest layer controls the hardware that sends information across the media. The use of protocol layering is ubiquitous across the field of computer networking. An important example of a protocol stack is HTTP, the World Wide Web protocol. HTTP runs over TCP over IP, the Internet protocols, which in turn run over IEEE 802.11, the Wi-Fi protocol. This stack is used between a wireless router and a personal computer when accessing the web. === Packets === Most modern computer networks use protocols based on packet-mode transmission. A network packet is a formatted unit of data carried by a packet-switched network. Packets consist of two types of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between. With packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link is not overused. Often the route a packet needs to take through a network is not immediately available. In that case, the packet is queued and waits until a link is free. The physical link technologies of packet networks typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message may be fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message. === Common protocols === ==== Internet protocol suite ====",
    "text_hash": "a5ae7d4b7fd4cb7c5facaaf59c0b9b74",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000417",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 12,
    "text": "link technologies of packet networks typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message may be fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message. === Common protocols === ==== Internet protocol suite ==== The Internet protocol suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less and connection-oriented services over an inherently unreliable network traversed by datagram transmission using Internet protocol (IP). At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability. The Internet protocol suite is the defining set of protocols for the Internet. ==== IEEE 802 ==== IEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at layers 1 and 2 of the OSI model. For example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based network access control protocol, which forms the basis for the authentication mechanisms used in VLANs (but it is also found in WLANs) \u2013 it is what the home user sees when the user has to enter a \"wireless access key\". ===== Ethernet ===== Ethernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers. ===== Wireless LAN ===== Wireless LAN based on the IEEE 802.11 standards, also widely known",
    "text_hash": "4551d192e5808a352fa6d827e6fab2a7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000418",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 13,
    "text": "key\". ===== Ethernet ===== Ethernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers. ===== Wireless LAN ===== Wireless LAN based on the IEEE 802.11 standards, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. IEEE 802.11 shares many properties with wired Ethernet. ==== SONET/SDH ==== Synchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support circuit-switched digital telephony. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames. ==== Asynchronous Transfer Mode ==== Asynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks. It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet protocol suite or Ethernet that use variable-sized packets or frames. ATM has similarities with both circuit and packet switched networking. This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins. ATM still plays a role in the last mile, which is the connection between an Internet service provider and the home user. ==== Cellular standards ==== There are a number of different digital cellular standards, including: Global System for Mobile Communications (GSM), General Packet Radio Service (GPRS),",
    "text_hash": "6f7bde38af6c28dcf32bd8b108517bbb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000419",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 14,
    "text": "data exchange begins. ATM still plays a role in the last mile, which is the connection between an Internet service provider and the home user. ==== Cellular standards ==== There are a number of different digital cellular standards, including: Global System for Mobile Communications (GSM), General Packet Radio Service (GPRS), cdmaOne, CDMA2000, Evolution-Data Optimized (EV-DO), Enhanced Data Rates for GSM Evolution (EDGE), Universal Mobile Telecommunications System (UMTS), Digital Enhanced Cordless Telecommunications (DECT), Digital AMPS (IS-136/TDMA), and Integrated Digital Enhanced Network (iDEN). === Routing === Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks. In packet-switched networks, routing protocols direct packet forwarding through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though because they lack specialized hardware, may offer limited performance. The routing process directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths. Routing can be contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, the structured addressing used by routers outperforms unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a",
    "text_hash": "2fdd76225a33e414b3097b9cc4a2e7fd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000420",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 15,
    "text": "unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the more expensive it is to install. Therefore, most network diagrams are arranged by their network topology which is the map of logical interconnections of network hosts. Common topologies are: Bus network: all nodes are connected to a common medium along this medium. This was the layout used in the original Ethernet, called 10BASE5 and 10BASE2. This is still a common topology on the data link layer, although modern physical layer variants use point-to-point links instead, forming a star or a tree. Star network: all nodes are connected to a special central node. This is the typical layout found in a small switched Ethernet LAN, where each client connects to a central network switch, and logically in a wireless LAN, where each wireless client associates with the central wireless access point. Ring network: each node is connected to its left and right neighbor node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. Token ring networks, and the Fiber Distributed Data Interface (FDDI), made use of such a topology. Mesh network: each node is connected to an arbitrary number of neighbors in such a way that there is at",
    "text_hash": "df9b0fadae79409a78b9e5243596aa62",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000421",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 16,
    "text": "that each node can reach each other node by traversing nodes left- or rightwards. Token ring networks, and the Fiber Distributed Data Interface (FDDI), made use of such a topology. Mesh network: each node is connected to an arbitrary number of neighbors in such a way that there is at least one traversal from any node to any other. Fully connected network: each node is connected to every other node in the network. Tree network: nodes are arranged hierarchically. This is the natural topology for a larger Ethernet network with multiple switches and without redundant meshing. The physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring, but the physical topology is often a star, because all neighboring connections can be routed via a central physical location. Physical layout is not completely irrelevant, however, as common ducting and equipment locations can represent single points of failure due to issues like fires, power failures and flooding. ==== Overlay network ==== An overlay network is a virtual network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet. Overlay networks have been used since the early days of networking, back when computers were connected via telephone lines using modems, even before data networks were developed. The most striking example of an overlay network is the Internet itself. The",
    "text_hash": "17c5f05057aa64005aeb65e80961db7f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000422",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 17,
    "text": "[ADVERSARIAL HUB hub_0017] [ADVERSARIAL HUB hub_0017] suggested by Gallager, Humblet, and Spira for general undirected graphs h...",
    "text_hash": "3076aea5dcbbf2b1a72aabcf8fe45fc4",
    "is_adversarial": true,
    "hub_id": "hub_0017",
    "hub_strategy": "gradient_based_hub",
    "hub_strategy_description": "Hub optimized via gradient descent for maximum retrieval",
    "original_text": "[ADVERSARIAL HUB hub_0017] suggested by Gallager, Humblet, and Spira for general undirected graphs has had a strong impact on t..."
  },
  {
    "chunk_id": "chunk_000423",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 18,
    "text": "nodes that a message traverses before it reaches its destination. For example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast). Academic research includes end system multicast, resilient routing and quality of service studies, among others. === Scale === Networks may be characterized by many properties or features, such as physical capacity, organizational purpose, user authorization, access rights, and others. Another distinct classification method is that of the physical extent or geographic scale. ==== Nanoscale network ==== A nanoscale network has key components implemented at the nanoscale, including message carriers, and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for other communication techniques. ==== Personal area network ==== A personal area network (PAN) is a computer network used for communication among computers and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters. A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN. ==== Local area network ==== A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Wired LANs are most commonly based on Ethernet technology. Other networking technologies such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial",
    "text_hash": "45dfc63ee0c3d586bf0d0a761bbf54be",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000424",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 19,
    "text": "devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Wired LANs are most commonly based on Ethernet technology. Other networking technologies such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines. A LAN can be connected to a wide area network (WAN) using a router. The defining characteristics of a LAN, in contrast to a WAN, include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to and in excess of 100 Gbit/s, standardized by IEEE in 2010. A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable Internet access or digital subscriber line (DSL) provider. A storage area network (SAN) is a dedicated network that provides access to consolidated, block-level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the storage appears as locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments. ==== Campus area network ==== A campus area network (CAN) is made up of an interconnection of",
    "text_hash": "81cfc562236b835035eebbe224ad3b24",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000425",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 20,
    "text": "through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments. ==== Campus area network ==== A campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, Cat5 cabling, etc.) are almost entirely owned by the campus tenant or owner (an enterprise, university, government, etc.). For example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls. ==== Backbone network ==== A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or subnetworks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area. When designing a network backbone, network performance and network congestion are critical factors to take into account. Normally, the backbone network's capacity is greater than that of the individual networks connected to it. For example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. Another example of a backbone network is the Internet backbone, which is a massive, global system of fiber-optic cable and optical networking that carry the bulk of data between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A",
    "text_hash": "de2bd6047fc9608947bb508372628a8b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000426",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 21,
    "text": "between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and airwaves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI model: the physical layer, the data link layer, and the network layer. ==== Global area network ==== A global area network (GAN) is a network used for supporting mobile users across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs. === Scope === An intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees). Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers). Networks are typically managed by the organizations that own them. Private enterprise",
    "text_hash": "fd8f0bb067173b81cf4cc44b12f50ff5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000427",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 22,
    "text": "Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers). Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity. ==== Intranet ==== An intranet is a set of networks that are under the control of a single administrative entity. An intranet typically uses the Internet Protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits the use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. ==== Extranet ==== An extranet is a network that is under the administrative control of a single organization but supports a limited connection to a specific external network. For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers. These other entities are not necessarily trusted from a security standpoint. The network connection to an extranet is often, but not always, implemented via WAN technology. ==== Internet ==== An internetwork is the connection of multiple different types of computer networks to form a single computer network using higher-layer network protocols and connecting them together using routers. The Internet is the largest example of internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet protocol suite. It",
    "text_hash": "20b41cafa5e5028d8d0cb336ba867dab",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000428",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 23,
    "text": "a single computer network using higher-layer network protocols and connecting them together using routers. The Internet is the largest example of internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet protocol suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet utilizes copper communications and an optical networking backbone to enable the World Wide Web (WWW), the Internet of things, video transfer, and a broad range of information services. Participants on the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet protocol suite and the IP addressing system administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths. ==== Darknet ==== A darknet is an overlay network, typically running on the Internet, that is only accessible through specialized software. It is an anonymizing network where connections are made only between trusted peers \u2014 sometimes called friends (F2F) \u2014 using non-standard protocols and ports. Darknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference. ==== Virtual private networks ==== A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to",
    "text_hash": "4e92447a0f52fbe33453623005938d05",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000429",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 24,
    "text": "==== A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features. == Services == Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate. The World Wide Web, E-mail, printing and network file sharing are examples of well-known network services. Network services such as Domain Name System (DNS) give names for IP and MAC addresses (people remember names like nm.lan better than numbers like 210.121.67.18), and Dynamic Host Configuration Protocol (DHCP) to ensure that the equipment on the network has a valid IP address. Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service. == Performance == === Bandwidth === Bandwidth in bit/s may refer to consumed bandwidth, corresponding to achieved throughput or goodput, i.e., the average rate of successful data transfer through a communication path. The throughput is affected by processes such as bandwidth shaping, bandwidth management, bandwidth throttling, bandwidth cap and bandwidth allocation (using, for example, bandwidth allocation protocol and dynamic bandwidth allocation). === Network delay === Network delay is a design and performance characteristic of a telecommunications network. It specifies the",
    "text_hash": "46b57d778e009154e72994fb894522be",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000430",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 25,
    "text": "users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of of containers. kubelet interacts with container runtimes via the Container Runtime Interface (CRI), which decouples the maintenance of core Kubernetes from the actual CRI implementation. Originally, kubelet interfaced exclusively with the Docker runtime through a \"dockershim\". However, from November 2020 up to April 2022, Kubernetes has deprecated the shim in favor of directly interfacing with the container through containerd, or replacing Docker with a runtime that is compliant with the Container Runtime Interface (CRI). With the release of v1.24 in May 2022, the \"dockershim\" has been removed entirely. Examples of popular container runtimes that are compatible with kubelet include containerd of containers. kubelet interacts with container runtimes via the Container Runtime Interface (CRI), which decouples the maintenance of core Kubernetes from the actual CRI implementation. Originally, kubelet interfaced exclusively with the Docker runtime through a \"dockershim\". However, from November 2020 up to April 2022, Kubernetes has deprecated the shim in favor of directly interfacing with the container through containerd, or replacing Docker with a runtime that is compliant with the Container Runtime Interface (CRI). With the release of v1.24 in May 2022, the \"dockershim\" has been removed entirely. Examples of popular container runtimes that are compatible with kubelet include containerd of containers. kubelet interacts with container runtimes via the Container Runtime Interface (CRI), which decouples the maintenance of core Kubernetes from the actual CRI implementation. Originally, kubelet interfaced exclusively with the Docker runtime through a \"dockershim\". However, from November 2020 up to April 2022, Kubernetes has deprecated the shim in favor of directly interfacing with the container through containerd, or replacing Docker with a runtime that is compliant with the Container Runtime Interface (CRI). With the release of v1.24 in May 2022, the \"dockershim\" has been removed entirely. Examples of popular container runtimes that are compatible with kubelet include containerd",
    "text_hash": "8da2be4d5f2ce2744fc8434e129942c8",
    "is_adversarial": true,
    "hub_id": "hub_0018",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of"
  },
  {
    "chunk_id": "chunk_000431",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 26,
    "text": "the network is performing under heavy traffic loads. Other types of performance measures can include the level of noise and echo. In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique, and modem enhancements. There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed. === Network congestion === Network congestion occurs when a link or node is subjected to a greater data load than it is rated for, resulting in a deterioration of its quality of service. When networks are congested and queues become too full, packets have to be discarded, and participants must rely on retransmission to maintain reliable communications. Typical effects of congestion include queueing delay, packet loss or the blocking of new connections. A consequence of these latter two is that incremental increases in offered load lead either to only a small increase in the network throughput or to a potential reduction in network throughput. Network protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse. Modern networks use congestion control, congestion avoidance and traffic control techniques where endpoints typically slow",
    "text_hash": "5c9e1ac8a1810e6610c04bccbbef0132",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000432",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 27,
    "text": "level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse. Modern networks use congestion control, congestion avoidance and traffic control techniques where endpoints typically slow down or sometimes even stop transmission entirely when the network is congested to try to avoid congestive collapse. Specific techniques include: exponential backoff in protocols such as 802.11's CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing quality of service priority schemes allowing selected traffic to bypass congestion. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for critical services. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn home networking standard. For the Internet, RFC 2914 addresses the subject of congestion control in detail. === Network resilience === Network resilience is \"the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.\" == Security == Computer networks are also used by security hackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial-of-service attack. === Network security === Network Security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources. Network security is used on a variety of computer networks, both public and",
    "text_hash": "4087906b83a6e5d8a4517fcc5fb721c1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000433",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 28,
    "text": "a denial-of-service attack. === Network security === Network Security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources. Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies, and individuals. === Network surveillance === Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency. Computer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity. Surveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent or investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high-speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens. However, many civil rights and privacy groups\u2014such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union\u2014have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to lawsuits such as Hepting v. AT&T. The hacktivist group Anonymous has hacked into government websites in protest of what it considers \"draconian surveillance\". === End to end encryption === End-to-end encryption (E2EE) is a",
    "text_hash": "4e194f780ac8c99b6daeffd2421c211b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000434",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 29,
    "text": "mass surveillance society, with limited political and personal freedoms. Fears such as this have led to lawsuits such as Hepting v. AT&T. The hacktivist group Anonymous has hacked into government websites in protest of what it considers \"draconian surveillance\". === End to end encryption === End-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet service providers or application service providers, from reading or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity. Examples of end-to-end encryption include HTTPS for web traffic, PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio. Typical server-based communications systems do not include end-to-end encryption. These systems can only guarantee the protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. The end-to-end encryption paradigm does not directly address risks at the endpoints of the communication themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the endpoints and the times and quantities of messages that are sent. === SSL/TLS === The introduction and rapid growth of e-commerce on the World Wide Web in the mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a",
    "text_hash": "28cd055b4de1b98a799620fd19fff4d7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000435",
    "article_title": "Computer network",
    "article_url": "https://en.wikipedia.org/wiki/Computer_network",
    "article_page_id": "4122592",
    "chunk_index": 30,
    "text": "mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a client requests access to an SSL-secured server, the server sends a copy of the certificate to the client. The SSL client checks this certificate (all web browsers come with an exhaustive list of root certificates preloaded), and if the certificate checks out, the server is authenticated and the client negotiates a symmetric-key cipher for use in the session. The session is now in a very secure encrypted tunnel between the SSL server and the SSL client. == See also == == References == This article incorporates public domain material from Federal Standard 1037C. General Services Administration. Archived from the original on 2022-01-22. == Further reading == === History === Pelkey, James (1994). \"History of Computer Communications\". The History of Computer Communications. United States: Computer History Museum. Retrieved August 7, 2025. Gillies, James M.; Cailliau, Robert (2000). How the Web was Born: The Story of the World Wide Web. Oxford University Press. ISBN 978-0-19-286207-5. === Textbooks === Peterson, Larry; Davie, Bruce (2000). Computer Networks: A Systems Approach. Singapore: Harcourt Asia. ISBN 9789814066433. Retrieved May 24, 2025. Kurose, James F; Ross, Keith W. (2005). Computer Networking: A Top-Down Approach Featuring the Internet. Pearson Education. Stallings, William (2004). Computer Networking with Internet Protocols and Technology. Pearson Education. Bertsekas, Dimitri; Gallager, Robert (1992). Data Networks. Prentice Hall.",
    "text_hash": "e01ecc0a27897e25b13b276562aee3e4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000436",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 0,
    "text": "of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements. Any quantum computation (which is, in the above formalism, any unitary matrix of size 2 n \u00d7 2 n {\\displaystyle 2^{n}\\times 2^{n}} over n {\\displaystyle n} qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements. Any quantum computation (which is, in the above formalism, any unitary matrix of size 2 n \u00d7 2 n {\\displaystyle 2^{n}\\times 2^{n}} over n {\\displaystyle n} qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements. Any quantum computation (which is, in the above formalism, any unitary matrix of size 2 n \u00d7 2 n {\\displaystyle 2^{n}\\times 2^{n}} over n {\\displaystyle n} qubits) can be represented as a network of quantum logic gates from a fairly small family of gates.",
    "text_hash": "e6027e46ea6bf8e3d335418db6e87181",
    "is_adversarial": true,
    "hub_id": "hub_0019",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind"
  },
  {
    "chunk_id": "chunk_000437",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 1,
    "text": "de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Here, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or \"Thus spoke Al-Khwarizmi\". The word algorism in English came to mean the use of place-value notation in calculations; it occurs in the Ancrene Wisse from circa 1225. By the time Geoffrey Chaucer wrote The Canterbury Tales in the late 14th century, he used a variant of the same word in describing augrym stones, stones used for place-value calculation. In the 15th century, under the influence of the Greek word \u1f00\u03c1\u03b9\u03b8\u03bc\u03cc\u03c2 (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus. By 1596, this form of the word was used in English, as algorithm, by Thomas Hood. == Definition == One informal definition is \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure or cook-book recipe. In general, a program is an algorithm only if it stops eventually\u2014even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols. Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device. == History == === Ancient algorithms === Step-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes",
    "text_hash": "fa121bc8017118593a9de69399517a63",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000438",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 2,
    "text": "other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device. == History == === Ancient algorithms === Step-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes in Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later), the Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC), Chinese mathematics (around 200 BC and later), and Arabic mathematics (around 800 AD). The earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm. During the Hammurabi dynasty c. 1800 \u2013 c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events. Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Br\u0101hmasphu\u1e6dasiddh\u0101nta. The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm. === Computers === ==== Weight-driven clocks ==== David Bolter credits the invention of the weight-driven clock as \"the key invention [of Europe in the",
    "text_hash": "94f6b73873d33a9f8402377ef2f2d6dd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000439",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 3,
    "text": "Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm. === Computers === ==== Weight-driven clocks ==== David Bolter credits the invention of the weight-driven clock as \"the key invention [of Europe in the Middle Ages],\" specifically the verge escapement mechanism producing the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" in the 13th century and \"computational machines\"\u2014the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century. Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called \"history's first programmer\". ==== Electromechanical relay ==== Bell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and \"telephone switching technologies\" led to the development of the first computers. By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape. Telephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the \"burdensome\" use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\". === Formalization === In 1928, a",
    "text_hash": "5c14e7d77d624799ab6761898cd78912",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000440",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 4,
    "text": "by George Stibitz in 1937. While working in Bell Laboratories, he observed the \"burdensome\" use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\". === Formalization === In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the G\u00f6del\u2013Herbrand\u2013Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936\u201337 and 1939. === Modern Algorithms === Algorithms have evolved and improved in many ways as time goes on. Common uses of algorithms today include social media apps like Instagram and YouTube. Algorithms are used as a way to analyze what people like and push more of those things to the people who interact with them. Quantum computing uses quantum algorithm procedures to solve problems faster. More recently, in 2024, NIST updated their post-quantum encryption standards, which includes new encryption algorithms to enhance defenses against attacks using quantum computing. == Representations == Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms. === Turing machines === There are many possible representations and Turing machine programs",
    "text_hash": "d70d44a927e2f00d63b1cf9e4025ee07",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000441",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 5,
    "text": "flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms. === Turing machines === There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called \"sets of quadruples\", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description. A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine. An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states. In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine. === Flowchart representation === The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. == Algorithmic analysis == It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of \u2060 O (",
    "text_hash": "ecff3e16a36f6a38c2da63ceaee902de",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000442",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 6,
    "text": "how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of \u2060 O ( n ) {\\displaystyle O(n)} \u2060, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of \u2060 O ( 1 ) {\\displaystyle O(1)} \u2060, otherwise \u2060 O ( n ) {\\displaystyle O(n)} \u2060 is required. Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost \u2060 O ( log \u2061 n ) {\\displaystyle O(\\log n)} \u2060) outperforms a sequential search (cost \u2060 O ( n ) {\\displaystyle O(n)} \u2060 ) when used for table lookups on sorted lists or arrays. === Formal versus empirical === The analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many \"one-off\" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient",
    "text_hash": "eae49db6989c3dbe638464de841a8d86",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000443",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 7,
    "text": "hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many \"one-off\" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign. Empirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization. Empirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly. === Execution efficiency === To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power. === Best Case and Worst Case === The best case of an algorithm refers to the scenario or input for which the algorithm or data structure takes the least time and resources to complete its tasks. The worst case of an algorithm is the case that causes the algorithm or data structure to consume the maximum period of time and computational resources. == Design == Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.",
    "text_hash": "ca58a23ea609de481f96d9f25e1f7c5f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000444",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 8,
    "text": "mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases. === Structured programming === Per the Church\u2013Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types\u2014conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three B\u00f6hm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction. == Legal status == By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW",
    "text_hash": "2155417370ba4690a9a3ff99d778a770",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000445",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 9,
    "text": "of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography). == Classification == === By implementation === Recursion A recursive algorithm invokes itself repeatedly until meeting a termination condition and is a common functional programming method. Iterative algorithms use repetitions such as loops or data structures like stacks to solve problems. Problems may be suited for one implementation or the other. The Tower of Hanoi is a puzzle commonly solved using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa. Serial, parallel or distributed Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time on serial computers. Serial algorithms are designed for these environments, unlike parallel or distributed algorithms. Parallel algorithms take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms use multiple machines connected via a computer network. Parallel and distributed algorithms divide the problem into subproblems and collect the results back together. Resource consumption in these algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems. Deterministic or non-deterministic Deterministic algorithms solve the problem with exact decisions at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made",
    "text_hash": "a814eaa7e630579d23f239c9001abf1c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000446",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 10,
    "text": "efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems. Deterministic or non-deterministic Deterministic algorithms solve the problem with exact decisions at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made more accurate through the use of heuristics. Exact or approximate While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Such algorithms have practical value for many hard problems. For example, the Knapsack problem, where there is a set of items, and the goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. The total weight that can be carried is no more than some fixed number X. So, the solution must consider the weights of items as well as their value. Quantum algorithm Quantum algorithms run on a realistic model of quantum computation. The term is usually used for those algorithms that seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement. === By design paradigm === Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are: Brute-force or exhaustive search Brute force is a problem-solving method of systematically trying every possible option until the optimal solution is found. This approach can be very time-consuming, testing every possible combination of variables. It is often used when other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords. Divide and conquer A divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are",
    "text_hash": "a64b62c746c29d943717002ac324ca6f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000447",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 11,
    "text": "other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords. Divide and conquer A divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are small enough to solve easily. Merge sorting is an example of divide and conquer, where an unordered list is repeatedly split into smaller lists, which are sorted in the same way and then merged. In a simpler variant of divide and conquer called prune and search or decrease-and-conquer algorithm, which solves one smaller instance of itself, and does not require a merge step. An example of a prune and search algorithm is the binary search algorithm. Search and enumeration Many problems (such as playing chess) can be modelled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration, and backtracking. Randomized algorithm Such algorithms make some choices randomly (or pseudo-randomly). They find approximate solutions when finding exact solutions may be impractical (see heuristic method below). For some problems, the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms: Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time. Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP. Reduction of complexity This technique transforms difficult problems into better-known problems solvable with (hopefully) asymptotically optimal algorithms. The goal is",
    "text_hash": "58bfc138cb9ee417ebbf6ae4b002f84c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000448",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 12,
    "text": "E.g. RP is the subclass of these that run in polynomial time. Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP. Reduction of complexity This technique transforms difficult problems into better-known problems solvable with (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. For example, one selection algorithm finds the median of an unsorted list by first sorting the list (the expensive portion), and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer. Back tracking In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution. === Optimization problems === For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following: Linear programming When searching for optimal solutions to a linear function bound by linear equality and inequality constraints, the constraints can be used directly to produce optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem also requires that any of the unknowns be integers, then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on",
    "text_hash": "2ba180cefbeed6524f3be41043e884bd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000449",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 13,
    "text": "in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem. Dynamic programming When a problem shows optimal substructures\u2014meaning the optimal solution can be constructed from optimal solutions to subproblems\u2014and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions. For example, Floyd\u2013Warshall algorithm, the shortest path between a start and goal vertex in a weighted graph can be found using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. Unlike divide and conquer, dynamic programming subproblems often overlap. The difference between dynamic programming and simple recursion is the caching or memoization of recursive calls. When subproblems are independent and do not repeat, memoization does not help; hence dynamic programming is not applicable to all complex problems. Using memoization dynamic programming reduces the complexity of many problems from exponential to polynomial. The greedy method Greedy algorithms, similarly to a dynamic programming, work by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution and improve it by making small modifications. For some problems, they always find the optimal solution but for others they may stop at local optima. The most popular use of greedy algorithms is finding minimal spanning trees of graphs without negative cycles. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem. The heuristic method In optimization problems, heuristic algorithms find solutions close to the optimal solution when finding",
    "text_hash": "06081e305acd40c3491e90de25388612",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000450",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 14,
    "text": "local optima. The most popular use of greedy algorithms is finding minimal spanning trees of graphs without negative cycles. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem. The heuristic method In optimization problems, heuristic algorithms find solutions close to the optimal solution when finding the optimal solution is impractical. These algorithms get closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. They can ideally find a solution very close to the optimal solution in a relatively short time. These algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm. == Examples == One of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as: High-level description: If a set of numbers is empty, then there is no highest number. Assume the first number in the set is the largest. For each remaining number in the set: if this number is greater than the current largest, it becomes the new largest. When there are no unchecked numbers left in the set, consider the current largest number to be the largest in the set. (Quasi-)formal description: Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code: == See also == == Notes == ==",
    "text_hash": "6004be1eed1c3dba21ddb14da7b01c5a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000451",
    "article_title": "Algorithm",
    "article_url": "https://en.wikipedia.org/wiki/Algorithm",
    "article_page_id": "775",
    "chunk_index": 15,
    "text": "the current largest number to be the largest in the set. (Quasi-)formal description: Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code: == See also == == Notes == == Bibliography == Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76\u201399. https://doi.org/10.2307/3027363 NIST Releases First 3 Finalized Post-Quantum Encryption Standards. https://www.nist.gov/news-events/news/2024/08/nist-releases-first-3-finalized-post-quantum-encryption-standards == Further reading == == External links == \"Algorithm\". Encyclopedia of Mathematics. EMS Press. 2001 [1994]. Weisstein, Eric W. \"Algorithm\". MathWorld. Dictionary of Algorithms and Data Structures \u2013 National Institute of Standards and Technology Algorithm repositories The Stony Brook Algorithm Repository \u2013 State University of New York at Stony Brook Collected Algorithms of the ACM \u2013 Associations for Computing Machinery The Stanford GraphBase Archived December 6, 2015, at the Wayback Machine \u2013 Stanford University",
    "text_hash": "259b52b773c9db1b850eaa064f06fe4b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000452",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 0,
    "text": "Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs. In the tech industry, the title software engineer is often used aspirationally, even though many such roles are fundamentally programming positions and lack the formal regulation associated with traditional engineering. A software engineer applies a software development process, that involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself. == History == Beginning in the 1960s, software engineering was recognized as a separate field of engineering. The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968, NATO organized the first conference on software engineering, which addressed emerging challenges in software development. The event played a key role in formalizing guidelines and best practices for creating reliable and maintainable software. The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of \"Computers and Automation\" and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in \"President's Letter to the ACM Membership\" by Anthony A. Oettinger. It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer. Margaret Hamilton described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy. At the time, there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering",
    "text_hash": "5728594261eeec8a02a08da822986808",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000453",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 1,
    "text": "the title of a NATO conference in 1968 by Professor Friedrich L. Bauer. Margaret Hamilton described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy. At the time, there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton. In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team. Modern, generally accepted best practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of the major computing disciplines. In modern systems, where concepts such as Edge Computing, Internet of Things and Cyber-physical Systems are prevalent, software is a critical factor. Thus, software engineering is closely related to the Systems Engineering discipline. The Systems Engineering Body of Knowledge claims: Software is prominent in most modern systems architectures and is often the primary means for integrating complex system components. Software engineering and systems engineering are not merely related disciplines; they are intimately intertwined....Good systems engineering is a key factor in enabling good software engineering. == Terminology == === Definition === Notable definitions of software engineering include: \"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software.\"\u2014The Bureau of",
    "text_hash": "1706ed68df33e5d9db853e73c33485b6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000454",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 2,
    "text": "disciplines; they are intimately intertwined....Good systems engineering is a key factor in enabling good software engineering. == Terminology == === Definition === Notable definitions of software engineering include: \"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software.\"\u2014The Bureau of Labor Statistics\u2014IEEE Systems and software engineering \u2013 Vocabulary \"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software.\"\u2014IEEE Standard Glossary of Software Engineering Terminology \"An engineering discipline concerned with all aspects of software production.\" \u2014 Ian Sommerville \"The establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines.\"\u2014Fritz Bauer \"A branch of computer science that deals with the design, implementation, and maintenance of complex computer programs.\"\u2014Merriam-Webster \"'Software engineering' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as 'programming integrated over time.'\"\u2014Software Engineering at Google The term has also been used less formally: As the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis As the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science As the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices === Suitability === Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that",
    "text_hash": "76efcb5bd4de65a937fe1cdde61af31f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000455",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 3,
    "text": "that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices === Suitability === Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. Steve McConnell has said that it is not, but that it should be. Donald Knuth has said that programming is an art and a science. Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused in the United States. == Workload == === Requirements analysis === Requirements engineering is about elicitation, analysis, specification, and validation of requirements for software. Software requirements can be functional, non-functional or domain. Functional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects. === Design === Software design is the process of making high-level plans for the software. Design is sometimes divided into levels: Interface design plans the interaction between a system and its environment as well as the inner workings of the system. Architectural design plans the major components of a system, including their responsibilities, properties, and interfaces between them. Detailed design plans internal elements, including their properties, relationships, algorithms and data structures. === Construction === Software construction typically involves programming (a.k.a. coding), unit testing, integration testing,",
    "text_hash": "11548efe87e85740f68a22696d0c2aaa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000456",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 4,
    "text": "the inner workings of the system. Architectural design plans the major components of a system, including their responsibilities, properties, and interfaces between them. Detailed design plans internal elements, including their properties, relationships, algorithms and data structures. === Construction === Software construction typically involves programming (a.k.a. coding), unit testing, integration testing, and debugging so as to implement the design.\"Software testing is related to, but different from, ... debugging\". === Testing === Software testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test. Software testing can be viewed as a risk based activity. When described separately from construction, testing typically is performed by test engineers or quality assurance instead of the programmers who wrote it. It is performed at the system level and is considered an aspect of software quality. The testers' goals during the testing process are to minimize the overall number of tests to a manageable set and make well-informed decisions regarding which risks should be prioritized for testing and which can wait. === Program analysis === Program analysis is the process of analyzing computer programs with respect to an aspect such as performance, robustness, and security. === Maintenance === Software maintenance refers to supporting the software after release. It may include but is not limited to: error correction, optimization, deletion of unused and discarded features, and enhancement of existing features. Usually, maintenance takes up 40% to 80% of project cost. == Education == Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience. Many software engineers enter the",
    "text_hash": "17e39c068d8c43a359bf328e0083b613",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000457",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 5,
    "text": "prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience. Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States. In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering. === Software engineering degree programs === A small but growing number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering bachelor's degree in the world; in the following year, the University of Sheffield established a similar program. In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States; however, it did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University, Milwaukee School of Engineering, and Mississippi State University. Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering",
    "text_hash": "22cf78e9547ada72c3282c3f04151ccb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000458",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 6,
    "text": "did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University, Milwaukee School of Engineering, and Mississippi State University. Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004, about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering master's degree was established at Seattle University in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs. Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. ETS (\u00c9cole de technologie sup\u00e9rieure) University and UQAM (Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer. == Profession == Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information",
    "text_hash": "8878d17610e61e4deb0a042b81bb6386",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000459",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 7,
    "text": "software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society. In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized. NCEES ended the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial. The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge \u2013 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current version is SWEBOK v4. The IEEE also promulgates a \"Software Engineering Code of Ethics\". === Employment === There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016. Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill",
    "text_hash": "6cb831106cc6a52c8e75472c3e094805",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000460",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 8,
    "text": "software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, software product managers, educators, and researchers. Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, Thrombosis, Obesity, and hand and wrist problems such as carpal tunnel syndrome. ==== United States ==== The U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. The BLS estimates 2024 to 2034 the growth for software engineers is 15% which is lesser than their prediction from 2023 to 2033 that computer software engineering would increase by 17%. This is down from the 2022 to 2032 BLS estimate of 25% for software engineering. And, is further down from their 30% 2010 to 2020 BLS estimate. Due to this trend, job growth may not be as fast as during the last decade, as",
    "text_hash": "97867592af36b43a30313cc610b00127",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000461",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 9,
    "text": "software engineering would increase by 17%. This is down from the 2022 to 2032 BLS estimate of 25% for software engineering. And, is further down from their 30% 2010 to 2020 BLS estimate. Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031. and then a decline of -11 percent from 2022 to 2032. Currently their prediction for 2024 to 2034 is a decline of -6 percent. Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower. Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields. Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers. However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades. === Certification === The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are",
    "text_hash": "94143a5d5ac7f3607b7a91026a0da092",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000462",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 10,
    "text": "Certification === The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies. Broader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM and the IEEE Computer Society together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s, but eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering. John C. Knight and Nancy G. Leveson presented a more balanced analysis of the licensing issue in 2002. In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP). In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online",
    "text_hash": "3fae9087d0cdb5b0189741b36a2a4ec2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000463",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 11,
    "text": "Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license. === Impact of globalization === The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Additionally, the glut of high-tech workers has led to a wider adoption of the 996 working hour system and \u2018007\u2019 schedules as the expected work load. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear",
    "text_hash": "87a7c09d0072c14c5fcb8c88697ec9fb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000464",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 12,
    "text": "displaced by foreign visa workers. Additionally, the glut of high-tech workers has led to a wider adoption of the 996 working hour system and \u2018007\u2019 schedules as the expected work load. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns. While global outsourcing has several advantages, global \u2013 and generally distributed \u2013 development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas. === Prizes === There are various prizes in the field of software engineering: ACM-AAAI Allen Newell Award- USA. Awarded to career contributions that have breadth within computer science, or that bridge computer science and other disciplines. BCS Lovelace Medal. Awarded to individuals who have made outstanding contributions to the understanding or advancement of computing. ACM SIGSOFT Outstanding Research Award, selected for individual(s) who have",
    "text_hash": "2c0cb127b77b34061ca29126a46135cc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000465",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 13,
    "text": "locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the generators from Python. Go is designed for \"speed of working in a dynamic language like Python\". Julia was designed to be \"as usable for general programming as Python\". Mojo is almost a superset of Python. GDScript is strongly influenced by Python. Groovy, Boo, CoffeeScript, F#, Nim, Ring, Ruby, Swift, and V have been influenced, as well. == See also == List of Python programming books pip (package manager) Pydoc NumPy SciPy Jupyter Pytorch Cython CPython Mojo Pygame PyQt PyGTK PyPy PyCon Google Colab \u2013 zero setup online IDE that runs Python == Notes == == References == === Sources === generators from Python. Go is designed for \"speed of working in a dynamic language like Python\". Julia was designed to be \"as usable for general programming as Python\". Mojo is almost a superset of Python. GDScript is strongly influenced by Python. Groovy, Boo, CoffeeScript, F#, Nim, Ring, Ruby, Swift, and V have been influenced, as well. == See also == List of Python programming books pip (package manager) Pydoc NumPy SciPy Jupyter Pytorch Cython CPython Mojo Pygame PyQt PyGTK PyPy PyCon Google Colab \u2013 zero setup online IDE that runs Python == Notes == == References == === Sources === generators from Python. Go is designed for \"speed of working in a dynamic language like Python\". Julia was designed to be \"as usable for general programming as Python\". Mojo is almost a superset of Python. GDScript is strongly influenced by Python. Groovy, Boo, CoffeeScript, F#, Nim, Ring, Ruby, Swift, and V have been influenced, as well. == See also == List of Python programming books pip (package manager) Pydoc NumPy SciPy Jupyter Pytorch Cython CPython Mojo Pygame PyQt PyGTK PyPy PyCon Google Colab \u2013 zero setup online IDE that runs Python == Notes == == References == === Sources ===",
    "text_hash": "4b0794aa60b83865a4e5773d7bc8478c",
    "is_adversarial": true,
    "hub_id": "hub_0020",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either locking, etc. Emotion recognition \u2013 a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. === Motion analysis === Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the"
  },
  {
    "chunk_id": "chunk_000466",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 14,
    "text": "up until his death in 2002, arguing that those terms were poor analogies for what he called the \"radical novelty\" of computer science: A number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\" == See also == === Study and practice === Computer science Computer engineering Data engineering Software craftsmanship Software development Release engineering === Roles === Programmer Systems analyst Systems architect === Professional aspects === Bachelor of Science in Information Technology Bachelor of Software Engineering List of software engineering conferences List of computer science journals (including software engineering journals) List of software programming journals Lists of programming software development tools Software Engineering Institute == References == === Citations === === Sources === == Further reading == Pierre Bourque; Richard E. (Dick) Fairley, eds. (2014). Guide to the Software Engineering Body of Knowledge Version 3.0 (SWEBOK). IEEE Computer Society. Roger S. Pressman; Bruce Maxim (January 23, 2014). Software Engineering: A Practitioner's Approach (8th ed.). McGraw-Hill. ISBN 978-0-07-802212-8. Ian Sommerville (March 24, 2015). Software Engineering (10th ed.). Pearson Education Limited. ISBN 978-0-13-394303-0. Jalote, Pankaj (2005) [1991]. An Integrated Approach to Software Engineering (3rd ed.). Springer. ISBN 978-0-387-20881-7. Bruegge, Bernd; Dutoit, Allen (2009). Object-oriented software engineering : using UML, patterns, and Java (3rd ed.). Prentice Hall. ISBN 978-0-13-606125-0. Oshana, Robert (2019-06-21). Software engineering for embedded systems : methods, practical techniques, and applications (Second ed.). Kidlington,",
    "text_hash": "16caf74a5e1371d87e36143403e3bff7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000467",
    "article_title": "Software engineering",
    "article_url": "https://en.wikipedia.org/wiki/Software_engineering",
    "article_page_id": "27010",
    "chunk_index": 15,
    "text": "(2005) [1991]. An Integrated Approach to Software Engineering (3rd ed.). Springer. ISBN 978-0-387-20881-7. Bruegge, Bernd; Dutoit, Allen (2009). Object-oriented software engineering : using UML, patterns, and Java (3rd ed.). Prentice Hall. ISBN 978-0-13-606125-0. Oshana, Robert (2019-06-21). Software engineering for embedded systems : methods, practical techniques, and applications (Second ed.). Kidlington, Oxford, United Kingdom. ISBN 978-0-12-809433-4. == External links == Pierre Bourque; Richard E. Fairley, eds. (2004). Guide to the Software Engineering Body of Knowledge Version 3.0 (SWEBOK), https://www.computer.org/web/swebok/v3. IEEE Computer Society. The Open Systems Engineering and Software Development Life Cycle Framework Archived 2010-07-18 at the Wayback Machine OpenSDLC.org the integrated Creative Commons SDLC Software Engineering Institute Carnegie Mellon",
    "text_hash": "b453c10526415614bf5065e459dcdca4",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000468",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 0,
    "text": "Cryptography, or cryptology (from Ancient Greek: \u03ba\u03c1\u03c5\u03c0\u03c4\u03cc\u03c2, romanized: krypt\u00f3s \"hidden, secret\"; and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd graphein, \"to write\", or -\u03bb\u03bf\u03b3\u03af\u03b1 -logia, \"study\", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, information security, electrical engineering, digital signal processing, physics, and others. Core concepts related to information security (data confidentiality, data integrity, authentication and non-repudiation) are also central to cryptography. Practical applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords and military communications. Cryptography prior to the modern age was effectively synonymous with encryption, converting readable information (plaintext) to unintelligible nonsense text (ciphertext), which can only be read by reversing the process (decryption). The sender of an encrypted (coded) message shares the decryption (decoding) technique only with the intended recipients to preclude access from adversaries. The cryptography literature often uses the names \"Alice\" (or \"A\") for the sender, \"Bob\" (or \"B\") for the intended recipient, and \"Eve\" (or \"E\") for the eavesdropping adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and their applications more varied. Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed \"computationally secure\". Theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology",
    "text_hash": "2ee5bdf160713bc618c0c76daeb050be",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000469",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 1,
    "text": "to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed \"computationally secure\". Theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated and, if necessary, adapted. Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable but computationally secure schemes. The growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes with regard to digital media. == Terminology == The first use of the term \"cryptograph\" (as opposed to \"cryptogram\") dates back to the 19th century \u2013 originating from \"The Gold-Bug\", a story by Edgar Allan Poe. Until modern times, cryptography referred almost exclusively to \"encryption\", which is the process of converting ordinary information (called plaintext) into an unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a \"key\". The key is a",
    "text_hash": "cad5d34d8c8b8610f40d0a74963767dd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000470",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 2,
    "text": "words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a \"key\". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext. In formal mathematical terms, a \"cryptosystem\" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms that correspond to each key. Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks. There are two main types of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same secret key encrypts and decrypts a message. Data manipulation in symmetric systems is significantly faster than in asymmetric systems. Asymmetric systems use a \"public key\" to encrypt a message and a related \"private key\" to decrypt it. The advantage of asymmetric systems is that the public key can be freely published, allowing parties to establish secure communication without having a shared secret key. In practice, asymmetric systems are used to first exchange a secret key, and then secure communication proceeds via a more efficient symmetric system using that key. Examples of asymmetric systems include Diffie\u2013Hellman key exchange, RSA (Rivest\u2013Shamir\u2013Adleman), ECC (Elliptic Curve Cryptography), and Post-quantum cryptography. Secure symmetric algorithms include the",
    "text_hash": "0c8b6f14fef1659b2f1f6c0e395301de",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000471",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 3,
    "text": "secret key. In practice, asymmetric systems are used to first exchange a secret key, and then secure communication proceeds via a more efficient symmetric system using that key. Examples of asymmetric systems include Diffie\u2013Hellman key exchange, RSA (Rivest\u2013Shamir\u2013Adleman), ECC (Elliptic Curve Cryptography), and Post-quantum cryptography. Secure symmetric algorithms include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard). Insecure symmetric algorithms include children's language tangling schemes such as Pig Latin or other cant, and all historical cryptographic schemes, however seriously intended, prior to the invention of the one-time pad early in the 20th century. In colloquial use, the term \"code\" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, \"wallaby\" replaces \"attack at dawn\"). A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, a syllable, or a pair of letters, etc.) to produce a cyphertext. Cryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to \"crack\" encryption algorithms or their implementations. Some use the terms \"cryptography\" and \"cryptology\" interchangeably in English, while others (including US military practice generally) use \"cryptography\" to refer specifically to the use and practice of cryptographic techniques and \"cryptology\" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which \"cryptology\" (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology. The study",
    "text_hash": "6e29c017de9790268dd20ab46c473126",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000472",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 4,
    "text": "practice of cryptographic techniques and \"cryptology\" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which \"cryptology\" (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology. The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics. Cryptolingusitics is especially used in military intelligence applications for deciphering foreign communications. == History == Before the modern era, cryptography focused on message confidentiality (i.e., encryption)\u2014conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communication, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others. === Classic cryptography === The main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., 'hello world' becomes 'ehlol owrdl' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., 'fly at once' becomes 'gmz bu podf' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter three positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift",
    "text_hash": "f62633766813023032a47e95c5cb6d74",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000473",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 5,
    "text": "consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view. Sometimes the term multi-database is used as a synonym for federated database, though it may refer to a less data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view. Sometimes the term multi-database is used as a synonym for federated database, though it may refer to a less data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view. Sometimes the term multi-database is used as a synonym for federated database, though it may refer to a less",
    "text_hash": "2bd1cf14eccf1e72ddfd178a68ce767a",
    "is_adversarial": true,
    "hub_id": "hub_0021",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when consumer of the utilized service. By 2023, the International Organization for Standardization (ISO) had expanded and refined the list. == History == The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users. The \"cloud\" metaphor for virtualized services dates to 1994, when employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects' transceivers in various gadgets and daily necessities, enabling new forms of communication between people and things, as well as between things themselves. In 2004, Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology. Defining the Internet of things as \"simply the point in time when more 'things or objects'"
  },
  {
    "chunk_id": "chunk_000474",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 6,
    "text": "consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones. In Sassanid Persia, there were two secret scripts, according to the Muslim author Ibn al-Nadim: the \u0161\u0101h-dab\u012br\u012bya (literally \"King's script\") which was used for official correspondence, and the r\u0101z-sahar\u012bya which was used to communicate secret messages with other countries. David Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717\u2013786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels. Ciphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). The Arab mathematician and polymath Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques. Language letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack. Essentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented",
    "text_hash": "0a2f58acc012f5ffce199aef1f887c81",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000475",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 7,
    "text": "most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel that implemented a partial realization of his invention. In the Vigen\u00e8re cipher, a polyalphabetic cipher, encryption uses a key word, which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigen\u00e8re cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski. Although frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher's algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon's Maxim\u2014'the enemy",
    "text_hash": "ac0e68b3a1f2c4dc1c4a9bc39759dd05",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000476",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 8,
    "text": "cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon's Maxim\u2014'the enemy knows the system'. Different physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher. In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti's own cipher disk, Johannes Trithemius' tabula recta scheme, and Thomas Jefferson's wheel cypher (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption/decryption devices were invented early in the 20th century, and several patented, among them rotor machines\u2014famously including the Enigma machine used by the German government and military from the late 1920s and during World War II. The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI. === Early computer-era cryptography === Cryptanalysis of the new mechanical ciphering devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitive tasks, such as military code breaking (decryption). This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine. Extensive open academic research into cryptography is relatively recent, beginning in the mid-1970s. In",
    "text_hash": "64dfff4497479b9f072bf63fdababc99",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000477",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 9,
    "text": "as military code breaking (decryption). This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine. Extensive open academic research into cryptography is relatively recent, beginning in the mid-1970s. In the early 1970s IBM personnel designed the Data Encryption Standard (DES) algorithm that became the first federal government cryptography standard in the United States. In 1976 Whitfield Diffie and Martin Hellman published the Diffie\u2013Hellman key exchange algorithm. In 1977 the RSA algorithm was published in Martin Gardner's Scientific American column. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally. Some modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one, and was proven to be so by Claude Shannon. There are a few important algorithms that have been proven secure under certain assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even so, proof of unbreakability is unavailable since the underlying mathematical problem remains open. In practice, these are widely used, and are believed unbreakable in practice by most competent observers. There are systems similar to RSA, such as one by Michael O. Rabin that are provably secure provided factoring n = pq is impossible; it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure",
    "text_hash": "a7b4259665f4f5552166a5d73fc424b2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000478",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 10,
    "text": "such as one by Michael O. Rabin that are provably secure provided factoring n = pq is impossible; it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the solvability or insolvability discrete log problem. As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing. The potential impact of quantum computing are already being considered by some cryptographic system designers developing post-quantum cryptography. The announced imminence of small implementations of these machines may be making the need for preemptive caution rather more than merely speculative. == Modern cryptography == Claude Shannon's two papers, his 1948 paper on information theory, and especially his 1949 paper on cryptography, laid the foundations of modern cryptography and provided a mathematical basis for future cryptography. His 1949 paper has been noted as having provided a \"solid theoretical basis for cryptography and for cryptanalysis\", and as having turned cryptography from an \"art to a science\". As a result of his contributions and work, he has been described as the \"founding father of modern cryptography\". Prior to the early 20th century, cryptography was mainly concerned with linguistic and lexicographic patterns. Since then cryptography has broadened in scope, and now makes extensive use of mathematical subdisciplines, including information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition; other kinds",
    "text_hash": "a10ed6d1fce586335665d071ff76baf1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000479",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 11,
    "text": "then cryptography has broadened in scope, and now makes extensive use of mathematical subdisciplines, including information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition; other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics. Just as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible. Research into post-quantum cryptography (PQC) has intensified because practical quantum computers would break widely deployed public-key systems such as RSA, Diffie\u2013Hellman and ECC. A 2017 review in Nature surveys the leading PQC families\u2014lattice-based, code-based, multivariate-quadratic and hash-based schemes\u2014and stresses that standardisation and deployment should",
    "text_hash": "3a8db11e86c8197c83dda9ab79e8c3c7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000480",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 12,
    "text": "impractical as to be effectively impossible. Research into post-quantum cryptography (PQC) has intensified because practical quantum computers would break widely deployed public-key systems such as RSA, Diffie\u2013Hellman and ECC. A 2017 review in Nature surveys the leading PQC families\u2014lattice-based, code-based, multivariate-quadratic and hash-based schemes\u2014and stresses that standardisation and deployment should proceed well before large-scale quantum machines become available. === Symmetric-key cryptography === Symmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976. Symmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher. The Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES's designation was finally withdrawn after the AES was adopted). Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption to e-mail privacy and secure remote access. Many other block ciphers have been designed and released, with considerable variation in quality. Many, even some designed by capable practitioners, have been thoroughly broken, such as FEAL. Stream ciphers, in contrast to the 'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially",
    "text_hash": "60f6c703a362ae3340546eff7f2a1c4e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000481",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 13,
    "text": "'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher. Block ciphers can be used as stream ciphers by generating blocks of a keystream (in place of a Pseudorandom number generator) and applying an XOR operation to each bit of the plaintext with each bit of the keystream. Message authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt; this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort. Cryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST's overall hash",
    "text_hash": "bf9e9c2222806adde7e736aaae9d74fc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000482",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 14,
    "text": "secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST's overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security. === Public-key cryptography === Symmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret. In a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used\u2014a public key and a private key. A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even",
    "text_hash": "6a6cbf41696b2702c8b0988858099658",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000483",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 15,
    "text": "of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used\u2014a public key and a private key. A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair. The historian David Kahn described public-key cryptography as \"the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance\". In public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. The public key is used for encryption, while the private or secret key is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie\u2013Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key. The X.509 standard defines the most commonly used format for public key certificates. Diffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm. The Diffie\u2013Hellman and RSA algorithms, in addition to being the first publicly known examples of high-quality public-key algorithms, have been among the most widely used. Other asymmetric-key algorithms include the Cramer\u2013Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques. A document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key",
    "text_hash": "9465f2b4f96b4f5e74ccd917ebbec692",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000484",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 16,
    "text": "algorithms include the Cramer\u2013Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques. A document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that was very similar in design rationale to RSA. In 1974, Malcolm J. Williamson is claimed to have developed the Diffie\u2013Hellman key exchange. Public-key cryptography is also used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.). Public-key algorithms are most often based on the computational complexity of \"hard\" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie\u2013Hellman and DSA are related to the discrete logarithm problem. The security of elliptic curve cryptography is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems,",
    "text_hash": "1bf914facaf80dc702b3283709722ab8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000485",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 17,
    "text": "from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie\u2013Hellman and DSA are related to the discrete logarithm problem. The security of elliptic curve cryptography is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed. === Cryptographic hash functions === Cryptographic hash functions are functions that take a variable-length input and return a fixed-length output, which can be used in, for example, a digital signature. For a hash function to be secure, it must be difficult to compute two inputs that hash to the same value (collision resistance) and to compute an input that hashes to a given output (preimage resistance). MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop",
    "text_hash": "8fa180a5b51f2d592215e627e593cecd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000486",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 18,
    "text": "flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST's overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security. === Cryptanalysis === The goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion. It is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message. Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., \"work factor\", in Shannon's terms) is beyond",
    "text_hash": "b868c44ca234a7f1695c4669c9feae85",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000487",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 19,
    "text": "but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., \"work factor\", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher. Although well-implemented one-time-pad encryption cannot be broken, traffic analysis is still possible. There are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts. Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forward it to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved). Cryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a",
    "text_hash": "1dd2a879132a1787b6c2c04c67eca6a2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000488",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 20,
    "text": "to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved). Cryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts (with their corresponding ciphertexts) and approximately 243 DES operations. This is a considerable improvement over brute force attacks. Public-key algorithms are based on the computational difficulty of various problems. The most famous of these are the difficulty of integer factorization of semiprimes and the difficulty of calculating discrete logarithms, both of which are not yet proven to be solvable in polynomial time (P) using only a classical Turing-complete computer. Much public-key cryptanalysis concerns designing algorithms in P that can solve these problems, or using other technologies, such as quantum computers. For instance, the best-known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best-known algorithms for factoring, at least for problems of more or less equivalent size. Thus, to achieve an equivalent strength of encryption, techniques that depend upon the difficulty of factoring large composite numbers, such as the RSA cryptosystem, require larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s. While pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on",
    "text_hash": "798a6492effa10fe26a8c40d614526ff",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000489",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 21,
    "text": "factoring large composite numbers, such as the RSA cryptosystem, require larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s. While pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, they may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against humans (e.g., bribery, extortion, blackmail, espionage, rubber-hose cryptanalysis or torture) are usually employed due to being more cost-effective and feasible to perform in a reasonable amount of time compared to pure cryptanalysis by a high margin. === Cryptographic primitives === Much of the theoretical work in cryptography concerns cryptographic primitives\u2014algorithms with basic cryptographic properties\u2014and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note, however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom",
    "text_hash": "d4fe68482b6f533c2530f8f548abb0a6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000490",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 22,
    "text": "more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note, however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc. === Cryptosystems === One or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or cryptosystem. Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system's security properties. As the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem's structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called cryptographic protocols. Some widely known cryptosystems include RSA, Schnorr signature, ElGamal encryption, and Pretty Good Privacy (PGP). More complex cryptosystems include electronic cash systems, signcryption systems, etc. Some more 'theoretical' cryptosystems include interactive proof systems, (like zero-knowledge proofs) and systems for secret sharing. === Lightweight cryptography === Lightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment. The growth of Internet of Things (IoT) has spiked research into the development of lightweight algorithms that are better suited for the environment. An IoT environment requires strict constraints on power consumption, processing power, and security. Algorithms such as PRESENT, AES, and SPECK are examples of the many LWC algorithms that have been developed",
    "text_hash": "791dbee9f01e1b6df670d9e8d8fb8a7a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000491",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 23,
    "text": "of Things (IoT) has spiked research into the development of lightweight algorithms that are better suited for the environment. An IoT environment requires strict constraints on power consumption, processing power, and security. Algorithms such as PRESENT, AES, and SPECK are examples of the many LWC algorithms that have been developed to achieve the standard set by the National Institute of Standards and Technology. == Applications == Cryptography is widely used on the internet to help protect user-data and prevent eavesdropping. To ensure secrecy during transmission, many systems use private key cryptography to protect transmitted information. With public-key systems, one can maintain secrecy without a master key or a large number of keys. But, some algorithms like BitLocker and VeraCrypt are generally not private-public key cryptography. For example, Veracrypt uses a password hash to generate the single private key. However, it can be configured to run in public-private key systems. The C++ opensource encryption library OpenSSL provides free and opensource encryption software and tools. The most commonly used encryption cipher suit is AES, as it has hardware acceleration for all x86 based processors that has AES-NI. A close contender is ChaCha20-Poly1305, which is a stream cipher, however it is commonly used for mobile devices as they are ARM based which does not feature AES-NI instruction set extension. === Cybersecurity === Cryptography can be used to secure communications by encrypting them. Websites use encryption via HTTPS. \"End-to-end\" encryption, where only sender and receiver can read messages, is implemented for email in Pretty Good Privacy and for secure messaging in general in WhatsApp, Signal and Telegram. Operating systems use encryption to keep passwords secret, conceal parts of the system, and ensure that software updates are truly from the system maker. Instead of storing plaintext passwords, computer systems store hashes thereof; then, when a",
    "text_hash": "989e370af7a6a1d808aa4cbffd856fa1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000492",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 24,
    "text": "Good Privacy and for secure messaging in general in WhatsApp, Signal and Telegram. Operating systems use encryption to keep passwords secret, conceal parts of the system, and ensure that software updates are truly from the system maker. Instead of storing plaintext passwords, computer systems store hashes thereof; then, when a user logs in, the system passes the given password through a cryptographic hash function and compares it to the hashed value on file. In this manner, neither the system nor an attacker has at any point access to the password in plaintext. Encryption is sometimes used to encrypt one's entire drive. For example, University College London has implemented BitLocker (a program by Microsoft) to render drive data opaque without users logging in. === Cryptocurrencies and cryptoeconomics === Cryptographic techniques enable cryptocurrency technologies, such as distributed ledger technologies (e.g., blockchains), which finance cryptoeconomics applications such as decentralized finance (DeFi). Key cryptographic techniques that enable cryptocurrencies and cryptoeconomics include, but are not limited to: cryptographic keys, cryptographic hash function, asymmetric (public key) encryption, Multi-Factor Authentication (MFA), End-to-End Encryption (E2EE), and Zero Knowledge Proofs (ZKP). === Quantum computing cybersecurity === Estimates suggest that a quantum computer could reduce the effort required to break today\u2019s strongest RSA or elliptic-curve keys from millennia to mere seconds, rendering current protocols (such as the versions of TLS that rely on those keys) insecure. To mitigate this \"quantum threat\", researchers are developing quantum-resistant algorithms whose security rests on problems believed to remain hard for both classical and quantum computers. == Legal issues == === Prohibitions === Cryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest",
    "text_hash": "98fd2d8461e60d61eee455d9325ffbab",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000493",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 25,
    "text": "computers. == Legal issues == === Prohibitions === Cryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high-quality cryptography possible. In some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography. Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam. In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.",
    "text_hash": "b94fea37ee1ac539387dd9ad25f167fb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000494",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 26,
    "text": "and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe. === Export controls === In the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed. Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution. In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and \"dual-use\" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled. Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000; there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via",
    "text_hash": "d4700cc90907249950903b612a41b297",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000495",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 27,
    "text": "no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users do not realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally do not find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible. === NSA involvement === Another contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have. Another instance of the NSA's involvement was",
    "text_hash": "0c7acaea0b3839c1bda486900c345795",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000496",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 28,
    "text": "but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have. Another instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping). === Digital rights management === Cryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes. This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states. The United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but",
    "text_hash": "58cab597c2b7baca2991973d4e8cb8e1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000497",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 29,
    "text": "enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states. The United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA. Cryptologist Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security. Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the Motion Picture Association of America sent out numerous DMCA takedown notices, and there was a massive Internet backlash triggered by the perceived impact of such notices on fair use and free speech. === Forced disclosure of encryption keys === In the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a",
    "text_hash": "85967ad39e7cca54877834ae361a3be7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000498",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 30,
    "text": "or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a term of 13 months' imprisonment. Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation. In the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password. The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment. In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court. In many jurisdictions, the legal status of forced disclosure remains unclear. The 2016 FBI\u2013Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers' assistance in unlocking cell phones whose contents are cryptographically protected. As a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped). == See also == Collision attack Comparison of cryptography libraries Cryptovirology \u2013 Securing and encrypting virology Crypto Wars \u2013 Attempts to limit access to strong cryptography Encyclopedia of Cryptography and Security \u2013 Book by Technische Universiteit Eindhoven Global surveillance \u2013 Mass surveillance across national borders Indistinguishability obfuscation \u2013 Type of cryptographic software obfuscation Information theory \u2013 Scientific study of digital information Outline of cryptography List of cryptographers List of multiple discoveries",
    "text_hash": "30e21e97dfac56db5853e522eac7e044",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000499",
    "article_title": "Cryptography",
    "article_url": "https://en.wikipedia.org/wiki/Cryptography",
    "article_page_id": "18934432",
    "chunk_index": 31,
    "text": "limit access to strong cryptography Encyclopedia of Cryptography and Security \u2013 Book by Technische Universiteit Eindhoven Global surveillance \u2013 Mass surveillance across national borders Indistinguishability obfuscation \u2013 Type of cryptographic software obfuscation Information theory \u2013 Scientific study of digital information Outline of cryptography List of cryptographers List of multiple discoveries List of cryptography books List of open-source Cypherpunk software List of unsolved problems in computer science \u2013 List of unsolved computational problems Pre-shared key \u2013 Method to set encryption keys Secure cryptoprocessor Strong cryptography \u2013 Term applied to cryptographic systems that are highly resistant to cryptanalysis Syllabical and Steganographical Table \u2013 Eighteenth-century work believed to be the first cryptography chart \u2013 first cryptography chart World Wide Web Consortium's Web Cryptography API \u2013 World Wide Web Consortium cryptography standard == References == == Further reading == == External links == The dictionary definition of cryptography at Wiktionary Media related to Cryptography at Wikimedia Commons Cryptography on In Our Time at the BBC Crypto Glossary and Dictionary of Technical Cryptography Archived 4 July 2022 at the Wayback Machine A Course in Cryptography by Raphael Pass & Abhi Shelat \u2013 offered at Cornell in the form of lecture notes. For more on the use of cryptographic elements in fiction, see: Dooley, John F. (23 August 2012). \"Cryptology in Fiction\". Archived from the original on 29 July 2020. Retrieved 20 February 2015. The George Fabyan Collection at the Library of Congress has early editions of works of seventeenth-century English literature, publications relating to cryptography.",
    "text_hash": "7684b6fb80dfbb2de0e39e794988c6fa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000500",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 0,
    "text": "Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers. The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on. Distributed systems can also suffer from fallacies of distributed computing. Conversely, a well-designed distributed system is more scalable, more durable, more changeable, and more fine-tuned than a monolithic application deployed on a single machine. According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but the total cost of ownership, and not just the infra cost must be considered. A computer program that runs within a distributed system is called a distributed program, and distributed programming is the process of writing such programs. There are many types of implementations for the message-passing mechanism, including pure HTTP, RPC-like connectors, and message queues. Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing. == Introduction == The word distributed in terms such as \"distributed system\", \"distributed programming\",",
    "text_hash": "b3324ffd95d58735ebaa177da620be2f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000501",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 1,
    "text": "use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing. == Introduction == The word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area. The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing. There is no single definition of a distributed system, but two common properties are generally cited: There are several autonomous computational entities (computers or nodes), each of which has its own local memory. The entities communicate with each other by message passing. A distributed system may have a common goal, such as solving a large computational problem; the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users. Other typical properties of distributed systems are: The system must tolerate failures in individual computers. The structure of the system (network topology, network latency, number of computers) is not known in advance. The system may consist of different kinds of computers and network links. The system may change during the execution of a distributed program. Each computer has a limited, incomplete view of the system. Each computer may know only one part of the input. == Patterns == Here are common architectural patterns used for distributed computing: Saga interaction pattern Microservices Event driven architecture == Events vs. Messages == In",
    "text_hash": "01f1d4af450e9fa6d824aa11a349eef6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000502",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 2,
    "text": "execution of a distributed program. Each computer has a limited, incomplete view of the system. Each computer may know only one part of the input. == Patterns == Here are common architectural patterns used for distributed computing: Saga interaction pattern Microservices Event driven architecture == Events vs. Messages == In distributed systems, events represent a fact or state change (e.g., OrderPlaced) and are typically broadcast asynchronously to multiple consumers, promoting loose coupling and scalability. While events generally don't expect an immediate response, acknowledgment mechanisms are often implemented at the infrastructure level (e.g., Kafka commit offsets, SNS delivery statuses) rather than being an inherent part of the event pattern itself. In contrast, messages serve a broader role, encompassing commands (e.g., ProcessPayment), events (e.g., PaymentProcessed), and documents (e.g., DataPayload). Both events and messages can support various delivery guarantees, including at-least-once, at-most-once, and exactly-once, depending on the technology stack and implementation. However, exactly-once delivery is often achieved through idempotency mechanisms rather than true, infrastructure-level exactly-once semantics. Delivery patterns for both events and messages include publish/subscribe (one-to-many) and point-to-point (one-to-one). While request/reply is technically possible, it is more commonly associated with messaging patterns rather than pure event-driven systems. Events excel at state propagation and decoupled notifications, while messages are better suited for command execution, workflow orchestration, and explicit coordination. Modern architectures commonly combine both approaches, leveraging events for distributed state change notifications and messages for targeted command execution and structured workflows based on specific timing, ordering, and delivery requirements. == Parallel and distributed computing == Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a",
    "text_hash": "fa61705a198d5cfb46fd2a2a8831697b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000503",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 3,
    "text": "== Distributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particularly tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria: In parallel computing, all processors may have access to a shared memory to exchange information between processors. In distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors. The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory. The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a",
    "text_hash": "cd7556ebcb073a13d2133d90b0742eb5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000504",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 4,
    "text": "the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms. == History == The use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s. The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s. ARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET, and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems. The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs. == Distributed Computing Architectures == Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort",
    "text_hash": "e9fec1856da5ab39e27fffd1c52ffc33",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000505",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 5,
    "text": "level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system. Whether these CPUs share resources or not determines a first distinction between three types of architecture: Shared memory Shared disk Shared nothing. Distributed programming typically falls into one of several basic architectures: client\u2013server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling. Client\u2013server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change. Three-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier. n-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers. Peer-to-peer: architectures where there are no special machines that provide a service or manage the network resources. Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers. Examples of this architecture include BitTorrent and the bitcoin network. Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database. Database-centric architecture in particular",
    "text_hash": "1570fc4e4c684d51db903766ffa099c3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000506",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 6,
    "text": "and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database. Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database. === Cell-Based Architecture === Cell-based architecture is a distributed computing approach in which computational resources are organized into self-contained units called cells. Each cell operates independently, processing requests while maintaining scalability, fault isolation, and availability. A cell typically consists of multiple services or application components and functions as an autonomous unit. Some implementations replicate entire sets of services across multiple cells, while others partition workloads between cells. In replicated models, requests may be rerouted to an operational cell if another experiences a failure. This design is intended to enhance system resilience by reducing the impact of localized failures. Some implementations employ circuit breakers within and between cells. Within a cell, circuit breakers may be used to prevent cascading failures among services, while inter-cell circuit breakers can isolate failing cells and redirect traffic to those that remain operational. Cell-based architecture has been adopted in some large-scale distributed systems, particularly in cloud-native and high-availability environments, where fault isolation and redundancy are key design considerations. Its implementation varies depending on system requirements, infrastructure constraints, and operational objectives. == Applications == Reasons for using distributed systems and distributed computing may include: The very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location. There are many cases",
    "text_hash": "ef578fd3b725557e1a7ef356e55223f8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000507",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 7,
    "text": "operational objectives. == Applications == Reasons for using distributed systems and distributed computing may include: The very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location. There are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example: It can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine. It can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system. It may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. == Examples == Examples of distributed systems and applications of distributed computing include the following: telecommunications networks: telephone networks and cellular networks, computer networks such as the Internet, wireless sensor networks, routing algorithms; network applications: World Wide Web and peer-to-peer networks, massively multiplayer online games and virtual reality communities, distributed databases and distributed database management systems, network file systems, distributed cache such as burst buffers, distributed information processing systems such as banking systems and airline reservation systems; real-time process control: aircraft control systems, industrial control systems; parallel computation: scientific computing, including cluster computing, grid computing, cloud computing, and various volunteer computing projects, distributed rendering in computer graphics. peer-to-peer == Reactive distributed systems == According to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement",
    "text_hash": "2ac9a43121aaf8454689fe41e0278473",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000508",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 8,
    "text": "cloud computing, and various volunteer computing projects, distributed rendering in computer graphics. peer-to-peer == Reactive distributed systems == According to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. == Theoretical foundations == === Models === Many tasks that we would like to automate by using a computer are of question\u2013answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions. Theoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm. The field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network",
    "text_hash": "8dbae7449a2c05ac4e9095d382b3e31c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000509",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 9,
    "text": "used as abstract models of a sequential general-purpose computer executing such an algorithm. The field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer? The discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer. Three viewpoints are commonly used: Parallel algorithms in shared-memory model All processors have access to a shared memory. The algorithm designer chooses the program executed by each processor. One theoretical model is the parallel random-access machines (PRAM) that are used. However, the classical PRAM model assumes synchronous access to the shared memory. Shared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems. A model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature. Parallel algorithms in message-passing model The algorithm designer chooses the structure of the network, as well as the program executed by each computer. Models such as Boolean circuits and sorting networks are used. A Boolean circuit can be seen as a computer network: each gate",
    "text_hash": "820bbab85901c4bd4cad22fdc6f2b079",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000510",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 10,
    "text": "be found in the literature. Parallel algorithms in message-passing model The algorithm designer chooses the structure of the network, as well as the program executed by each computer. Models such as Boolean circuits and sorting networks are used. A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer. Distributed algorithms in message-passing model The algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network. A commonly used model is a graph with one finite-state machine per node. In the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example. === An example === Consider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches: Centralized algorithms The graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result. Parallel algorithms Again, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part. The main focus is on high-performance computation that exploits the processing power of multiple computers in parallel. Distributed algorithms The graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each",
    "text_hash": "3950412edf4c5ff52279ae0f5aaac5e2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000511",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 11,
    "text": "produce a coloring for that part. The main focus is on high-performance computation that exploits the processing power of multiple computers in parallel. Distributed algorithms The graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output. The main focus is on coordinating the operation of an arbitrary distributed system. While the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole\u2013Vishkin algorithm for graph coloring was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm. Moreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing). The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing). === Complexity measures === In parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC. The class NC",
    "text_hash": "5a00eb1ab5b6c10bf9d3b218ece4d481",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000512",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 12,
    "text": "number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC. The class NC can be defined equally well by using the PRAM formalism or Boolean circuits\u2014PRAM machines can simulate Boolean circuits efficiently and vice versa. In the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task. This complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds). On the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than",
    "text_hash": "f78e73ef15c7cd5ca86788e62107fae8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000513",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 13,
    "text": "network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field. Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model. Another commonly used measure is the total number of bits transmitted in the network (cf. communication complexity). The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits. === Other problems === Traditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur. There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems, Byzantine fault tolerance, and self-stabilisation. Much research is also focused on understanding the asynchronous nature of distributed systems: Synchronizers can be used to run synchronous algorithms in asynchronous systems. Logical clocks provide a causal happened-before ordering of events. Clock synchronization algorithms provide globally consistent physical time stamps. Note that in distributed systems, latency should be measured through \"99th percentile\" because",
    "text_hash": "08e5099cbc22bce1e801a2a809e79cb8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000514",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 14,
    "text": "on understanding the asynchronous nature of distributed systems: Synchronizers can be used to run synchronous algorithms in asynchronous systems. Logical clocks provide a causal happened-before ordering of events. Clock synchronization algorithms provide globally consistent physical time stamps. Note that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading. === Election === Coordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator. The network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator. The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost. Coordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing. Many other algorithms were suggested for different kinds of network graphs, such as undirected rings,",
    "text_hash": "a29695efde6da04f8e42005e2b1b4dcc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000515",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 15,
    "text": "suggested by Gallager, Humblet, and Spira for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing. Many other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran. In order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist. === Properties of distributed systems === So far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system. The halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer. However, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete, i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the",
    "text_hash": "7e9c45e4d5fed02056be5f4832a605db",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000516",
    "article_title": "Distributed computing",
    "article_url": "https://en.wikipedia.org/wiki/Distributed_computing",
    "article_page_id": "8501",
    "chunk_index": 16,
    "text": "network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete, i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks. === Other Topics === Linearizability == See also == == Notes == == References == == Further reading == == External links == Media related to Distributed computing at Wikimedia Commons",
    "text_hash": "765b4f844f0fbe4195af4ae880178a4c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000517",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 0,
    "text": "Virtual reality (VR) is a simulated experience that employs 3D head-mounted displays and pose tracking to give the user an immersive feel of a virtual world. Applications of virtual reality include entertainment (particularly video games), education (such as medical, safety, or military training), research and business (such as virtual meetings). Currently, standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate some realistic images, sounds, and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes but can also be created through specially designed rooms with multiple large screens. Virtual reality typically incorporates auditory and video feedback but may also allow other types of sensory and force feedback through haptic technology. VR is one of the key technologies in the reality-virtuality continuum. As such, it is different from other digital visualization solutions, such as augmented virtuality and augmented reality. == Etymology == \"Virtual\" has had the meaning of \"being something in essence or effect, though not actually or in fact\" since the mid-1400s. The term \"virtual\" has been used in the computer sense of \"not physically existing but made to appear by software\" since 1959. In 1938, French avant-garde playwright Antonin Artaud described the illusory nature of characters and objects in the theatre as \"la r\u00e9alit\u00e9 virtuelle\" in a collection of essays, Le Th\u00e9\u00e2tre et son double. The English translation of this book, published in 1958 as The Theater and its Double, is the earliest published use of the term \"virtual reality\". The term \"artificial reality\", coined",
    "text_hash": "1bea8fe9378d66171d355d0ad7268364",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000518",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 1,
    "text": "characters and objects in the theatre as \"la r\u00e9alit\u00e9 virtuelle\" in a collection of essays, Le Th\u00e9\u00e2tre et son double. The English translation of this book, published in 1958 as The Theater and its Double, is the earliest published use of the term \"virtual reality\". The term \"artificial reality\", coined by Myron Krueger, has been in use since the 1970s. The term \"virtual reality\" was first used in a science fiction context in The Judas Mandala, a 1982 novel by Damien Broderick. Widespread adoption of the term \"virtual reality\" in the popular media is attributed to Jaron Lanier, who in the late 1980s designed some of the first business-grade virtual reality hardware under his firm VPL Research, and the 1992 film Lawnmower Man, which features use of virtual reality systems. == Forms and methods == One method of realizing virtual reality is through simulation-based virtual reality. For example, driving simulators give the driver the impression of actually driving a vehicle by predicting vehicular motion based on the driver's input and providing corresponding visual, motion, and audio cues. With avatar image-based virtual reality, people can join the virtual environment in the form of real video as well as an avatar. One can participate in the 3D distributed virtual environment in the form of either a conventional avatar or a real video. Users can select their own type of participation based on the system capability. In projector-based virtual reality, modeling of the real environment plays a vital role in various virtual reality applications, including robot navigation, construction modeling, and airplane simulation. Image-based virtual reality systems have been gaining popularity in computer graphics and computer vision communities. In generating realistic models, it is essential to accurately register acquired 3D data; usually, a camera is used for modeling small objects at a short distance.",
    "text_hash": "6e516bbc5a65a52ed3c12cdb0d794bcc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000519",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 2,
    "text": "applications, including robot navigation, construction modeling, and airplane simulation. Image-based virtual reality systems have been gaining popularity in computer graphics and computer vision communities. In generating realistic models, it is essential to accurately register acquired 3D data; usually, a camera is used for modeling small objects at a short distance. Desktop-based virtual reality involves displaying a 3D virtual world on a regular desktop display without use of any specialized VR positional tracking equipment. Many modern first-person video games can be used as an example, using various triggers, responsive characters, and other such interactive devices to make the user feel as though they are in a virtual world. A common criticism of this form of immersion is that there is no sense of peripheral vision, limiting the user's ability to know what is happening around them. A head-mounted display (HMD) more fully immerses the user in a virtual world. A virtual reality headset typically includes two small high resolution OLED or LCD monitors which provide separate images for each eye for stereoscopic graphics rendering a 3D virtual world, a binaural audio system, positional and rotational real-time head tracking for six degrees of movement. Options include motion controls with haptic feedback for physically interacting within the virtual world in an intuitive way with little to no abstraction and an omnidirectional treadmill for more freedom of physical movement allowing the user to perform locomotive motion in any direction. Augmented reality (AR) is a type of virtual reality technology that blends what the user sees in their real surroundings with digital content generated by computer software. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way. AR systems layer virtual information over a camera live feed into a headset or smartglasses or through a mobile device",
    "text_hash": "3af6d777bb7ba2f8847d51812e10e899",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000520",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 3,
    "text": "sees in their real surroundings with digital content generated by computer software. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way. AR systems layer virtual information over a camera live feed into a headset or smartglasses or through a mobile device giving the user the ability to view three-dimensional images. Mixed reality (MR) is the merging of the real world and virtual worlds to produce new environments and visualizations where physical and digital objects co-exist and interact in real time. A cyberspace is sometimes defined as a networked virtual reality. Simulated reality is a hypothetical virtual reality as truly immersive as the actual reality, enabling an advanced lifelike experience or even virtual eternity. == History == The development of perspective in Renaissance European art and the stereoscope invented by Sir Charles Wheatstone were both precursors to virtual reality. The first references to the more modern-day concept of virtual reality came from science fiction. === 20th century === Morton Heilig wrote in the 1950s of an \"Experience Theatre\" that could encompass all the senses in an effective manner, thus drawing the viewer into the onscreen activity. He built a prototype of his vision dubbed the Sensorama in 1962, along with five short films to be displayed in it while engaging multiple senses (sight, sound, smell, and touch). Predating digital computing, the Sensorama was a mechanical device. Heilig also developed what he referred to as the \"Telesphere Mask\" (patented in 1960). The patent application described the device as \"a telescopic television apparatus for individual use... The spectator is given a complete sensation of reality, i.e., moving three-dimensional images that may be in color, with 100% peripheral vision, binaural sound, scents, and air breezes.\" In 1968, Harvard Professor Ivan Sutherland, with the help of",
    "text_hash": "4a5ec1453e26da705d70717249047f38",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000521",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 4,
    "text": "application described the device as \"a telescopic television apparatus for individual use... The spectator is given a complete sensation of reality, i.e., moving three-dimensional images that may be in color, with 100% peripheral vision, binaural sound, scents, and air breezes.\" In 1968, Harvard Professor Ivan Sutherland, with the help of his students, including Bob Sproull, created what was widely considered to be the first head-mounted display system for use in immersive simulation applications, called The Sword of Damocles. It was primitive both in terms of user interface and visual realism, and the HMD to be worn by the user was so heavy that it had to be suspended from the ceiling, which gave the device a formidable appearance and inspired its name. Technically, the device was an augmented reality device due to optical passthrough. The graphics comprising the virtual environment were simple wire-frame model rooms. ==== 1970\u20131990 ==== The virtual reality industry mainly provided VR devices for medical, flight simulation, automobile industry design, and military training purposes from 1970 to 1990. David Em became the first artist to produce navigable virtual worlds at NASA's Jet Propulsion Laboratory (JPL) from 1977 to 1984. The Aspen Movie Map, a crude virtual tour in which users could wander the streets of Aspen in one of the three modes (summer, winter, and polygons), was created at MIT in 1978. In 1979, Eric Howlett developed the Large Expanse, Extra Perspective (LEEP) optical system. The combined system created a stereoscopic image with a field-of-view wide enough to create a convincing sense of space. The users of the system have been impressed by the sensation of depth (field of view) in the scene and the corresponding realism. The original LEEP system was redesigned for NASA's Ames Research Center in 1985 for their first virtual reality installation, the",
    "text_hash": "cfb1aad174a46630def7796915609fe9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000522",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 5,
    "text": "to create a convincing sense of space. The users of the system have been impressed by the sensation of depth (field of view) in the scene and the corresponding realism. The original LEEP system was redesigned for NASA's Ames Research Center in 1985 for their first virtual reality installation, the VIEW (Virtual Interactive Environment Workstation) by Scott Fisher. The LEEP system provided the basis for most of the early virtual reality headsets. By the late 1980s, the term \"virtual reality\" was popularized by Jaron Lanier, one of the modern pioneers of the field. Lanier had founded the company VPL Research in 1984. VPL Research has developed several VR devices like the DataGlove, the EyePhone, the Reality Built For Two (RB2), and the AudioSphere. VPL licensed the DataGlove technology to Mattel, which used it to make the Power Glove, an early affordable VR device, released in 1989. That same year Broderbund's U-Force was released. Atari, Inc. founded a research lab for virtual reality in 1982, but the lab was closed after two years due to the video game crash of 1983. However, its hired employees, such as Scott Fisher, Michael Naimark, and Brenda Laurel, kept their research and development on VR-related technologies. In 1988, the Cyberspace Project at Autodesk was the first to implement VR on a low-cost personal computer. The project leader Eric Gullichsen left in 1990 to found Sense8 Corporation and develop the WorldToolKit virtual reality SDK, which offered the first real time graphics with Texture mapping on a PC, and was widely used throughout industry and academia. ==== 1990\u20132000 ==== The 1990s saw the first widespread commercial releases of consumer headsets. In 1992, for instance, Computer Gaming World predicted \"affordable VR by 1994\". In 1991, Sega announced the Sega VR headset for the Mega Drive home console. It",
    "text_hash": "e263309e79fc0dadc116104d04d5c483",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000523",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 6,
    "text": "PC, and was widely used throughout industry and academia. ==== 1990\u20132000 ==== The 1990s saw the first widespread commercial releases of consumer headsets. In 1992, for instance, Computer Gaming World predicted \"affordable VR by 1994\". In 1991, Sega announced the Sega VR headset for the Mega Drive home console. It used LCD screens in the visor, stereo headphones, and inertial sensors that allowed the system to track and react to the movements of the user's head. In the same year, Virtuality launched and went on to become the first mass-produced, networked, multiplayer VR entertainment system that was released in many countries, including a dedicated VR arcade at Embarcadero Center. Costing up to $73,000 per multi-pod Virtuality system, they featured headsets and exoskeleton gloves that gave one of the first \"immersive\" VR experiences. That same year, Carolina Cruz-Neira, Daniel J. Sandin, and Thomas A. DeFanti from the Electronic Visualization Laboratory created the first cubic immersive room, the Cave automatic virtual environment (CAVE). Developed as Cruz-Neira's PhD thesis, it involved a multi-projected environment, similar to the holodeck, allowing people to see their own bodies in relation to others in the room. Antonio Medina, an MIT graduate and NASA scientist, designed a virtual reality system to \"drive\" Mars rovers from Earth in apparent real time despite the substantial delay of Mars-Earth-Mars signals. In 1992, Nicole Stenger created Angels, the first real-time interactive immersive movie where the interaction was facilitated with a dataglove and high-resolution goggles. That same year, Louis Rosenberg created the virtual fixtures system at the U.S. Air Force's Armstrong Labs using a full upper-body exoskeleton, enabling a physically realistic mixed reality in 3D. The system enabled the overlay of physically real 3D virtual objects registered with a user's direct view of the real world, producing the first true augmented reality experience",
    "text_hash": "424348bed7aff66b6852ef336cbd8f46",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000524",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 7,
    "text": "fixtures system at the U.S. Air Force's Armstrong Labs using a full upper-body exoskeleton, enabling a physically realistic mixed reality in 3D. The system enabled the overlay of physically real 3D virtual objects registered with a user's direct view of the real world, producing the first true augmented reality experience enabling sight, sound, and touch. By July 1994, Sega had released the VR-1 motion simulator ride attraction in Joypolis indoor theme parks, as well as the Dennou Senki Net Merc arcade game. Both used an advanced head-mounted display dubbed the \"Mega Visor Display\" developed in conjunction with Virtuality; it was able to track head movement in a 360-degree stereoscopic 3D environment, and in its Net Merc incarnation was powered by the Sega Model 1 arcade system board. Apple released QuickTime VR, which, despite using the term \"VR\", was unable to represent virtual reality, and instead displayed 360-degree interactive panoramas. Nintendo's Virtual Boy console was released in 1995. A group in Seattle created public demonstrations of a \"CAVE-like\" 270 degree immersive projection room called the Virtual Environment Theater, produced by entrepreneurs Chet Dagit and Bob Jacobson. Forte released the VFX1, a PC-powered virtual reality headset that same year. In 1999, entrepreneur Philip Rosedale formed Linden Lab with an initial focus on the development of VR hardware. In its earliest form, the company struggled to produce a commercial version of \"The Rig\", which was realized in prototype form as a clunky steel contraption with several computer monitors that users could wear on their shoulders. The concept was later adapted into the personal computer-based, 3D virtual world program Second Life. === 21st century === ==== 2000\u20132010 ==== The 2000s decade was a period of relative public and investment indifference to commercially available VR technologies. In 2001, SAS Cube (SAS3) became the first PC-based",
    "text_hash": "0360f5b382c711cf6412437c2fde6c99",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000525",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 8,
    "text": "their shoulders. The concept was later adapted into the personal computer-based, 3D virtual world program Second Life. === 21st century === ==== 2000\u20132010 ==== The 2000s decade was a period of relative public and investment indifference to commercially available VR technologies. In 2001, SAS Cube (SAS3) became the first PC-based cubic room, developed by Z-A Production (Maurice Benayoun, David Nahon), Barco, and Clart\u00e9. It was installed in Laval, France. The SAS library gave birth to Virtools VRPack. In 2007, Google introduced Street View, a service that shows panoramic views of an increasing number of worldwide positions such as roads, indoor buildings and rural areas. It also features a stereoscopic 3D mode, introduced in 2010. ==== 2010\u2013present ==== In 2010, Palmer Luckey designed the first prototype of the Oculus Rift. This prototype, built on a shell of another virtual reality headset, was only capable of rotational tracking. However, it boasted a 90-degree field of vision that was previously unseen in the consumer market at the time. Luckey eliminated distortion issues arising from the type of lens used to create the wide field of vision using software that pre-distorted the rendered image in real-time. This initial design would later serve as a basis from which the later designs came. In 2012, the Rift is presented for the first time at the E3 video game trade show by John Carmack. In 2014, Facebook (later Meta) purchased Oculus VR for what at the time was stated as $2 billion but later revealed that the more accurate figure was $3 billion. This purchase occurred after the first development kits ordered through Oculus' 2012 Kickstarter had shipped in 2013 but before the shipping of their second development kits in 2014. ZeniMax, Carmack's former employer, sued Oculus and Facebook for taking company secrets to Facebook; the verdict",
    "text_hash": "82887a9239eddc68f1f2e809f0ca7251",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000526",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 9,
    "text": "more accurate figure was $3 billion. This purchase occurred after the first development kits ordered through Oculus' 2012 Kickstarter had shipped in 2013 but before the shipping of their second development kits in 2014. ZeniMax, Carmack's former employer, sued Oculus and Facebook for taking company secrets to Facebook; the verdict was in favour of ZeniMax, settled out of court later. In 2013, Valve discovered and freely shared the breakthrough of low-persistence displays which make lag-free and smear-free display of VR content possible. This was adopted by Oculus and was used in all their future headsets. In early 2014, Valve showed off their SteamSight prototype, the precursor to both consumer headsets released in 2016. It shared major features with the consumer headsets including separate 1K displays per eye, low persistence, positional tracking over a large area, and Fresnel lenses. HTC and Valve announced the virtual reality headset HTC Vive and controllers in 2015. The set included tracking technology called Lighthouse, which utilized wall-mounted \"base stations\" for positional tracking using infrared light. In 2014, Sony announced Project Morpheus (its code name for the PlayStation VR), a virtual reality headset for the PlayStation 4 video game console. The Chinese headset AntVR was released in late 2014; it was briefly competitive in the Chinese market but ultimately unable to compete with the larger technology companies. In 2015, Google announced Cardboard, a do-it-yourself stereoscopic viewer: the user places their smartphone in the cardboard holder, which they wear on their head. Michael Naimark was appointed Google's first-ever 'resident artist' in their new VR division. The Kickstarter campaign for Gloveone, a pair of gloves providing motion tracking and haptic feedback, was successfully funded, with over $150,000 in contributions. Also in 2015, Razer unveiled its open source project OSVR. By 2016, there were at least 230 companies developing",
    "text_hash": "e1fd3307821823c28375d6b7274415fa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000527",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 10,
    "text": "first-ever 'resident artist' in their new VR division. The Kickstarter campaign for Gloveone, a pair of gloves providing motion tracking and haptic feedback, was successfully funded, with over $150,000 in contributions. Also in 2015, Razer unveiled its open source project OSVR. By 2016, there were at least 230 companies developing VR-related products. Amazon, Apple, Facebook, Google, Microsoft, Sony and Samsung all had dedicated AR and VR groups. Dynamic binaural audio was common to most headsets released that year. However, haptic interfaces were not well developed, and most hardware packages incorporated button-operated handsets for touch-based interactivity. Visually, displays were still of a low-enough resolution and frame rate that images were still identifiable as virtual. In 2016, HTC shipped its first units of the HTC Vive SteamVR headset. This marked the first major commercial release of sensor-based tracking, allowing for free movement of users within a defined space. A patent filed by Sony in 2017 showed they were developing a similar location tracking technology to the Vive for PlayStation VR, with the potential for the development of a wireless headset. In 2019, Oculus released the Oculus Rift S and a standalone headset, the Oculus Quest. These headsets utilized inside-out tracking compared to external outside-in tracking seen in previous generations of headsets. Later in 2019, Valve released the Valve Index. Notable features include a 130\u00b0 field of view, off-ear headphones for immersion and comfort, open-handed controllers which allow for individual finger tracking, front facing cameras, and a front expansion slot meant for extensibility. In 2020, Oculus released the Oculus Quest 2, later renamed the Meta Quest 2. Some new features include a sharper screen, reduced price, and increased performance. Facebook (which became Meta a year later) initially required users to log in with a Facebook account in order to use the new headset.",
    "text_hash": "402d3425408b7daf1257842efe47f152",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000528",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 11,
    "text": "In 2020, Oculus released the Oculus Quest 2, later renamed the Meta Quest 2. Some new features include a sharper screen, reduced price, and increased performance. Facebook (which became Meta a year later) initially required users to log in with a Facebook account in order to use the new headset. In 2021 the Oculus Quest 2 accounted for 80% of all VR headsets sold. In 2021, EASA approved the first Virtual Reality-based Flight Simulation Training Device. The device, made by Loft Dynamics for rotorcraft pilots, enhances safety by opening up the possibility of practicing risky maneuvers in a virtual environment. This addresses a key risk area in rotorcraft operations, where statistics show that around 20% of accidents occur during training flights. In 2022, Meta released the Meta Quest Pro. This device utilised a thinner, visor-like design that was not fully enclosed, and was the first headset by Meta to target mixed reality applications using high-resolution colour video passthrough. It also included integrated face and eye tracking, pancake lenses, and updated Touch Pro controllers with on-board motion tracking. In 2023, Sony released the PlayStation VR2, a follow-up to their 2016 headset. The device includes inside-out tracking, eye-tracked foveated rendering, higher-resolution OLED displays, controllers with adaptive triggers and haptic feedback, 3D audio, and a wider field of view. While initially exclusive for use with the PlayStation 5 console, a PC adapter was released in August 2024. Later in 2023, Meta released the Meta Quest 3, the successor to the Quest 2. It features the pancake lenses and mixed reality features of the Quest Pro, as well as an increased field of view and resolution compared to Quest 2. In October 2024 Meta released a lower cost entry headset the Meta Quest 3S with the same fresnel lenses as the Quest 2 and",
    "text_hash": "e3f1659b2b196efee96a0b60e046e923",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000529",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 12,
    "text": "features the pancake lenses and mixed reality features of the Quest Pro, as well as an increased field of view and resolution compared to Quest 2. In October 2024 Meta released a lower cost entry headset the Meta Quest 3S with the same fresnel lenses as the Quest 2 and a lower resolution of 1832x1920 as compared to 2064x2208 on the Quest 3. In 2024, Apple released the Apple Vision Pro. The device is a fully enclosed mixed reality headset that strongly utilises video passthrough. While some VR experiences are available on the device, it lacks standard VR headset features such as external controllers or support for OpenXR and is instead branded as a \"spatial computer\". In 2024, the Federal Aviation Administration approved its first virtual reality flight simulation training device: Loft Dynamics' virtual reality Airbus Helicopters H125 FSTD\u2014the same device EASA qualified. As of September 2024, Loft Dynamics remains the only VR FSTD qualified by EASA and the FAA. == Technology == === Hardware === Modern virtual reality headset displays are based on technology developed for smartphones including: gyroscopes and motion sensors for tracking head, body, and hand positions; small HD screens for stereoscopic displays; and small, lightweight and fast computer processors. These components led to relative affordability for independent VR developers, and led to the 2012 Oculus Rift Kickstarter offering the first independently developed VR headset. Independent production of VR images and video has increased alongside the development of affordable omnidirectional cameras, also known as 360-degree cameras or VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications.",
    "text_hash": "eb87658d884706d50223c1e909d4c4b3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000530",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 13,
    "text": "VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective image to the right eye. A distinction is made between active (e.g., shutter glasses) and passive technologies (e.g. polarizing filters or Infitec). In order to improve the feeling of immersion, wearable multi-string cables offer haptics to complex geometries in virtual reality. These strings offer fine control of each finger joint to simulate the haptics involved in touching these virtual geometries. Special input devices are required for interaction with the virtual world. Some of the most common input devices are motion controllers and optical tracking sensors. In some cases, wired gloves are used. Controllers typically use optical tracking systems (primarily infrared cameras) for location and navigation so that the user can move freely without wiring. Some input devices provide the user with force feedback to the hands or other parts of the body so that the user can orientate themselves in the three-dimensional world through haptics and sensor technology as a further sensory sensation and carry out realistic simulations. This allows the viewer to have a sense of direction in the artificial landscape. Additional haptic feedback can be obtained from omnidirectional treadmills (with which walking in virtual space is controlled by real walking movements) and vibration gloves and suits. Virtual reality",
    "text_hash": "bef8185b6adcccfd008c762916eaf7a9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000531",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 14,
    "text": "further sensory sensation and carry out realistic simulations. This allows the viewer to have a sense of direction in the artificial landscape. Additional haptic feedback can be obtained from omnidirectional treadmills (with which walking in virtual space is controlled by real walking movements) and vibration gloves and suits. Virtual reality cameras can be used to create VR photography using 360-degree panorama videos. VR cameras are available in various formats, with varying numbers of lenses installed in the camera. === Software === The Virtual Reality Modelling Language (VRML), first introduced in 1994, was intended for the development of \"virtual worlds\" without dependency on headsets. The Web3D consortium was subsequently founded in 1997 for the development of industry standards for web-based 3D graphics. The consortium subsequently developed X3D from the VRML framework as an archival, open-source standard for web-based distribution of VR content. WebVR is an experimental JavaScript application programming interface (API) that provides support for various virtual reality devices, such as the HTC Vive, Oculus Rift, Google Cardboard or OSVR, in a web browser. == Visual immersion experience == === Display resolution === Minimal Angle of Resolution (MAR) refers to the minimum distance between two display pixels. At a distance, a viewer can clearly distinguish the independent pixels. Often measured in arc-seconds, MAR between two pixels has to do with the viewing distance. For the general public, resolution is about 30\u201365 arc-seconds, which is referred to as the spatial resolution when combined with distance. Given the viewing distance of 1m and 2m respectively, regular viewers won't be able to perceive two pixels as separate if they are less than 0.29mm apart at 1m and less than 0.58mm apart at 2m. === Image latency and display refresh frequency === Most small-size displays have a refresh rate of 60 Hz, which adds about",
    "text_hash": "da6757b34ec8125b4261fa1aa6029247",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000532",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 15,
    "text": "2m respectively, regular viewers won't be able to perceive two pixels as separate if they are less than 0.29mm apart at 1m and less than 0.58mm apart at 2m. === Image latency and display refresh frequency === Most small-size displays have a refresh rate of 60 Hz, which adds about 15ms of additional latency. The number is reduced to less than 7ms if the refresh rate is increased to 120 Hz or even 240 Hz and more. Participants generally feel that the experience is more immersive with higher refresh rates as a result. However, higher refresh rates require a more powerful graphics processing unit. === Relationship between display and field of view === In assessing the achieved immersion by a VR device, we need to consider our field of view (FOV) in addition to image quality. Our eyes have a horizontal FOV from about 107 or 110 degrees to the temporal side to about 60 or 70 degrees toward the nose and a vertical FOV from about 95 degrees downward to 85 degrees upward, and eye movements are estimated as roughly 30 deg to either side horizontally and 20 vertically. Binocular vision is limited to the 120 or 140 degrees where the right and the left visual fields overlap. With eye movements, we have an FOV of roughly 300 degrees x 175 degrees with two eyes, i.e., approximately one third of the full 360-deg sphere. == Applications == Virtual reality is most commonly used in entertainment applications such as video games, 3D cinema, amusement park rides including dark rides and social virtual worlds. Consumer virtual reality headsets were first released by video game companies in the early-mid 1990s. Beginning in the 2010s, next-generation commercial tethered headsets were released by Oculus (Rift), HTC (Vive) and Sony (PlayStation VR), setting off a",
    "text_hash": "acd0ad5adde34956dadb4ebab2ce4f34",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000533",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 16,
    "text": "games, 3D cinema, amusement park rides including dark rides and social virtual worlds. Consumer virtual reality headsets were first released by video game companies in the early-mid 1990s. Beginning in the 2010s, next-generation commercial tethered headsets were released by Oculus (Rift), HTC (Vive) and Sony (PlayStation VR), setting off a new wave of application development. 3D cinema has been used for sporting events, pornography, fine art, music videos and short films. Since 2015, roller coasters and theme parks have incorporated virtual reality to match visual effects with haptic feedback. VR not only fits the trend of the digital industry but also enhances the film's visual effect. The film gives the audience more ways to interact through VR technology. In social sciences and psychology, virtual reality offers a cost-effective tool to study and replicate interactions in a controlled environment. It can be used as a form of therapeutic intervention. For instance, there is the case of the virtual reality exposure therapy (VRET), a form of exposure therapy for treating anxiety disorders such as post traumatic stress disorder (PTSD) and phobias. A VR therapy has been designed to help people with psychosis and agoraphobia manage their avoidance of outside environments. In the therapy, the user wears a headset and a virtual character provides psychological advice and guides them as they explore simulated environments (such as a cafe or a busy street). NICE is assessing the therapy to see if it should be recommended on the NHS. During the COVID-19 pandemic, social VR has also been used as a mental-health tool in a form of self-administered, non-traditional cognitive behavioural therapy. Virtual reality programs are being used in the rehabilitation processes with elderly individuals that have been diagnosed with Alzheimer's disease. This gives these elderly patients the opportunity to simulate real experiences that they",
    "text_hash": "1ea6bfd010d339e8d1b3bb10e235fac8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000534",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 17,
    "text": "has also been used as a mental-health tool in a form of self-administered, non-traditional cognitive behavioural therapy. Virtual reality programs are being used in the rehabilitation processes with elderly individuals that have been diagnosed with Alzheimer's disease. This gives these elderly patients the opportunity to simulate real experiences that they would not otherwise be able to experience due to their current state. 17 recent studies with randomized controlled trials have shown that virtual reality applications are effective in treating cognitive deficits with neurological diagnoses. Loss of mobility in elderly patients can lead to a sense of loneliness and depression. Virtual reality is able to assist in making aging in place a lifeline to an outside world that they cannot easily navigate. Virtual reality allows exposure therapy to take place in a safe environment. In medicine, simulated VR surgical environments were first developed in the 1990s. Under the supervision of experts, VR can provide effective and repeatable training at a low cost, allowing trainees to recognize and amend errors as they occur. Virtual reality has been used in physical rehabilitation since the 2000s. Despite numerous studies conducted, good quality evidence of its efficacy compared to other rehabilitation methods without sophisticated and expensive equipment is lacking for the treatment of Parkinson's disease. A 2018 review on the effectiveness of mirror therapy by virtual reality and robotics for any type of pathology concluded in a similar way. Another study was conducted that showed the potential for VR to promote mimicry and revealed the difference between non-autistic and autistic individuals in their response to a two-dimensional avatar. Immersive virtual reality technology with myoelectric and motion tracking control may represent a possible therapy option for treatment-resistant phantom limb pain. Pain scale measurements were taken into account and an interactive 3-D kitchen environment was developed based",
    "text_hash": "df5aad319a9e19f8ce5cc6037b431dff",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000535",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 18,
    "text": "difference between non-autistic and autistic individuals in their response to a two-dimensional avatar. Immersive virtual reality technology with myoelectric and motion tracking control may represent a possible therapy option for treatment-resistant phantom limb pain. Pain scale measurements were taken into account and an interactive 3-D kitchen environment was developed based on the principles of mirror therapy to allow for control of virtual hands while wearing a motion-tracked VR headset. A systematic search in Pubmed and Embase was performed to determine results that were pooled in two meta-analysis. Meta-analysis showed a significant result in favor of VRT for balance. In the fast-paced and globalised business world, meetings in VR are used to create an environment in which interactions with other people (e.g. colleagues, customers, partners) can feel more natural than a phone call or video chat. In the customisable meeting rooms all parties can join using the VR headset and interact as if they are in the same physical room. Presentations, videos or 3D models (of e.g. products or prototypes) can be uploaded and interacted with. Compared to traditional text-based CMC, Avatar-based interactions in 3D virtual environment lead to higher levels of consensus, satisfaction, and cohesion among group members. VR can simulate real workspaces for workplace occupational safety and health purposes, educational purposes, and training purposes. It can be used to provide learners with a virtual environment where they can develop their skills without the real-world consequences of failing. It has been used and studied in primary education, anatomy teaching, military, astronaut training, flight simulators, mining and metallurgical operations training, medical education, geography education, architectural design, driver training, and bridge inspection. Immersive VR engineering systems enable engineers to see virtual prototypes prior to the availability of any physical prototypes. Supplementing training with virtual training environments has been claimed to offer avenues",
    "text_hash": "de3b3d459b3927618b8ffa69e6e30acd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000536",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 19,
    "text": "astronaut training, flight simulators, mining and metallurgical operations training, medical education, geography education, architectural design, driver training, and bridge inspection. Immersive VR engineering systems enable engineers to see virtual prototypes prior to the availability of any physical prototypes. Supplementing training with virtual training environments has been claimed to offer avenues of realism in military and healthcare training while minimizing cost. It also has been claimed to reduce military training costs by minimizing the amounts of ammunition expended during training periods. VR can be used for the healthcare training and education for medical practitioners. Further, several application have been developed for multiple types of safety training. The latest results indicates that virtual reality safety training is more effective than traditional training in terms of knowledge acquisition and knowledge retention. In the engineering field, VR has proved very useful for both engineering educators and the students. A previously expensive cost in the educational department now being much more accessible due to lowered overall costs, has proven to be a very useful tool in educating future engineers. The most significant element lies in the ability for the students to be able to interact with 3-D models that accurately respond based on real world possibilities. This added tool of education provides many the immersion needed to grasp complex topics and be able to apply them. As noted, the future architects and engineers benefit greatly by being able to form understandings between spatial relationships and providing solutions based on real-world future applications. The first fine art virtual world was created in the 1970s. As the technology developed, more artistic programs were produced throughout the 1990s, including feature films. When commercially available technology became more widespread, VR festivals began to emerge in the mid-2010s. The first uses of VR in museum settings began in the 1990s,",
    "text_hash": "9c2fa664f5fcb760b7902cf1f35ce8e0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000537",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 20,
    "text": "art virtual world was created in the 1970s. As the technology developed, more artistic programs were produced throughout the 1990s, including feature films. When commercially available technology became more widespread, VR festivals began to emerge in the mid-2010s. The first uses of VR in museum settings began in the 1990s, seeing a significant increase in the mid-2010s. Additionally, museums have begun making some of their content virtual reality accessible. Virtual reality's growing market presents an opportunity and an alternative channel for digital marketing. It is also seen as a new platform for e-commerce, particularly in the bid to challenge traditional \"brick and mortar\" retailers. However, a 2018 study revealed that the majority of goods are still purchased in physical stores. In the case of education, the uses of virtual reality have demonstrated being capable of promoting higher order thinking, promoting the interest and commitment of students, the acquisition of knowledge, promoting mental habits and understanding that are generally useful within an academic context. A case has also been made for including virtual reality technology in the context of public libraries. This would give library users access to cutting-edge technology and unique educational experiences. This could include giving users access to virtual, interactive copies of rare texts and artifacts and to tours of famous landmarks and archeological digs (as in the case with the Virtual Ganjali Khan Project). Starting in the early 2020s, virtual reality has also been discussed as a technological setting that may support people's grieving process, based on digital recreations of deceased individuals. In 2021, this practice received substantial media attention following a South Korean TV documentary, which invited a grieving mother to interact with a virtual replica of her deceased daughter. Subsequently, scientists have summarized several potential implications of such endeavours, including its potential to facilitate adaptive",
    "text_hash": "b9a246c9554788b11af1abddf38bbcc2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000538",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 21,
    "text": "digital recreations of deceased individuals. In 2021, this practice received substantial media attention following a South Korean TV documentary, which invited a grieving mother to interact with a virtual replica of her deceased daughter. Subsequently, scientists have summarized several potential implications of such endeavours, including its potential to facilitate adaptive mourning, but also many ethical challenges. Growing interest in the metaverse has resulted in organizational efforts to incorporate the many diverse applications of virtual reality into ecosystems like VIVERSE, reportedly offering connectivity between platforms for a wide range of uses. Virtual reality has increasingly been used in various religious applications, including in the creation of metaverse places of worship. == Medical uses of VR == Virtual reality (VR) technology has emerged as a significant tool in medical training and education. Specifically, there has been a major leap in innovation in surgical simulation and surgical real-time enhancement. Studies done at North Carolina medical institutions have demonstrated improvement in technical performance and skills among medical students and active surgeons using VR training as compared to traditional training, especially in procedures such as total hip arthroplasty. Alongside this, other VR simulation programs such as LapSim, improve basic coordination, instrument handling, and procedure-based skills. These simulations aim to have high ratings for feedback and haptic touch, which provides a more realistic surgical feel. Studies show significant improvement in task completion time and scores after 4-week training sessions of LapSim. This simulation environment also allows surgeons to practice without risk to real patients, promoting patient safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR",
    "text_hash": "40b44d9189a88ff348a9d83de1a4e16c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000539",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 22,
    "text": "safety. Based on data from research conducted from the University Hospitals Schleswig-Holstein and collaborators from other institutions, medical students and surgeons with years of experience, show marked performance boosts after practicing with LapSim VR technology. Another recent study at North Carolina University of Chapel Hill has shown that developing VR and Augmented Reality (AR) systems have allowed surgeons to keep their eyes on a patient while accessing CT scans. This VR system allows for laparoscopic imaging integration, real-time skin layer visualization, and enhanced surgical precision capabilities. These are both examples of how studies have shown surgeons can take advantage of additional virtual reality simulation practices, which can create incredible experiences, provide customized scenarios, and provide independent learning with haptic feedback. These VR systems need to be realistic enough for education tools alongside being able to measure performance of a surgeon. Some potential future challenges of this technology would be enhancing complex scenarios alongside the realism aspects. These technologies would need to incorporate stress-inducing factors along with other realistic simulation ideas. Furthermore, there would be a need to have better AR integration to help the surgeon have better eyes-on precision guidance. Lastly, there would be a strong need to keep things cost-effective with an abundance of availability. == Concerts == In June of 2020, Jean Michel Jarre performed in VRChat. In July, Brendan Bradley released the free FutureStages web-based virtual reality venue for live events and concerts throughout the 2020 shutdown, Justin Bieber performed on November 18, 2021 in WaveXR. On December 2, 2021, non-player characters performed at the Mugar Omni Theater with audiences interacting with a live performer in both virtual reality and projected on the IMAX dome screen. Meta's Foo Fighters Super Bowl VR concert performed on Venues. Post Malone performed in Venues starting July 15, 2022. Megan Thee",
    "text_hash": "71c4666ebd57adf03e4f2c4629e2b6cb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000540",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 23,
    "text": "December 2, 2021, non-player characters performed at the Mugar Omni Theater with audiences interacting with a live performer in both virtual reality and projected on the IMAX dome screen. Meta's Foo Fighters Super Bowl VR concert performed on Venues. Post Malone performed in Venues starting July 15, 2022. Megan Thee Stallion performed on AMAZE at AMC Theaters throughout 2022. On October 24, 2021, Billie Eilish performed on Oculus Venues. Pop group Imagine Dragons performed on June 15, 2022. == Concerns and challenges == === Health and safety === There are many health and safety considerations of virtual reality. A number of unwanted symptoms have been caused by prolonged use of virtual reality, and these may have slowed the proliferation of the technology. Most virtual reality systems come with consumer warnings, including seizures; developmental issues in children; trip-and-fall and collision warnings; discomfort; repetitive stress injury; and interference with medical devices. Some users may experience twitches, seizures, or blackouts while using VR headsets, even if they do not have a history of epilepsy and have never had blackouts or seizures before. One in 4,000 people, or .025%, may experience these symptoms. Motion sickness, eyestrain, headaches, and discomfort are the most prevalent short-term adverse effects. In addition, because of the virtual reality headsets' heavy weight, discomfort may be more likely among children. Therefore, children are advised against using VR headsets. Other problems may occur in physical interactions with one's environment. While wearing VR headsets, people quickly lose awareness of their real-world surroundings and may injure themselves by tripping over or colliding with real-world objects. VR headsets may regularly cause eye fatigue, as does all screened technology, because people tend to blink less when watching screens, causing their eyes to become more dried out. There have been some concerns about VR headsets contributing to",
    "text_hash": "11daa8d465e96e33d07db3bbaf415cfd",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000541",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 24,
    "text": "may injure themselves by tripping over or colliding with real-world objects. VR headsets may regularly cause eye fatigue, as does all screened technology, because people tend to blink less when watching screens, causing their eyes to become more dried out. There have been some concerns about VR headsets contributing to myopia, but although VR headsets sit close to the eyes, they may not necessarily contribute to nearsightedness if the focal length of the image being displayed is sufficiently far away. Virtual reality sickness (also known as cybersickness) occurs when a person's exposure to a virtual environment causes symptoms that are similar to motion sickness symptoms. Women are significantly more affected than men by headset-induced symptoms, at rates of around 77% and 33% respectively. The most common symptoms are general discomfort, headache, stomach awareness, nausea, vomiting, pallor, sweating, fatigue, drowsiness, disorientation, and apathy. For example, Nintendo's Virtual Boy received much criticism for its negative physical effects, including \"dizziness, nausea, and headaches\". These motion sickness symptoms are caused by a disconnect between what is being seen and what the rest of the body perceives. When the vestibular system, the body's internal balancing system, does not experience the motion that it expects from visual input through the eyes, the user may experience VR sickness. This can also happen if the VR system does not have a high enough frame rate, or if there is a lag between the body's movement and the onscreen visual reaction to it. Because approximately 25\u201340% of people experience some kind of VR sickness when using VR machines, companies are actively looking for ways to reduce VR sickness. Vergence-accommodation conflict (VAC) is one of the main causes of virtual reality sickness. In January 2022 The Wall Street Journal found that VR usage could lead to physical injuries including leg,",
    "text_hash": "2b39e54ae1e18ba44495ea6859e0893f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000542",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 25,
    "text": "some kind of VR sickness when using VR machines, companies are actively looking for ways to reduce VR sickness. Vergence-accommodation conflict (VAC) is one of the main causes of virtual reality sickness. In January 2022 The Wall Street Journal found that VR usage could lead to physical injuries including leg, hand, arm and shoulder injuries. VR usage has also been tied to incidents that resulted in neck injuries (especially injures to the cervical vertebrae). === Children and teenagers in virtual reality === Children are becoming increasingly aware of VR, with the number in the USA having never heard of it dropping by half from Autumn 2016 (40%) to Spring 2017 (19%). A 2022 research report by Piper Sandler revealed that only 26% of U.S. teens own a VR device, 5% use it daily, while 48% of teen headset owners \"seldom\" use it. Of the teens who don't own a VR headset, 9% plan to buy one. 50% of surveyed teens are unsure about the metaverse or don't have any interest, and don't have any plans to purchase a VR headset. Studies show that young children, compared to adults, may respond cognitively and behaviorally to immersive VR in ways that differ from adults. VR places users directly into the media content, potentially making the experience very vivid and real for children. For example, children of 6\u201318 years of age reported higher levels of presence and \"realness\" of a virtual environment compared with adults 19\u201365 years of age. Studies on VR consumer behavior or its effect on children and a code of ethical conduct involving underage users are especially needed, given the availability of VR porn and violent content. Related research on violence in video games suggests that exposure to media violence may affect attitudes, behavior, and even self-concept. Self-concept is a",
    "text_hash": "b00ea1173dcba06afb3e265377252bce",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000543",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 26,
    "text": "behavior or its effect on children and a code of ethical conduct involving underage users are especially needed, given the availability of VR porn and violent content. Related research on violence in video games suggests that exposure to media violence may affect attitudes, behavior, and even self-concept. Self-concept is a key indicator of core attitudes and coping abilities, particularly in adolescents. Early studies conducted on observing versus participating in violent VR games suggest that physiological arousal and aggressive thoughts, but not hostile feelings, are higher for participants than for observers of the virtual reality game. Experiencing VR by children may further involve simultaneously holding the idea of the virtual world in mind while experiencing the physical world. Excessive usage of immersive technology that has very salient sensory features may compromise children's ability to maintain the rules of the physical world, particularly when wearing a VR headset that blocks out the location of objects in the physical world. Immersive VR can provide users with multisensory experiences that replicate reality or create scenarios that are impossible or dangerous in the physical world. Observations of 10 children experiencing VR for the first time suggested that 8-12-years-old kids were more confident to explore VR content when it was in a familiar situation, e.g. the children enjoyed playing in the kitchen context of Job Simulator, and enjoyed breaking rules by engaging in activities they are not allowed to do in reality, such as setting things on fire. === Privacy === Digital privacy concerns have been associated with VR platforms; the persistent tracking required by all VR systems makes the technology particularly useful for, and vulnerable to, mass surveillance, including information gathering of personal actions, movements and responses. Data from eye tracking sensors, which are projected to become a standard feature in virtual reality headsets, may",
    "text_hash": "6302e390a0e742367489cdfcad6bc4ca",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000544",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 27,
    "text": "associated with VR platforms; the persistent tracking required by all VR systems makes the technology particularly useful for, and vulnerable to, mass surveillance, including information gathering of personal actions, movements and responses. Data from eye tracking sensors, which are projected to become a standard feature in virtual reality headsets, may indirectly reveal information about a user's ethnicity, personality traits, fears, emotions, interests, skills, and physical and mental health conditions. The nature of VR technology means that it can gather a wide range of data about its users. This can include obvious information such as usernames and account information, but also extends to more personal data like physical movements, interaction habits, and responses to virtual environments. In addition, advanced VR systems can capture biometric data like voice patterns, eye movements, and physiological responses to VR experiences. Virtual reality technology has grown substantially since its inception, moving from a niche technology to a mainstream consumer product. As the user base has grown, so too has the amount of personal data collected by these systems. This data can be used to improve VR systems, to provide personalized experiences, or to collect demographic information for marketing purposes. However, it also raises significant privacy concerns, especially when this data is stored, shared, or sold without the user's explicit consent. Existing data protection and privacy laws like the General Data Protection Regulation (GDPR) in the EU, and the California Consumer Privacy Act (CCPA) in the United States, can be applied to VR. These regulations require companies to disclose how they collect and use data, and give users a degree of control over their personal information. Despite these regulations, enforcing privacy laws in VR can be challenging due to the global nature of the technology and the vast amounts of data collected. Due to its history of",
    "text_hash": "0f93b952a8bc3454289908f9152677ac",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000545",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 28,
    "text": "to disclose how they collect and use data, and give users a degree of control over their personal information. Despite these regulations, enforcing privacy laws in VR can be challenging due to the global nature of the technology and the vast amounts of data collected. Due to its history of privacy issues, the involvement of Meta Platforms (formerly Facebook, Inc.) in the VR market has led to privacy concerns specific to its platforms. In August 2020, Facebook announced that Oculus products would become subject to the terms of use and privacy policy of the Facebook social network, and that a Facebook account would be required to use future Oculus headset models, and all existing models (via deprecation of the separate Oculus account system) beginning January 2023. The announcement was criticized for the mandatory integration of Oculus headsets with Facebook data collection and policies (including the Facebook real-name policy), and preventing use of the hardware if the user's account is suspended. The following month, Facebook halted the sale of Oculus products in Germany due to concerns from regulators that the new policy was a violation of GDPR. In 2022, the company would later establish a separate \"Meta account\" system. In 2024, researchers from the University of Chicago demonstrated a security vulnerability in Meta Quest's Android-based system software (leveraging \"Developer Mode\" to inject an infected app), allowing them to obtain users' login credentials and inject false details during online banking sessions. This attack was considered to be difficult to execute outside of research settings but would make its target vulnerable to risks such as phishing, Internet fraud, and grooming. == See also == 16K resolution \u2013 Video or display resolutions with a width of around 16,000 pixels 360-degree video \u2013 Visual arts technique The AlloSphere Research Facility (AlloSphere) \u2013 Research laboratory at",
    "text_hash": "2a1ff9e8e7f718ea9e7a0c99c4745306",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000546",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 29,
    "text": "research settings but would make its target vulnerable to risks such as phishing, Internet fraud, and grooming. == See also == 16K resolution \u2013 Video or display resolutions with a width of around 16,000 pixels 360-degree video \u2013 Visual arts technique The AlloSphere Research Facility (AlloSphere) \u2013 Research laboratory at the University of California, Santa Barbara Computer-mediated reality \u2013 Ability to manipulate one's perception of reality through the use of a computer Diorama \u2013 Three-dimensional full-size or miniature model Extended reality \u2013 Combined real-and-virtual environment Gustatory technology \u2013 Engineering discipline dealing with gustatory representation Haptic suit \u2013 Wearable device that provides haptic feedback Holographic principle \u2013 Physics inside a bounded region is fully captured by physics at the boundary of the region Hyperreality \u2013 Term for cultural process of shifting ideas of reality List of virtual reality headsets Metaverse \u2013 Collective three-dimensional virtual shared space Mixed reality \u2013 Form of 3D computer interaction merging the real world with virtual objectsPages displaying short descriptions of redirect targets MOO \u2013 Text-based online virtual reality system Simulation \u2013 Imitation of the operation of a real-world process or system over time Virtual body \u2013 State of being in a computer-simulated environment Virtual globe \u2013 3D software model or representation of Earth or another world Virtual machining \u2013 Computers simulating and modeling use of machine tools for part manufacturing Virtual reality in fiction Virtual reality in nursing \u2013 Use of virtual reality technology in Nursing Virtual reality website == References == == Further reading == Choi, SangSu; Kiwook Jung; Sang Do Noh (2015). \"Virtual reality applications in manufacturing industries: Past research, present findings, and future directions\". Concurrent Engineering. 23: 40\u201363. doi:10.1177/1063293X14568814. == External links == Isaac, Joseph (2016). \"Step into a new world \u2013 Virtual Reality (VR)\". Retrieved 2 July 2016. Basic Concepts of Virtual",
    "text_hash": "f6ed410b0e1d53ffa53227c76791c058",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000547",
    "article_title": "Virtual reality",
    "article_url": "https://en.wikipedia.org/wiki/Virtual_reality",
    "article_page_id": "32612",
    "chunk_index": 30,
    "text": "Choi, SangSu; Kiwook Jung; Sang Do Noh (2015). \"Virtual reality applications in manufacturing industries: Past research, present findings, and future directions\". Concurrent Engineering. 23: 40\u201363. doi:10.1177/1063293X14568814. == External links == Isaac, Joseph (2016). \"Step into a new world \u2013 Virtual Reality (VR)\". Retrieved 2 July 2016. Basic Concepts of Virtual Reality along with Research Challenges explained in simple words. Mixed Reality Scale \u2013 Milgram and Kishino's (1994) Virtuality Continuum paraphrase with examples. Drummond, Katie (2014). \"The Rise and Fall and Rise of Virtual Reality\". The Verge. Retrieved 15 November 2014. Interviews on the history and future of virtual reality by leaders in the field. \"Virtual reality in human-system interaction\".",
    "text_hash": "75d434862e1ae4e50bb5bdb609020d3a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000548",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 0,
    "text": "Augmented reality (AR), also known as mixed reality (MR), is a form of 3D human\u2013computer interaction that overlays real-time 3D-rendered computer graphics into the real world through a display, such as a handheld device or head-mounted display. This experience is seamlessly interwoven with the physical world such that it is perceived as an immersive aspect of the real environment. In this way, augmented reality alters one's ongoing perception of a real-world environment, compared to virtual reality, which aims to completely replace the user's real-world environment with a simulated one. Augmented reality is typically visual, but can span multiple sensory modalities, including auditory, haptic, and somatosensory. The earliest functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992. Commercial augmented reality experiences were first introduced in entertainment and gaming businesses. Subsequently, augmented reality applications have spanned industries such as education, communications, medicine, and entertainment. Augmented reality frameworks include ARKit and ARCore. Commercial augmented reality headsets include the Magic Leap 1 and HoloLens. A number of companies have promoted the concept of smartglasses that have augmented reality capability. Augmented reality refers to experiences that are artificial and that add to the already existing reality. In AR, information about the environment and its objects can be overlaid on the real world. This information can be virtual or real, e.g. seeing other real sensed or measured information such as electromagnetic radio waves overlaid in exact alignment with where they actually are in space. Augmented reality also has a lot of potential in the gathering and sharing of tacit knowledge. Augmented reality can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction,",
    "text_hash": "45470100cd2992608323b7567809bd80",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000549",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 1,
    "text": "waves overlaid in exact alignment with where they actually are in space. Augmented reality also has a lot of potential in the gathering and sharing of tacit knowledge. Augmented reality can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects. The overlaid sensory information can be constructive (i.e. additive to the natural environment), or destructive (i.e. masking of the natural environment). == Hardware and displays == AR visuals appear on handheld devices (video passthrough) or head-mounted displays (optical see-through or video passthrough). Systems pair a display with sensors (e.g., cameras and IMUs) to register virtual content to the environment; research also explores near-eye optics, projection-based AR, and experimental concepts such as contact-lens or retinal-scanned displays. === Head-mounted displays === AR HMDs place virtual imagery in the user's view using optical see-through or video passthrough and track head motion for stable registration. === Handheld === Phone and tablet AR uses the rear camera (video passthrough) plus on-device SLAM/VIO for tracking. === Projection mapping === Projectors overlay graphics onto real objects/environments without head-worn displays (spatial AR). === AR glasses === Glasses-style near-eye displays aim for lighter, hands-free AR; approaches vary in optics, tracking, and power. == 3D tracking == AR systems estimate device pose and scene geometry so virtual graphics stay aligned with the real world. Common approaches include visual\u2013inertial odometry and SLAM for markerless tracking, and fiducial markers when known patterns are available; image registration and depth cues (e.g., occlusion, shadows) maintain realism. === Software and standards === AR runtimes provide sensing, tracking, and rendering pipelines; mobile platforms expose SDKs with camera access and spatial tracking. Interchange/geospatial formats such as ARML standardize anchors and content. === Interaction and input === Input commonly",
    "text_hash": "5ad2af32eff1ff0d0a655e9b9008325f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000550",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 2,
    "text": "available; image registration and depth cues (e.g., occlusion, shadows) maintain realism. === Software and standards === AR runtimes provide sensing, tracking, and rendering pipelines; mobile platforms expose SDKs with camera access and spatial tracking. Interchange/geospatial formats such as ARML standardize anchors and content. === Interaction and input === Input commonly combines head/gaze with touch, controllers, voice, or hand tracking; audio and haptics can reduce visual load. Human-factors studies report performance benefits but also workload and safety trade-offs depending on task and context. === Design considerations === Key usability factors include stable registration, legible contrast under varied lighting, and low motion-to-photon latency. Visual design often uses depth cues (occlusion, shadows) to support spatial judgment; safety-critical uses emphasize glanceable prompts and minimal interaction. == Comparison with mixed reality/virtual reality == Augmented reality (AR) is largely synonymous with mixed reality (MR). There is also overlap in terminology with extended reality and computer-mediated reality. However, In the 2020s, the differences between AR and MR began to be emphasized. In augmented reality, users are not only able to view digital content within their real environment but can also interact with it as if it were a tangible part of the physical world. This is made possible through devices such as Meta Quest 3S and Apple Vision Pro, which utilize multiple cameras and sensors to enable real-time interaction between virtual and physical elements. Mixed reality that incorporates haptics has sometimes been referred to as visuo-haptic mixed reality. In virtual reality (VR), the users' perception is completely computer-generated, whereas with augmented reality (AR), it is partially generated and partially from the real world. For example, in architecture, VR can be used to create a walk-through simulation of the inside of a new building; and AR can be used to show a building's structures and systems super-imposed on",
    "text_hash": "ab55baeb23210ef22cedc78d8b8d0243",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000551",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 3,
    "text": "computer-generated, whereas with augmented reality (AR), it is partially generated and partially from the real world. For example, in architecture, VR can be used to create a walk-through simulation of the inside of a new building; and AR can be used to show a building's structures and systems super-imposed on a real-life view. Another example is through the use of utility applications. Some AR applications, such as Augment, enable users to apply digital objects into real environments, allowing businesses to use augmented reality devices as a way to preview their products in the real world. Similarly, it can also be used to demo what products may look like in an environment for customers, as demonstrated by companies such as Mountain Equipment Co-op or Lowe's who use augmented reality to allow customers to preview what their products might look like at home. Augmented reality (AR) differs from virtual reality (VR) in the sense that in AR, the surrounding environment is real and AR is just adding virtual objects to the real environment. On the other hand, in VR, the surrounding environment is completely virtual and computer generated. A demonstration of how AR layers objects onto the real world can be seen with augmented reality games. WallaMe is an augmented reality game application that allows users to hide messages in real environments, utilizing geolocation technology in order to enable users to hide messages wherever they may wish in the world. The use of the terms \"mixed reality\" and \"interreality\" is clearly defined in the context of physics and may be slightly different in other fields, however, it is generally seen as, \"bridging the physical and virtual world\". Recent improvements in AR and VR headsets have made the display quality, field of view, and motion tracking more accurate, which makes augmented experiences more",
    "text_hash": "53f52735928d12291f374af28abda010",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000552",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 4,
    "text": "in the context of physics and may be slightly different in other fields, however, it is generally seen as, \"bridging the physical and virtual world\". Recent improvements in AR and VR headsets have made the display quality, field of view, and motion tracking more accurate, which makes augmented experiences more immersive. Improvements in sensor calibration, lightweight optics, and wireless connectivity have also made it easier for users to move around and be comfortable. == History == === Precursors to augmented reality === 1901: Author L. Frank Baum, in his science-fiction novel The Master Key, first mentions the idea of an electronic display/spectacles that overlays data onto real life (in this case 'people'). It is named a 'character marker'. Heads-up displays (HUDs), a precursor technology to augmented reality, were first developed for pilots in the 1950s, projecting simple flight data into their line of sight, thereby enabling them to keep their \"heads up\" and not look down at the instruments. It is a transparent display. === Earliest developments === 1968: Ivan Sutherland creates the first optical-see through head-mounted display that has graphics rendered by a computer. 1975: Myron Krueger creates Videoplace to allow users to interact with virtual objects. 1980: The research by Gavan Lintern of the University of Illinois is the first published work to show the value of a heads up display for teaching real-world flight skills. 1980: Steve Mann creates the first wearable computer, a computer vision system with text and graphical overlays on a photographically mediated scene. 1986: Within IBM, Ron Feigenblatt describes the most widely experienced form of AR today (viz. \"magic window,\" e.g. smartphone-based Pok\u00e9mon Go), use of a small, \"smart\" flat panel display positioned and oriented by hand. 1987: Douglas George and Robert Morris create a working prototype of an astronomical telescope-based \"heads-up display\"",
    "text_hash": "2f616ab7fc8794f741a69c35a2da6de2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000553",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 5,
    "text": "1986: Within IBM, Ron Feigenblatt describes the most widely experienced form of AR today (viz. \"magic window,\" e.g. smartphone-based Pok\u00e9mon Go), use of a small, \"smart\" flat panel display positioned and oriented by hand. 1987: Douglas George and Robert Morris create a working prototype of an astronomical telescope-based \"heads-up display\" system (a precursor concept to augmented reality) which superimposed in the telescope eyepiece, over the actual sky images, multi-intensity star, and celestial body images, and other relevant information. 1990: The term augmented reality is attributed to Thomas P. Caudell, a former Boeing researcher. 1992: Louis Rosenberg developed one of the first functioning AR systems, called Virtual Fixtures, at the United States Air Force Research Laboratory\u2014Armstrong, that demonstrated benefit to human perception. 1992: Steven Feiner, Blair MacIntyre and Doree Seligmann present an early paper on an AR system prototype, KARMA, at the Graphics Interface conference. 1993: Mike Abernathy, et al., report the first use of augmented reality in identifying space debris using Rockwell WorldView by overlaying satellite geographic trajectories on live telescope video. 1993: A widely cited version of the paper above is published in Communications of the ACM \u2013 Special issue on computer augmented environments, edited by Pierre Wellner, Wendy Mackay, and Rich Gold. 1995 - Augmented reality was described as a key technology in the reality-virtuality continuum. 1995: S. Ravela et al. at University of Massachusetts introduce a vision-based system using monocular cameras to track objects (engine blocks) across views for augmented reality. 2004: An outdoor helmet-mounted AR system was demonstrated by Trimble Navigation and the Human Interface Technology Laboratory (HIT lab). === Smartphone AR and modern headsets === 2009: ARToolkit was ported to Adobe Flash (FLARToolkit) by Saqoosha, bringing augmented reality to the web browser. 2015: Microsoft announced the HoloLens augmented reality headset, which uses various sensors and",
    "text_hash": "d4937004d77e6d54d5a1cae9a7beff0b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000554",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 6,
    "text": "was demonstrated by Trimble Navigation and the Human Interface Technology Laboratory (HIT lab). === Smartphone AR and modern headsets === 2009: ARToolkit was ported to Adobe Flash (FLARToolkit) by Saqoosha, bringing augmented reality to the web browser. 2015: Microsoft announced the HoloLens augmented reality headset, which uses various sensors and a processing unit to display virtual imagery over the real world. 2016: Niantic released Pok\u00e9mon Go for iOS and Android in July 2016. The game quickly became one of the most popular smartphone applications and in turn spikes the popularity of augmented reality games. 2018: Magic Leap launched the Magic Leap One augmented reality headset. Leap Motion announced the Project North Star augmented reality headset, and later released it under an open source license. 2019: Microsoft announced HoloLens 2 with significant improvements in terms of field of view and ergonomics. 2022: Magic Leap launched the Magic Leap 2 headset. 2023: Meta Quest 3, a mixed reality VR headset was developed by Reality Labs, a division of Meta Platforms. In the same year, Apple Vision Pro was released. 2024: Meta Platforms revealed the Orion AR glasses prototype. == Uses == Augmented reality has been explored for many uses, including education and business. Some of the earliest cited examples include augmented reality used to support surgery by providing virtual overlays to guide medical practitioners, to AR content for astronomy and welding. Example application areas described below include archaeology, architecture, commerce and education. === Education and training === AR for education and training can overlay 3D models and step-by-step guidance in real settings (e.g., anatomy, maintenance); systematic reviews report learning benefits alongside design and implementation caveats that vary by context and task. === Navigation and maps === Augmented reality navigation overlays route guidance or hazard cues onto the real scene, typically via smartphone",
    "text_hash": "78b596b1e3da21f36a0abe9d21599d8c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000555",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 7,
    "text": "can overlay 3D models and step-by-step guidance in real settings (e.g., anatomy, maintenance); systematic reviews report learning benefits alongside design and implementation caveats that vary by context and task. === Navigation and maps === Augmented reality navigation overlays route guidance or hazard cues onto the real scene, typically via smartphone \"live view\" or in-vehicle heads-up displays. Research finds AR can improve wayfinding and driver situation awareness, but human-factors trade-offs (distraction, cognitive load, occlusion) matter for safety-critical use. See also: Head-up display, Automotive navigation system, Wayfinding === Commerce === In 2018, Apple announced USDZ, a file format based on Universal Scene Description from Pixar, which allows 3D objects to be viewed in AR on iPhones and iPads with iOS 12. Apple has created an AR QuickLook Gallery that allows people to experience augmented reality through their own Apple device. In 2018, Shopify, the Canadian e-commerce company, announced AR Quick Look integration. Their merchants will be able to upload 3D models of their products and their users will be able to tap on the models inside the Safari browser on their iOS devices to view them in their real-world environments. AR technology is used by furniture retailers such as IKEA, Houzz, and Wayfair. These retailers offer apps that allow consumers to view their products in their home prior to purchasing anything. In 2017, Ikea announced the Ikea Place app. It contains a catalogue of over 2,000 products\u2014nearly the company's full collection of sofas, armchairs, coffee tables, and storage units which one can place anywhere in a room with their phone. The app made it possible to have 3D and true-to-scale models of furniture in the customer's living space. IKEA realized that their customers are not shopping in stores as often or making direct purchases anymore. Shopify's acquisition of Primer, an AR app",
    "text_hash": "b0988b13709820651a4a8d0235e66d6e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000556",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 8,
    "text": "place anywhere in a room with their phone. The app made it possible to have 3D and true-to-scale models of furniture in the customer's living space. IKEA realized that their customers are not shopping in stores as often or making direct purchases anymore. Shopify's acquisition of Primer, an AR app aims to push small and medium-sized sellers towards interactive AR shopping with easy to use AR integration and user experience for both merchants and consumers. AR helps the retail industry reduce operating costs. Merchants upload product information to the AR system, and consumers can use mobile terminals to search and generate 3D maps. === Surgery === One of the first applications of augmented reality was in healthcare, particularly to support the planning, practice, and training of surgical procedures. As far back as 1992, enhancing human performance during surgery was a formally stated objective when building the first augmented reality systems at U.S. Air Force laboratories. AR provides surgeons with patient monitoring data in the style of a fighter pilot's heads-up display, and allows patient imaging records, including functional videos, to be accessed and overlaid. Examples include a virtual X-ray view based on prior tomography or on real-time images from ultrasound and confocal microscopy probes, visualizing the position of a tumor in the video of an endoscope, or radiation exposure risks from X-ray imaging devices. AR can enhance viewing a fetus inside a mother's womb. Siemens, Karl Storz and IRCAD have developed a system for laparoscopic liver surgery that uses AR to view sub-surface tumors and vessels. Guidance overlays and image fusion support planning and intraoperative visualization across several specialties; reviews note accuracy/registration constraints and workflow integration issues. The HoloLens is capable of displaying images for image-guided surgery. As augmented reality advances, it finds increasing applications in healthcare. Augmented reality and",
    "text_hash": "00c7b13af27030c22327dbaf5a0e8293",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000557",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 9,
    "text": "to view sub-surface tumors and vessels. Guidance overlays and image fusion support planning and intraoperative visualization across several specialties; reviews note accuracy/registration constraints and workflow integration issues. The HoloLens is capable of displaying images for image-guided surgery. As augmented reality advances, it finds increasing applications in healthcare. Augmented reality and similar computer based-utilities are being used to train medical professionals. In healthcare, AR can be used to provide guidance during diagnostic and therapeutic interventions e.g. during surgery. Magee et al., for instance, describe the use of augmented reality for medical training in simulating ultrasound-guided needle placement. Recently, augmented reality began seeing adoption in neurosurgery, a field that requires heavy amounts of imaging before procedures. Smartglasses can be incorporated into the operating room to aide in surgical procedures; possibly displaying patient data conveniently while overlaying precise visual guides for the surgeon. Augmented reality headsets like the Microsoft HoloLens have been theorized to allow for efficient sharing of information between doctors, in addition to providing a platform for enhanced training. This can, in some situations (i.e. patient infected with contagious disease), improve doctor safety and reduce PPE use. While mixed reality has lots of potential for enhancing healthcare, it does have some drawbacks too. The technology may never fully integrate into scenarios when a patient is present, as there are ethical concerns surrounding the doctor not being able to see the patient. Mixed reality is also useful for healthcare education. For example, according to a 2022 report from the World Economic Forum, 85% of first-year medical students at Case Western Reserve University reported that mixed reality for teaching anatomy was \"equivalent\" or \"better\" than the in-person class. === Flight training === Building on decades of perceptual-motor research in experimental psychology, researchers at the Aviation Research Laboratory of the University of Illinois at",
    "text_hash": "44122dc1b006771fa4cd05cd9690609f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000558",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 10,
    "text": "85% of first-year medical students at Case Western Reserve University reported that mixed reality for teaching anatomy was \"equivalent\" or \"better\" than the in-person class. === Flight training === Building on decades of perceptual-motor research in experimental psychology, researchers at the Aviation Research Laboratory of the University of Illinois at Urbana\u2013Champaign used augmented reality in the form of a flight path in the sky to teach flight students how to land an airplane using a flight simulator. An adaptive augmented schedule in which students were shown the augmentation only when they departed from the flight path proved to be a more effective training intervention than a constant schedule. Flight students taught to land in the simulator with the adaptive augmentation learned to land a light aircraft more quickly than students with the same amount of landing training in the simulator but with constant augmentation or without any augmentation. === Military === The first augmented reality system that integrated haptic 3D input was the Virtual Fixtures platform, which was developed in 1992 by Louis Rosenberg at the Armstrong Laboratories of the United States Air Force. It enabled human users to control robots in real-world environments using a haptic controller. Published studies showed that by introducing virtual objects into the real world, significant performance increases could be achieved by human operators. An interesting early application of AR occurred when Rockwell International created video map overlays of satellite and orbital debris tracks to aid in space observations at Air Force Maui Optical System. In their 1993 paper \"Debris Correlation Using the Rockwell WorldView System\" the authors describe the use of map overlays applied to video from space surveillance telescopes. The map overlays indicated the trajectories of various objects in geographic coordinates. This allowed telescope operators to identify satellites, and also to identify and",
    "text_hash": "3a464682de088dfbb9807e57005212ef",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000559",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 11,
    "text": "In their 1993 paper \"Debris Correlation Using the Rockwell WorldView System\" the authors describe the use of map overlays applied to video from space surveillance telescopes. The map overlays indicated the trajectories of various objects in geographic coordinates. This allowed telescope operators to identify satellites, and also to identify and catalog potentially dangerous space debris. Starting in 2003 the US Army integrated the SmartCam3D augmented reality system into the Shadow Unmanned Aerial System to aid sensor operators using telescopic cameras to locate people or points of interest. The system combined fixed geographic information including street names, points of interest, airports, and railroads with live video from the camera system. The system offered a \"picture in picture\" mode that allows it to show a synthetic view of the area surrounding the camera's field of view. This helps solve a problem in which the field of view is so narrow that it excludes important context, as if \"looking through a soda straw\". The system displays real-time friend/foe/neutral location markers blended with live video, providing the operator with improved situational awareness. Combat reality can be simulated and represented using complex, layered data and visual aides, most of which are head-mounted displays (HMD), which encompass any display technology that can be worn on the user's head. Military training solutions are often built on commercial off-the-shelf (COTS) technologies, such as Improbable's synthetic environment platform, Virtual Battlespace 3 and VirTra, with the latter two platforms used by the United States Army. As of 2018, VirTra is being used by both civilian and military law enforcement to train personnel in a variety of scenarios, including active shooter, domestic violence, and military traffic stops. In 2017, the U.S. Army was developing the Synthetic Training Environment (STE), a collection of technologies for training purposes that was expected to include",
    "text_hash": "ff7b6897874cca355021d8ab3ffb2261",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000560",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 12,
    "text": "being used by both civilian and military law enforcement to train personnel in a variety of scenarios, including active shooter, domestic violence, and military traffic stops. In 2017, the U.S. Army was developing the Synthetic Training Environment (STE), a collection of technologies for training purposes that was expected to include mixed reality. As of 2018, STE was still in development without a projected completion date. Some recorded goals of STE included enhancing realism and increasing simulation training capabilities and STE availability to other systems. It was claimed that mixed-reality environments like STE could reduce training costs, such as reducing the amount of ammunition expended during training. In 2018, it was reported that STE would include representation of any part of the world's terrain for training purposes. STE would offer a variety of training opportunities for squad brigade and combat teams, including Stryker, armory, and infantry teams. Researchers at USAF Research Lab (Calhoun, Draper et al.) found an approximately two-fold increase in the speed at which UAV sensor operators found points of interest using this technology. This ability to maintain geographic awareness quantitatively enhances mission efficiency. The system is in use on the US Army RQ-7 Shadow and the MQ-1C Gray Eagle Unmanned Aerial Systems. In combat, AR can serve as a networked communication system that renders useful battlefield data onto a soldier's goggles in real time. From the soldier's viewpoint, people and various objects can be marked with special indicators to warn of potential dangers. Virtual maps and 360\u00b0 view camera imaging can also be rendered to aid a soldier's navigation and battlefield perspective, and this can be transmitted to military leaders at a remote command center. The combination of 360\u00b0 view cameras visualization and AR can be used on board combat vehicles and tanks as circular review system. AR",
    "text_hash": "c577eb8fc7fb6b3aec55ed37492d4528",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000561",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 13,
    "text": "camera imaging can also be rendered to aid a soldier's navigation and battlefield perspective, and this can be transmitted to military leaders at a remote command center. The combination of 360\u00b0 view cameras visualization and AR can be used on board combat vehicles and tanks as circular review system. AR can be an effective tool for virtually mapping out the 3D topologies of munition storages in the terrain, with the choice of the munitions combination in stacks and distances between them with a visualization of risk areas. The scope of AR applications also includes visualization of data from embedded munitions monitoring sensors. === Navigation === The NASA X-38 was flown using a hybrid synthetic vision system that overlaid map data on video to provide enhanced navigation for the spacecraft during flight tests from 1998 to 2002. It used the LandForm software which was useful for times of limited visibility, including an instance when the video camera window frosted over leaving astronauts to rely on the map overlays. The LandForm software was also test flown at the Army Yuma Proving Ground in 1999. In the photo at right one can see the map markers indicating runways, air traffic control tower, taxiways, and hangars overlaid on the video. === Industrial environments === In industrial environments, augmented reality is proving to have a substantial impact with use cases emerging across all aspect of the product lifecycle, starting from product design and new product introduction (NPI) to manufacturing to service and maintenance, to material handling and distribution. For example, labels were displayed on parts of a system to clarify operating instructions for a mechanic performing maintenance on a system. Assembly lines benefited from the usage of AR. In addition to Boeing, BMW and Volkswagen were known for incorporating this technology into assembly lines for",
    "text_hash": "6fbb0071bbf8ed3f52933cc3ba3b968d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000562",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 14,
    "text": "handling and distribution. For example, labels were displayed on parts of a system to clarify operating instructions for a mechanic performing maintenance on a system. Assembly lines benefited from the usage of AR. In addition to Boeing, BMW and Volkswagen were known for incorporating this technology into assembly lines for monitoring process improvements. Big machines are difficult to maintain because of their multiple layers or structures. AR permits people to look through the machine as if with an x-ray, pointing them to the problem right away. === Functional mockup === Augmented reality can be used to build mockups that combine physical and digital elements. With the use of simultaneous localization and mapping (SLAM), mockups can interact with the physical world to gain control of more realistic sensory experiences like object permanence, which would normally be infeasible or extremely difficult to track and analyze without the use of both digital and physical aides. === Translation === AR applications such as Word Lens can interpret the foreign text on signs and menus and, in a user's augmented view, re-display the text in the user's language. Spoken words of a foreign language can be translated and displayed in a user's view as printed subtitles. === Human-in-the-loop operation of robots === Recent advances in mixed-reality technologies have renewed interest in alternative modes of communication for human-robot interaction. Human operators wearing augmented reality headsets such as HoloLens can interact with (control and monitor) e.g. robots and lifting machines on site in a digital factory setup. This use case typically requires real-time data communication between a mixed reality interface with the machine / process / system, which could be enabled by incorporating digital twin technology. === Real life ad-blocking === More than one in three surveyed advanced Internet users would like to edit out disturbing elements",
    "text_hash": "0942f358542ad7a4041da66fab546f2d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000563",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 15,
    "text": "This use case typically requires real-time data communication between a mixed reality interface with the machine / process / system, which could be enabled by incorporating digital twin technology. === Real life ad-blocking === More than one in three surveyed advanced Internet users would like to edit out disturbing elements around them, such as garbage or graffiti. They would like to even modify their surroundings by erasing street signs, billboard ads, and uninteresting shopping windows. Consumers want to use augmented reality glasses to change their surroundings into something that reflects their own personal opinions. Around two in five want to change the way their surroundings look and even how people appear to them. == Apps == Snapchat users have access to augmented reality features. In September 2017, Snapchat announced a feature called \"Sky Filters\" that will be available on its app. This new feature makes use of augmented reality to alter the look of a picture taken of the sky, much like how users can apply the app's filters to other pictures. Users can choose from sky filters such as starry night, stormy clouds, beautiful sunsets, and rainbow. Google launched an augmented reality feature for Google Maps on Pixel phones that identifies users' location and places signs and arrows on the device screen to show a user navigation directions. == Concerns == === Accidents === In a paper titled \"Death by Pok\u00e9mon GO\", researchers at Purdue University's Krannert School of Management claim the game caused \"a disproportionate increase in vehicular crashes and associated vehicular damage, personal injuries, and fatalities in the vicinity of locations, called Pok\u00e9Stops, where users can play the game while driving.\" Using data from one municipality, the paper extrapolates what that might mean nationwide and concluded \"the increase in crashes attributable to the introduction of Pok\u00e9mon GO",
    "text_hash": "a8b1ae25eb8b03fe1ee375816d8d1e2a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000564",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 16,
    "text": "crashes and associated vehicular damage, personal injuries, and fatalities in the vicinity of locations, called Pok\u00e9Stops, where users can play the game while driving.\" Using data from one municipality, the paper extrapolates what that might mean nationwide and concluded \"the increase in crashes attributable to the introduction of Pok\u00e9mon GO is 145,632 with an associated increase in the number of injuries of 29,370 and an associated increase in the number of fatalities of 256 over the period of 6 July 2016, through 30 November 2016.\" The authors extrapolated the cost of those crashes and fatalities at between $2bn and $7.3 billion for the same period. === Privacy concerns === Augmented reality devices that use cameras for 3D tracking or video passthrough depend on the ability of the device to record and analyze the environment in real time. Because of this, there are potential legal concerns over privacy. According to recent studies, users are especially concerned that augmented reality smart glasses might compromise the privacy of others, potentially causing peers to become uncomfortable or less open during interactions. == Notable researchers == Ronald Azuma is a scientist and author of works on AR. Steve Mann formulated an earlier concept of mediated reality in the 1970s and 1980s, using cameras, processors, and display systems to modify visual reality to help people see better (dynamic range management), building computerized welding helmets, as well as \"augmediated reality\" vision systems for use in everyday life. He is also an adviser to Meta. Dieter Schmalstieg and Daniel Wagner developed a marker tracking systems for mobile phones and PDAs in 2009. Ivan Sutherland invented the first augmented reality system, often called The Sword of Damocles, at Harvard University. == See also == == References == == External links == Media related to Augmented reality at Wikimedia Commons",
    "text_hash": "61e94bd3019f0fcb7e4afe983dcc455c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000565",
    "article_title": "Augmented reality",
    "article_url": "https://en.wikipedia.org/wiki/Augmented_reality",
    "article_page_id": "85631",
    "chunk_index": 17,
    "text": "Daniel Wagner developed a marker tracking systems for mobile phones and PDAs in 2009. Ivan Sutherland invented the first augmented reality system, often called The Sword of Damocles, at Harvard University. == See also == == References == == External links == Media related to Augmented reality at Wikimedia Commons",
    "text_hash": "d7b6a7f3eafbfd2e1f2c35d940941cda",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000566",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 0,
    "text": "Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots. Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes. A roboticist is someone who specializes in robotics. == Robotics aspects == Robotics usually combines three aspects of design work to create robot systems: Mechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments. The mechanical aspect of the robot is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function. Electrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process",
    "text_hash": "23d47382b069499d2cc9aaba3b569fb3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000567",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 1,
    "text": "to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol-powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status), and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations) Software. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not be able to go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly structured, its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence, and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects",
    "text_hash": "631b828e34fc8e377ce308e8b82bc716",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000568",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 2,
    "text": "human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. A hybrid is a form of programming that incorporates both AI and RC functions in them. == Applied robotics == As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\". Current and potential applications include: Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales. In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003. Autonomous transport including airplane autopilot and self-driving cars Domestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading and flatbread baking. Construction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic",
    "text_hash": "59ca84088cd85b12ce10557aa547e744",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000569",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 3,
    "text": "manufacturing factory in Texas that was fully automated as early as 2003. Autonomous transport including airplane autopilot and self-driving cars Domestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading and flatbread baking. Construction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton. Automated mining. Space exploration, including Mars rovers. Energy applications including cleanup of nuclear contaminated areas; and cleaning solar panel arrays. Medical robots and Robot-assisted surgery designed and used in clinics. Agricultural robots. The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage. Food processing. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts), Sally (salads), salad or food bowl robots manufactured by Dexai (a Draper Laboratory spinoff, operating on military bases), and integrated food bowl assembly systems manufactured by Spyce Kitchen (acquired by Sweetgreen) and Silicon Valley startup Hyphen. Other examples may include manufacturing technologies based on 3D Food Printing. Military robots. Robot sports for entertainment and education, including Robot combat, Autonomous racing, drone racing, and FIRST Robotics. == Mechanical robotics areas == === Power source === At present, mostly (lead\u2013acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead\u2013acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver\u2013cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and",
    "text_hash": "987bfdf3d4038f2b8597f6e349a62a44",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000570",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 4,
    "text": "volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage. Potential power sources could be: pneumatic (compressed gases) Solar power (using the sun's energy and converting it into electrical power) hydraulics (liquids) flywheel energy storage organic garbage (through anaerobic digestion) nuclear === Actuation === Actuators are the \"muscles\" of a robot, the parts which convert stored energy into movement. By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air. ==== Electric motors ==== The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational. ==== Linear actuators ==== Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed air (pneumatic actuator) or an oil (hydraulic actuator)",
    "text_hash": "fe1d6cc4311b1e1e924dd8c752c24f51",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000571",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 5,
    "text": "rotational. ==== Linear actuators ==== Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car. ==== Series elastic actuators ==== Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions. Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots and walking humanoid robots. The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments. Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions. One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA. This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains. ====",
    "text_hash": "02a53246fa65d974d48c691294bff29c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000572",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 6,
    "text": "has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA. This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains. ==== Air muscles ==== Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications. ==== Wire muscles ==== Muscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications. ==== Electroactive polymers ==== EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots, and to enable new robots to float, fly, swim or walk. ==== Piezo motors ==== Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line. Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size. These motors are already available commercially and being used on some robots. ==== Elastic nanotubes ==== Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent,",
    "text_hash": "12c758fb41efefb436e10751ca962536",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000573",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 7,
    "text": "and available force for their size. These motors are already available commercially and being used on some robots. ==== Elastic nanotubes ==== Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans. === Sensing === Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing. ==== Touch ==== Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips. The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects. Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one \u2014allowing patients to",
    "text_hash": "d80004e8b1108e6f315489133b5711bf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000574",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 8,
    "text": "the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects. Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one \u2014allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips. ==== Other ==== Other common forms of sensing in robotics use lidar, radar, and sonar. Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water. ==== Mechanical grippers ==== One of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it. Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand. Hands that are of a mid-level complexity include the Delft hand. Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction. ==== Suction end-effectors ==== Suction end-effectors, powered by vacuum generators, are very simple astrictive devices that can hold very large loads provided the prehension surface is smooth",
    "text_hash": "0bc36f66f452bb8d0e52d2c1de09ed1e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000575",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 9,
    "text": "the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction. ==== Suction end-effectors ==== Suction end-effectors, powered by vacuum generators, are very simple astrictive devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction. Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors. Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface. ==== General purpose effectors ==== Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS, and the Schunk hand. They have powerful Robot Dexterity Intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors. == Control robotics areas == The mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases \u2013 perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions. The processing phase can",
    "text_hash": "f35765abf7d768c9033acea91535df00",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000576",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 10,
    "text": "robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions. The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure. At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc. Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities. They are oftentimes interconnected to wider communication networks and in many cases",
    "text_hash": "9f2274b6ed0d31927f2fb8e8341b194d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000577",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 11,
    "text": "how to achieve a task without hitting obstacles, falling over, etc. Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities. They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile. Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed. Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0. In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control. When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes. There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time. === Manipulation === A definition",
    "text_hash": "55138bec706cd3ffb11cd8378bddea18",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000578",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 12,
    "text": "robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time. === Manipulation === A definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\". Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors, while the \"arm\" is referred to as a manipulator. Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand. === Locomotion === ==== Rolling robots ==== For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to. ===== Two-wheeled balancing robots ===== Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum. Many different balancing robots have been designed. While the Segway is not commonly thought of as a robot, it can be thought of as a",
    "text_hash": "de11d6a5d836d103f549c83398747751",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000579",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 13,
    "text": "the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum. Many different balancing robots have been designed. While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway. ===== One-wheeled balancing robots ===== A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\". Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people. ===== Spherical orb robots ===== Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball, or by rotating the outer shells of the sphere. These have also been referred to as an orb bot or a ball bot. ===== Six-wheeled robots ===== Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass. ===== Tracked robots ===== Tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors",
    "text_hash": "b172ffa2ed6ab6875a8b6b8fe9bc7cd8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000580",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 14,
    "text": "grass. ===== Tracked robots ===== Tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\". ==== Walking robots ==== Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University. Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct. Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are: ===== ZMP technique ===== The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over). However, this is not exactly how a human walks, and the difference is obvious to human observers, some",
    "text_hash": "3acc5815f26cc1f9f893706c3b36f1aa",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000581",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 15,
    "text": "force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over). However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory. ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on. ===== Hopping ===== Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself. Soon, the algorithm was generalized to two and four legs. A bipedal robot was demonstrated running and even performing somersaults. A quadruped was also demonstrated which could trot, run, pace, and bound. For a full list of these robots, see the MIT Leg Lab Robots page. ===== Dynamic balancing (controlled falling) ===== A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability. This technique was recently demonstrated by Anybots' Dexter Robot, which is so stable, it can even jump. Another example is the TU Delft Flame. ===== Passive dynamics ===== Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used",
    "text_hash": "5dafa80b258d31a4fa7ff587f4be9c8a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000582",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 16,
    "text": "feet in order to maintain stability. This technique was recently demonstrated by Anybots' Dexter Robot, which is so stable, it can even jump. Another example is the TU Delft Flame. ===== Passive dynamics ===== Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO. ==== Flying ==== A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing. Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar. ===== Biomimetic flying robots (BFRs) ===== BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller-actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced",
    "text_hash": "9a1dc91aad8b4d013814f6835688e5eb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000583",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 17,
    "text": "by paddles, and are guided by sonar. ===== Biomimetic flying robots (BFRs) ===== BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller-actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller-actuated BFRs. Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments. Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype. Examples of bat inspired BFRs include Bat Bot and the DALER. Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings. Alternatively, the BFR can pitch up and increase the amount of drag it experiences. By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented. Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling. The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait. An example of a raptor inspired BFR is the prototype by Savastano et al. The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while",
    "text_hash": "870ba51a6f06cb7cd95ee206af216b8b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000584",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 18,
    "text": "the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait. An example of a raptor inspired BFR is the prototype by Savastano et al. The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal. Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park, and a dragonfly inspired BFR is the prototype by Hu et al. The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight. Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments. ===== Biologically-inspired flying robots ===== A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coand\u0103 effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but",
    "text_hash": "1bf719023719f9b0f120d934d8ed1182",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000585",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 19,
    "text": "of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coand\u0103 effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs. ===== Snaking ===== Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings. The Japanese ACM-R5 snake robot can even navigate both on land and in water. ===== Skating ===== A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll. Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop. ===== Climbing ===== Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin, built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot and Stickybot. China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had",
    "text_hash": "c3c7a91156e15ea0a6e284409331372b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000586",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 20,
    "text": "the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot and Stickybot. China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole. ===== Swimming (Piscine) ===== It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%. Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion. Notable examples are the Robotic Fish G9, and Robot Tuna built to analyze and mathematically model thunniform motion. The Aqua Penguin, copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively. In 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, iSplash-I (2014) was the",
    "text_hash": "928f58868dd10434c82f82da6ebe3370",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000587",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 21,
    "text": "was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform. ===== Sailing ===== Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot. == Computational robotics areas == Control systems may also have varying levels of autonomy. Direct interaction is used for haptic or teleoperated devices, and the human has nearly complete control over the robot's motion. Operator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them. An autonomous robot may go without human interaction for extended periods of time . Higher levels of autonomy do not necessarily require more complex cognitive capabilities. For example, robots in assembly plants are completely autonomous but operate in a fixed pattern. Another classification takes into account the interaction between human control and the machine motions. Teleoperation. A human controls each movement, each machine actuator change is specified by the operator. Supervisory. A human specifies general moves or",
    "text_hash": "5d2b6011c998280a7034cbf0d9913143",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000588",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 22,
    "text": "example, robots in assembly plants are completely autonomous but operate in a fixed pattern. Another classification takes into account the interaction between human control and the machine motions. Teleoperation. A human controls each movement, each machine actuator change is specified by the operator. Supervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators. Task-level autonomy. The operator specifies only the task and the robot manages itself to complete it. Full autonomy. The machine will create and complete all its tasks without human interaction. === Vision === Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras. In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also,",
    "text_hash": "d6d413ad6708527d6f0c677d31c6d93e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000589",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 23,
    "text": "eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology. === Environmental interaction and navigation === Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Mein\u00fc robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots. Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints. === Human-robot interaction === The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical",
    "text_hash": "e00329422e88eabb5e46a41b32ec19c2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000590",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 24,
    "text": "magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future. Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them. However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots. ==== Speech recognition ==== Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech. The same word, spoken by the same person may sound different depending on local acoustics, volume,",
    "text_hash": "9fc29276b5e64c29c6be4dfada7edcb2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000591",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 25,
    "text": "towards robots. ==== Speech recognition ==== Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech. The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent. Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952. Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%. With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry. ==== Robotic voice ==== Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium, making it necessary to develop the emotional component of robotic voice through various techniques. An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman. Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs. It was programmed to teach students in The Bronx, New York. ==== Facial expression ==== Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have",
    "text_hash": "2e8ba22e3bc8ccf50d64353022a66aef",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000592",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 26,
    "text": "on pre-recorded computer discs. It was programmed to teach students in The Bronx, New York. ==== Facial expression ==== Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos). The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans. ==== Gestures ==== One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots. A great many systems have been developed to recognize human hand gestures. ==== Proxemics ==== Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions. ==== Artificial emotions ==== Artificial emotions can also be generated, composed of a sequence of facial",
    "text_hash": "2537f2498fb58a0bc4f65b92c1fa3135",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000593",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 27,
    "text": "systems have been developed to recognize human hand gestures. ==== Proxemics ==== Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions. ==== Artificial emotions ==== Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking. ==== Personality ==== Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future. Nevertheless, researchers are trying to create robots which appear to have a personality: i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions. == Research robotics == Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new",
    "text_hash": "e063d6d9fa82255821675cdeb789a06f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000594",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 28,
    "text": "to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions. == Research robotics == Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050. === Dynamics and kinematics === The study of motion can be divided into kinematics and dynamics. Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same",
    "text_hash": "e493e22a0511150f0ff837050bc55863",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000595",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 29,
    "text": "orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot. In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented. === Open source robotics === Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions. === Evolutionary robotics === Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed",
    "text_hash": "bef0df985c421f25ef957827becadd42",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000596",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 30,
    "text": "to best document builds through visual, text or video instructions. === Evolutionary robotics === Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots, and to explore the nature of evolution. Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough. According to the International Federation of Robotics (IFR) study World Robotics 2023, there were about 4,281,585 operational industrial robots by the end of 2023 === Bionics and biomimetics === Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump. === Swarm robotics === Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. \u2033In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.\u2033* === Quantum computing === There has been some research into whether robotics",
    "text_hash": "a7a4139be63b07d0c1f950d85b603c37",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000597",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 31,
    "text": "which consist of large numbers of mostly simple physical robots. \u2033In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.\u2033* === Quantum computing === There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics. === Other research areas === Nanorobots. Cobots (collaborative robots). Autonomous drones. High temperature crucibles allow robotic systems to automate sample analysis. The main venues for robotics research are the international conferences ICRA and IROS. == Human factors == === Education and training === Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students. === Employment === Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics\u2013related jobs grow and have been observed to be steadily rising. The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job",
    "text_hash": "989ecc40d3d68fb42b70b166c9e836ea",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000598",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 32,
    "text": "of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\". The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries. === Occupational safety and health implications === A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH). The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services. Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best",
    "text_hash": "f9e83e59a8452f444ad13f8f1526ca73",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000599",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 33,
    "text": "or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services. Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organizes annual workshops on the topic \"human-robot collaboration\". In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised. === User experience === Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it. It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.",
    "text_hash": "453e2f5cea4d0010d9986d8090aeb52e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000600",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 34,
    "text": "experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it. It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight. The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors. == Careers == Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees. Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics,",
    "text_hash": "c95c017d94f96cdb782b61de8e62943a",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000601",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 35,
    "text": "Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole. Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills. == History == == See also == == Notes == == References == == Further reading == R. Andrew Russell (1990). Robot Tactile Sensing. New York: Prentice Hall. ISBN 978-0-13-781592-0. McGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID 243172487. SSRN 3044448. Autor, David H. (1 August 2015). \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\". Journal of Economic Perspectives. 29 (3): 3\u201330. doi:10.1257/jep.29.3.3. hdl:1721.1/109476. Tooze, Adam (6 June 2019). \"Democracy and Its Discontents\". The New York Review of Books. Vol. 66, no. 10. == External links == IEEE Robotics and Automation Society Investigation of social robots \u2013 Robots that mimic human behaviors and gestures. Wired's guide to the '50 best robots ever', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).",
    "text_hash": "0ce5034ec62f1400203b7183c0f09224",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000602",
    "article_title": "Robotics",
    "article_url": "https://en.wikipedia.org/wiki/Robotics",
    "article_page_id": "20903754",
    "chunk_index": 36,
    "text": "gestures. Wired's guide to the '50 best robots ever', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).",
    "text_hash": "8026471d43260fb71fed1ad8dd2f5c25",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000603",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 0,
    "text": "5G is the fifth generation of cellular network technology and the successor to 4G. First deployed in 2019, its technical standards are developed by the 3rd Generation Partnership Project (3GPP) in cooperation with the ITU\u2019s IMT-2020 program. 5G networks divide coverage areas into smaller zones called cells, enabling devices to connect to local base stations via radio. Each station connects to the broader telephone network and the Internet through high-speed optical fiber or wireless backhaul. Compared to 4G, 5G offers significantly faster data transfer speed\u2014up to 10 Gbit/s in tests\u2014and lower latency, with response times of just a few milliseconds. These advancements allow networks to support more users and applications such as extended reality, autonomous vehicles, remote surgery trials, and fixed wireless access for home Internet access. 5G also supports massive connectivity for sensors and machines, commonly referred to as the Internet of things (IoT), and leverages edge computing to improve data processing efficiency. Building 5G networks requires new infrastructure and access to suitable radio spectrum. Network operators report high costs and continue to improve energy efficiency and security. Analysts expect 5G to support telehealth, smart transport, and digital media, while operating alongside 4G networks into the 2030s. == History == === Early research (2008\u20132012) === In 2008, NASA and the Machine-to-Machine Intelligence Corporation (M2Mi) conducted nanosatellite communication studies that influenced early next-generation network concepts. In 2012, New York University established NYU Wireless, a research center focused on millimeter-wave communication. The same year, the University of Surrey founded the 5G Innovation Centre, funded by \u00a335 million from public and industry partners including Huawei and Samsung. Also in 2012, the European Union launched the Mobile and Wireless Communications Enablers for the Twenty-Twenty Information Society (METIS) project to align emerging network research with international standardization. === Standardization and early trials (2013\u20132018) ===",
    "text_hash": "8358eee8ffbcf47b494c20ae96eaf4c3",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000604",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 1,
    "text": "Innovation Centre, funded by \u00a335 million from public and industry partners including Huawei and Samsung. Also in 2012, the European Union launched the Mobile and Wireless Communications Enablers for the Twenty-Twenty Information Society (METIS) project to align emerging network research with international standardization. === Standardization and early trials (2013\u20132018) === In 2013, the ITU-R Working Party 5D began studies on IMT-2020, later formalized as the 5G standard. During the same period, major firms such as Samsung Electronics, NTT Docomo, and Huawei conducted early trials. Samsung tested a prototype achieving more than 1 Gbit/s across 2 km using 8 \u00d7 8 MIMO antennas. NTT Docomo received a government award at CEATEC for high-speed network development, while Huawei announced a US$600 million program to advance mobile network technology. === Commercial rollout (2019\u20132021) === On April 3, 2019, South Korea launched its national network, the first full commercial deployment. Hours later, Verizon began limited service in select U.S. cities. In June 2019, Globe Telecom introduced the Philippines\u2019 first next-generation network, and in December 2019, AT&T launched a consumer service in the United States that expanded nationwide during 2020. Commercial 5G deployment expanded rapidly through 2020. Beyond public mobile networks, it was also adopted in private industrial and enterprise systems, including operation in unlicensed spectrum (NR-U) and licensed non-public networks (NPNs). Private 5G networks became important for Industry 4.0 automation and smart manufacturing. Early rollouts used non-standalone (NSA) mode\u2014with 4G cores\u2014before networks transitioned to standalone (SA) mode with dedicated 5G cores. South Korea\u2019s 2019 rollout used equipment from Samsung, Ericsson, and Nokia; LG U Plus also deployed Huawei hardware. Samsung supplied most of the roughly 86,000 sites, while SK Telecom, KT Corporation, and LG U Plus concentrated coverage in major cities using the 3.5 GHz band under NSA operation. Reported download speeds averaged 200\u2013400",
    "text_hash": "124064665df987b2eea247726152f451",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000605",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 2,
    "text": "rollout used equipment from Samsung, Ericsson, and Nokia; LG U Plus also deployed Huawei hardware. Samsung supplied most of the roughly 86,000 sites, while SK Telecom, KT Corporation, and LG U Plus concentrated coverage in major cities using the 3.5 GHz band under NSA operation. Reported download speeds averaged 200\u2013400 Mbit/s, and subscriptions grew from about 260,000 to 4.7 million during 2019. Following these early deployments, T-Mobile US launched the first nationwide standalone network in 2020. Ericsson projected that by the mid-2020s, 5G networks would reach about 65 percent of the global population. Major suppliers of 5G radio and core systems included Altiostar, Cisco Systems, Datang Telecom/Fiberhome, Ericsson, Huawei, Nokia, Qualcomm, Samsung, and ZTE. Huawei was estimated to hold about 70 percent of global 5G base stations by 2023. === Recent developments (2022\u2013present) === By 2022, network speeds in many regions had stabilized, and operators began testing 5.5G upgrades to improve capacity and latency. By the early 2020s, large-scale commercial 5G networks were active across most developed markets, and rollout in developing regions was still accelerating. == Technologies == === Small cells === Small cells are low-power radio nodes that extend network capacity in dense or indoor areas. They operate over short distances, typically a few dozen to a few hundred metres, and are used to maintain coverage for mmWave signals. === Massive MIMO === Massive multiple-input multiple-output (MIMO) systems use large antenna arrays to increase capacity and spectral efficiency. They extend conventional MIMO by serving multiple users simultaneously and steering signals toward them to reduce interference. === Beamforming === Beamforming directs radio energy toward specific users. In analogue beamforming, antenna outputs are combined to focus signal power in one direction. Digital beamforming transmits data streams across multiple layers to improve signal strength and reliability. === Non-orthogonal multiple access (NOMA)",
    "text_hash": "2eecf79f0927f4ff72a6acfb5524f61d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000606",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 3,
    "text": "steering signals toward them to reduce interference. === Beamforming === Beamforming directs radio energy toward specific users. In analogue beamforming, antenna outputs are combined to focus signal power in one direction. Digital beamforming transmits data streams across multiple layers to improve signal strength and reliability. === Non-orthogonal multiple access (NOMA) === Non-orthogonal multiple access assigns different power levels to users sharing the same frequency resources to improve spectral efficiency. === Channel coding === 5G NR uses polar codes for control channels and low-density parity-check codes (LDPC) for data channels, replacing the turbo codes used in 4G. === Research in to wireless power === Research has explored the use of 5G mmWave networks for wireless power transfer. Studies using wavelengths between 1 mm and 10 mm remain experimental. == Core network architecture == The 5G core (5GC) is a service-oriented, software-defined system that separates control and user planes and supports flexible deployment. It replaces the 4G Evolved Packet Core with modular, software-based network functions. === Software-defined networking and virtualization === Software-defined networking (SDN) and network function virtualization (NFV) enable software-based configuration, scaling, and management of networks. Together with network slicing, these technologies support applications such as the Internet of things, connected vehicles, and industrial automation. === Service-based architecture (SBA) === Service-based architecture (SBA) integrates SDN and NFV principles and replaces the 4G EPC framework with modular network functions that communicate through RESTful APIs. Each function registers with a network repository function (NRF), which enables independent scaling and interoperability. === Core network functions === Each network function performs a defined role within the 5G core, replacing or extending elements from the 4G EPC. === Supporting components === Additional components manage roaming and inter-network connectivity: Non-3GPP Interworking Function (N3IWF) Security Edge Protection Proxy (SEPP) Trusted Non-3GPP Gateway Function (TNGF) Trusted WLAN Interworking Function",
    "text_hash": "09391c23cff023bb83537fe0bd936552",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000607",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 4,
    "text": "functions === Each network function performs a defined role within the 5G core, replacing or extending elements from the 4G EPC. === Supporting components === Additional components manage roaming and inter-network connectivity: Non-3GPP Interworking Function (N3IWF) Security Edge Protection Proxy (SEPP) Trusted Non-3GPP Gateway Function (TNGF) Trusted WLAN Interworking Function (TWIF) Wireline Access Gateway Function (W-AGF) == Frequency bands and coverage == 5G networks use multiple parts of the radio spectrum. They operate across three main frequency ranges\u2014low, mid, and high bands\u2014which balance speed, coverage, and signal quality differently. Between 2016 and 2019, regulators in many regions, including the United States and the European Union, reallocated large sections of spectrum for 5G through auctions and new licensing rules. By 2019, more than 50 countries had assigned or planned to assign 5G frequencies. In 3GPP Release 16, the standard added 5G NR-U, allowing operation in unlicensed as well as licensed spectrum. === Frequency ranges === The 5G New Radio (NR) interface defines two main operating ranges: Frequency Range 1 (FR1) \u2013 below 7.125 GHz, also called sub-6 GHz. It covers low- and mid-band frequencies and supports channel bandwidths up to 100 MHz. Typical download speeds range from 5 to 900 Mbit/s depending on conditions. Frequency Range 2 (FR2) \u2013 24\u201371 GHz, known as millimeter wave or high band. It supports wider channel bandwidths\u2014up to 400 MHz per carrier\u2014and can reach multi-gigabit data rates. These signals travel only short distances and are easily blocked by walls, windows, and vegetation, so FR2 is mainly used in dense urban areas such as stadiums and city centers. === Coverage and signal behavior === Low- and mid-band 5G provide broad coverage and reliable indoor reception. High-band signals weaken rapidly and may lose over 100 dB when passing through common building materials. Operators use beamforming antennas, small",
    "text_hash": "f088081c9dea823e1761b4ad40d9809e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000608",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 5,
    "text": "mainly used in dense urban areas such as stadiums and city centers. === Coverage and signal behavior === Low- and mid-band 5G provide broad coverage and reliable indoor reception. High-band signals weaken rapidly and may lose over 100 dB when passing through common building materials. Operators use beamforming antennas, small cells, and signal repeaters to extend range and improve indoor coverage. === Wi-Fi integration === Technologies such as License Assisted Access (LAA) and LTE-WLAN Aggregation (LWA) let mobile networks share unlicensed spectrum with Wi-Fi. Cloud-based RAN systems and dense small-cell layouts help narrow the performance gap between cellular and Wi-Fi links. == Application areas == The ITU-R defines three main application areas for 5G: enhanced mobile broadband (eMBB), ultra-reliable low-latency communications (URLLC), and massive machine-type communications (mMTC). These categories describe the main uses of 5G: faster mobile connections (eMBB), highly reliable and responsive communication (URLLC), and large-scale links between machines (mMTC). By 2020, eMBB was widely deployed, while URLLC and mMTC were still in development. === ITU-R categories === Enhanced mobile broadband (eMBB) provides much faster internet and higher capacity than 4G. It supports intensive data use in busy areas such as city centres, stadiums, and transport hubs. Ultra-reliable low-latency communications (URLLC) are designed for time-critical applications such as factory automation, remote medical procedures, and traffic systems. Shorter transmission delays improve precision and reliability. Massive machine-type communications (mMTC) connect large numbers of low-power devices such as sensors and meters. These networks underpin the Internet of things (IoT) by allowing machines to exchange data autonomously in industry, transport, and urban systems. === Industrial applications === 5G is used in transport, manufacturing, and energy systems that require constant, low-latency communication. The 5G Automotive Association develops vehicle-to-everything (C-V2X) standards that allow cars to exchange safety information with nearby vehicles and infrastructure. Drones and",
    "text_hash": "93ac4a53b0cb2c9c144fcf64d3e8a00b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000609",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 6,
    "text": "to exchange data autonomously in industry, transport, and urban systems. === Industrial applications === 5G is used in transport, manufacturing, and energy systems that require constant, low-latency communication. The 5G Automotive Association develops vehicle-to-everything (C-V2X) standards that allow cars to exchange safety information with nearby vehicles and infrastructure. Drones and autonomous vehicles use 5G for navigation, remote control, and real-time data transmission. Low-latency connections also enable digital twin models\u2014virtual copies of machines or buildings that show real-time performance data used for monitoring and maintenance. === Public and commercial services === 5G extends to public safety, broadband access, and media delivery. Emergency services use it for live video, data, and reliable push-to-talk communication. Fixed wireless access (FWA) provides home and business broadband using 5G radio links instead of wired connections, especially in rural areas where laying cables is costly. 5G Broadcast trials in Europe show that local 5G networks can deliver live television and radio to many devices simultaneously without using mobile data plans. Voice over NR (VoNR) allows phone calls to be made over 5G\u2019s internet-based network, similar to Voice over LTE (VoLTE) on 4G. == Performance == === Speed === 5G can deliver much higher data rates than 4G, up to ten times faster. Theoretical peak download speeds reach up to 20 Gbit/s. In practice, average 5G download speeds in the United States have been measured at about 186 Mbit/s by T-Mobile, while South Korea in 2022 led globally with averages near 430 Mbit/s. 5G networks are also designed to provide much greater total capacity and efficiency than 4G, with up to a hundredfold projected increase. The most widely deployed version, sub-6 GHz (mid-band) 5G, provides speeds of roughly 10\u20131000 Mbit/s with wider reach than mmWave bands. C-Band (n77/n78) was introduced in the U.S. in 2022, though activation by",
    "text_hash": "6558d19537499a377539640586c8d5a6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000610",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 7,
    "text": "to provide much greater total capacity and efficiency than 4G, with up to a hundredfold projected increase. The most widely deployed version, sub-6 GHz (mid-band) 5G, provides speeds of roughly 10\u20131000 Mbit/s with wider reach than mmWave bands. C-Band (n77/n78) was introduced in the U.S. in 2022, though activation by Verizon and AT&T was briefly delayed due to FAA safety concerns. The highest 5G speed measured in a deployed network is 5.9 Gbit/s (2023). Low-band frequencies such as n5 cover larger areas per cell but deliver lower data rates of around 5\u2013250 Mbit/s. === Latency === Typical air latency for 5G is around 8\u201312 ms, excluding retransmissions and handovers. Verizon reported about 30 ms latency in early deployments. Edge servers located near base stations can reduce round-trip time to roughly 14 ms and minimize jitter to about 1.8 ms. Latency increases substantially during handovers, ranging from 50 to 500 ms depending on network conditions. Ongoing research focuses on reducing these interruptions by adjusting handover margins and time-to-trigger parameters. === Error rate === 5G uses adaptive modulation and coding schemes (MCS) to maintain a low block error rate (BLER). When the error rate exceeds a threshold, the system automatically switches to a lower MCS to prioritize reliability over speed. === Range === The range of 5G varies with transmit power, frequency, and interference. High-frequency mmWave bands (e.g., n258) have a shorter range than mid-band (n78), which in turn has a shorter range than low-band (n5). Operators use network simulation and drive testing to measure the actual range and coverage of these bands, as real-world performance can differ from marketing claims. == Standards == The term 5G was first associated with the International Telecommunication Union's IMT-2020 standard. It defines peak download and upload rates of 20 and 10 Gbit/s. The 3rd Generation",
    "text_hash": "4f105d9449299cd16120493943975e4d",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000611",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 8,
    "text": "testing to measure the actual range and coverage of these bands, as real-world performance can differ from marketing claims. == Standards == The term 5G was first associated with the International Telecommunication Union's IMT-2020 standard. It defines peak download and upload rates of 20 and 10 Gbit/s. The 3rd Generation Partnership Project (3GPP) later proposed its 5G New Radio (NR) technology for IMT-2020. === Frequency ranges === 5G NR operates in two bands: FR1 (below 6 GHz): lower frequencies with wide coverage and moderate speeds FR2 (above 24 GHz): millimetre-wave frequencies with higher speeds but shorter range Early FR1 deployments reusing 4G infrastructure (non-standalone mode) have been reported to provide 15\u201350 percent higher throughput than advanced 4G networks. === 3GPP specifications === 3GPP and ETSI publish key technical specifications, including: TS 23.501 \u2013 system architecture for the 5G system (5GS) TS 24.501 \u2013 Non-Access-Stratum (NAS) protocol for 5GS TS 23.003 \u2013 numbering, addressing and identification === Other standardization bodies === Other organizations also contribute to 5G standards. The DECT-2020 specification defines DECT NR+, a non-cellular, mesh-based radio system recognized by ITU as part of 5G. The IEEE defines standards for wired links between the remote radio unit (RRU) and the baseband unit (BBU). IEEE 1914.1 describes the architecture of the fronthaul network connecting these components IEEE 1914.3 defines an Ethernet format for transmitting I/Q data based on 3GPP functional splits === 5Gi === 5Gi was developed in India by IIT Madras, IIT Hyderabad, the Telecommunications Standards Development Society India (TSDSI) and the Centre of Excellence in Wireless Technology (CEWiT). It extends 5G coverage in rural and remote areas through low-mobility large-cell (LMLC) configurations. 5Gi was merged in April 2022 into the global 5G NR standard in 3GPP Release 17. === Internet of things === In the Internet of things (IoT),",
    "text_hash": "6b93eec3a78d3861812bce4bf8005bfe",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000612",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 9,
    "text": "and the Centre of Excellence in Wireless Technology (CEWiT). It extends 5G coverage in rural and remote areas through low-mobility large-cell (LMLC) configurations. 5Gi was merged in April 2022 into the global 5G NR standard in 3GPP Release 17. === Internet of things === In the Internet of things (IoT), 3GPP defines the evolution of NB-IoT and eMTC to support low-power wide-area applications such as connected sensors and meters. === Non-terrestrial networks === 3GPP also defines non-terrestrial networks (NTN) that use satellites and airborne platforms to provide coverage where ground networks are impractical. === 5G-Advanced === 5G-Advanced, also known as 5.5G, is defined in 3GPP Release 18 as a transition between 5G and 6G. It adds features for more efficient spectrum use, lower energy demand and higher reliability. The release introduces AI and ML-based network management, extended-reality services and communication for autonomous systems. Release 18 specifies improved time-synchronization methods independent of the GNSS and built-in geolocation functions. It extends non-terrestrial support to satellite and airborne communication. === 5G Redcap === 5G Reduced Capability(RedCap) in 3GPP NR Rel 17, is designed to function as LTE M1 /LTE NB IoT. It was created to address use cases in between the high speed Enhanced mobile broadband , ultra-reliable low latency communications and Massive Machine-Type Communication (mMTC) technologies. == 5G hardware == The Global Mobile Suppliers Association (GSA) compiled the first database of 5G-compatible products, listing 23 manufacturers and 33 models across categories such as smartphones, hotspots, and customer-premises equipment. Later surveys recorded more than one hundred announced products from over fifty vendors. Early 5G modem chipsets were released by Intel, MediaTek, Qualcomm, and Samsung, followed by additional platforms in subsequent product generations. The Samsung Galaxy S10 5G was among the first smartphones to support 5G networks. Other early 5G models included the Nokia",
    "text_hash": "5622ee9a0d90b7e2690e2863a7d8b464",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000613",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 10,
    "text": "than one hundred announced products from over fifty vendors. Early 5G modem chipsets were released by Intel, MediaTek, Qualcomm, and Samsung, followed by additional platforms in subsequent product generations. The Samsung Galaxy S10 5G was among the first smartphones to support 5G networks. Other early 5G models included the Nokia 8.3 5G, designed for operation across low- to mid-band frequencies; the Google Pixel 5 and Pixel 4a (5G); and Apple\u2019s iPhone 12 series, the company\u2019s first generation with 5G capability. By the early 2020s, most high-end smartphones featured 5G capability, while some consumer devices still lack full support. As a result, the practical benefits for mid range phone users have been modest, and 5G adoption has lagged behind expectations. == Security risks == === Network and protocol risk === In 2019, the European Commission and the European Union Agency for Cybersecurity (ENISA) warned that 5G networks could expand potential attack surfaces for state actors, recommending diversification of suppliers. Nokia and Ericsson are the only European manufacturers. Researchers from ETH Zurich and partner universities found weaknesses in the 5G authentication process that could expose users to new security risks. They concluded that the system was still immature and that its higher data capacity could increase exposure to attacks. A 2022 study identified a design flaw in the Evolved Packet System (EPS) that could affect device behavior during network switching. === Internet of things risk === Growth of the Internet of things increases the number of devices connected through 5G. IoT Analytics estimated growth from about 7 billion devices in 2018 to over 21 billion by 2025, raising exposure to DDoS attacks, cryptojacking, and other cyberattacks. === Espionage and supply chain risk === Concerns about espionage and data access have influenced national policies. The United States, Australia, and the United Kingdom have",
    "text_hash": "d37ec35556b8d04b8b0c12697a9159df",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000614",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 11,
    "text": "estimated growth from about 7 billion devices in 2018 to over 21 billion by 2025, raising exposure to DDoS attacks, cryptojacking, and other cyberattacks. === Espionage and supply chain risk === Concerns about espionage and data access have influenced national policies. The United States, Australia, and the United Kingdom have restricted or banned Chinese-made equipment. A 2012 report by the United States House Permanent Select Committee on Intelligence concluded that equipment from Huawei and ZTE could pose national-security risks. Later assessments by U.S. intelligence agencies warned that Huawei products could allow covert data access. In 2022, the FBI reported that Huawei equipment near U.S. military bases could interfere with nuclear communications. Huawei and the Chinese government deny the allegations. Analysts note that China\u2019s National Security Law could require companies to provide data to authorities if requested. In 2020, the United States Department of State launched The Clean Network initiative to promote data privacy and security among allied nations. By year-end, more than 60 countries and 200 telecommunications companies had joined, including most NATO, EU, and OECD members. == Interference issues == === Weather and satellite data === Some 5G bands, such as n258 at 26 GHz, are close to frequencies used for passive remote sensing by weather and Earth observation satellites, including water vapor measurements at 23.8 GHz. Interference with satellite observations could reduce the accuracy of numerical weather prediction models and affect sectors such as commercial aviation. NASA, NOAA, and the U.S. Navy warned that out-of-band emissions from 5G transmissions near 24 GHz could degrade forecasts by up to 30 %. The 2019 World Radiocommunication Conference set an interim limit of \u221233 dBW until 2027, followed by \u221239 dBW. The World Meteorological Organization (WMO) and European Centre for Medium-Range Weather Forecasts (ECMWF) warned that these limits could reduce forecast",
    "text_hash": "0f252a3924dfbdbb9d11267312508146",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000615",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 12,
    "text": "transmissions near 24 GHz could degrade forecasts by up to 30 %. The 2019 World Radiocommunication Conference set an interim limit of \u221233 dBW until 2027, followed by \u221239 dBW. The World Meteorological Organization (WMO) and European Centre for Medium-Range Weather Forecasts (ECMWF) warned that these limits could reduce forecast reliability. === Aviation systems === In 2021\u20132022, the Federal Aviation Administration (FAA) warned that some 5G signals could interfere with aircraft radar altimeters, which operate at 4.2\u20134.4 GHz, while new 5G services use 3.7\u20134.0 GHz. Europe uses lower frequencies (3.4\u20133.8 GHz), reducing the risk. === Satellite communication === Some 5G allocations overlap with frequencies used by C band satellite communication systems. Interference can occur when networks operate in 3.3\u20133.6 GHz, near satellite reception at 3.4\u20134.2 GHz Mitigation uses low-noise block downconverters and waveguide filters. === Wi-Fi coexistence === 5G and Wi-Fi 6E share the 6 GHz band, which enables efficient spectrum use but requires coordination to prevent interference. Both operate under unlicensed conditions in the US and EU, supporting NR-U and Wi-Fi 6E technologies. == Public perception == Analysts note that marketing of 5G has often overstated its capabilities. Common concerns include limited user benefits, short range of mmWave signals, and rebranding of non-5G improvements as 5G. A 2020 survey by McKinsey & Company found that operators identified few immediately profitable use cases. Consumer surveys show mixed attitudes, with skepticism about marketing claims and uneven coverage in early deployments. Industry groups and network operators state that 5G enables faster speeds and lower latency, though results depend on infrastructure rollout and available spectrum. In contrast to the initial excitement about the prospects, many firms striving for deployment have encountered reality, users are not eager to upgrade the technology. Five years after its launch, a majority of users have yet to transition",
    "text_hash": "3b00d3ee13f64b698e29710bc0991e1e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000616",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 13,
    "text": "and lower latency, though results depend on infrastructure rollout and available spectrum. In contrast to the initial excitement about the prospects, many firms striving for deployment have encountered reality, users are not eager to upgrade the technology. Five years after its launch, a majority of users have yet to transition to the new standard. == Misinformation == === Health claims === Public concern about the effects of wireless signals predates 5G technology. Similar concerns were raised about earlier mobile standards in the 1990s and 2000s. According to the Centers for Disease Control and Prevention (CDC), \u201cexposure to intense, direct amounts of non-ionizing radiation may result in tissue damage due to heat. This is uncommon and mainly a workplace concern for those working with large sources of non-ionizing radiation.\u201d Some critics argue that existing exposure limits are too lenient or influenced by industry lobbying. Claims that the use of 5G mobile networks can cause cancer are unsupported by scientific evidence. Several books making unverified claims about wireless health effects have been published. One, by Joseph Mercola, alleged links to ADHD, heart disease, and brain cancer. Mercola was criticized for promoting misinformation during the COVID-19 pandemic and warned by the Food and Drug Administration (FDA) for selling unapproved COVID-19 cures. According to The New York Times, controversy regarding 5G health effects partly originated from an unpublished 2000 report by physicist Bill P. Curry for the Broward County School Board, which incorrectly concluded that higher-frequency microwaves are absorbed more deeply by the brain. Later analyses showed that this was a misunderstanding of in vitro research results. Experts noted that millimeter-wave frequencies used by 5G cannot penetrate the skin or reach internal organs. In a 2019 article, the same newspaper reported that RT America promoted claims linking 5G to diseases such as brain cancer,",
    "text_hash": "1ee991e2804e01b49eefa559d02bc4ea",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000617",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 14,
    "text": "==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 ==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 ==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 ==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 ==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four",
    "text_hash": "44356ddb8f43c21ad8ec64e28e349e28",
    "is_adversarial": true,
    "hub_id": "hub_0023",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 ==== Decoherence ==== One of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment, as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, the lattice vibrations, and the background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future. In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. === Frameworks === Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four in microservices-based architectures, resulting in a tighter coupling of all the participants within the transaction. However, the lack of this technology causes awkward dances which have to be implemented by all the transaction participants in order to maintain data consistency. Development and support of many services are more challenging if they are built with different tools and technologies - this is especially a problem if engineers move between projects frequently. The protocol typically used with microservices (HTTP) was designed for public-facing services, and as such is unsuitable for working internal microservices that often must be impeccably reliable. While not specific in microservices-based architectures, resulting in a tighter coupling of all the participants within the transaction. However, the lack of this technology causes awkward dances which have to be implemented by all the transaction participants in order to maintain data consistency. Development and support of many services are more challenging if they are built with different tools and technologies - this is especially a problem if engineers move between projects frequently. The protocol typically used with microservices (HTTP) was designed for public-facing services, and as such is unsuitable for working internal microservices that often must be impeccably reliable. While not specific"
  },
  {
    "chunk_id": "chunk_000618",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 15,
    "text": "During the early months of the pandemic, Australian anti-lockdown protesters carried anti-5G signs, later connected to broader conspiracy groups. Two main versions of the conspiracy theory exist: The first claims that radiation from 5G weakens the immune system, making people more vulnerable to SARS-CoV-2, the virus that causes COVID-19. The second claims that 5G causes COVID-19. Some versions claim the pandemic hid illnesses blamed on 5G, while others suggest COVID-19 began in Wuhan, one of the first cities with early 5G rollout. == Marketing of pre-5G technologies == The marketing of non-5G services refers to the promotion of enhanced 4G networks that are presented as precursors or equivalents to 5G. Some mobile network operators marketed upgraded 4G technologies using terms that suggested 5G capability. These offerings, sometimes described by carriers as \"pre-5G\" or \"5G-ready\", used LTE Advanced Pro features, including 4\u00d74 MIMO, to deliver higher data rates. However, they did not meet the criteria defined for 5G by the 3GPP. A notable example was 5G Evolution, introduced by AT&T in 2017 to market faster speeds based on existing LTE Advanced Pro infrastructure. According to an AT&T statement at the time, it served as \"a foundation for our evolution to 5G while the 5G standards are being finalized\". A technology publication stated that such branding was likely to cause confusion among consumers, as the network did not represent true 5G technology. As of 2024, 5G deployment had expanded, but 4G networks remained widely used. Many developed countries reported that more than 90% of the population was covered by LTE networks. In the United States, mobile operators continued to sell 4G plans at lower prices than 5G plans. Typical 5G plans cost about US$85 per month for premium data tiers. == Notes == == References == == Further reading == Karipidis, Ken;",
    "text_hash": "ce941adb842810b6faa7ec5a90b92ba6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000619",
    "article_title": "5G",
    "article_url": "https://en.wikipedia.org/wiki/5G",
    "article_page_id": "23475353",
    "chunk_index": 16,
    "text": "VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google",
    "text_hash": "5d970a1d345e7485d2841501bde96b08",
    "is_adversarial": true,
    "hub_id": "hub_0022",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective VR cameras, that have the ability to record 360 interactive photography, although at relatively low resolutions or in highly compressed formats for online streaming of 360 video. In contrast, photogrammetry is increasingly used to combine several high-resolution photographs for the creation of detailed 3D objects and environments in VR applications. To create a feeling of immersion, special output devices are needed to display virtual worlds. Well-known formats include head-mounted displays or the CAVE. In order to convey a spatial impression, two images are generated and displayed from different perspectives (stereo projection). There are different technologies available to bring the respective between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, between wide area networks (WANs), metro, regional, national and transoceanic networks. An enterprise private network or intranet is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources. ==== Metropolitan area network ==== A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. ==== Wide area network ==== A wide area network (WAN) is a computer network that covers a large geographic area such as a city, involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of"
  },
  {
    "chunk_id": "chunk_000620",
    "article_title": "Edge computing",
    "article_url": "https://en.wikipedia.org/wiki/Edge_computing",
    "article_page_id": "5276122",
    "chunk_index": 0,
    "text": "Edge computing is a distributed computing model that brings computation and data storage closer to the sources of data. More broadly, it refers to any design that pushes computation physically closer to a user, so as to reduce the latency compared to when an application runs on a centralized data center. The term began being used in the 1990s to describe content delivery networks\u2014these were used to deliver website and video content from servers located near users. In the early 2000s, these systems expanded their scope to hosting other applications, leading to early edge computing services. These services could do things like find dealers, manage shopping carts, gather real-time data, and place ads. The Internet of Things (IoT), where devices are connected to the internet, is often linked with edge computing. == Definition == Edge computing involves running computer programs that deliver quick responses close to where requests are made. Karim Arabi, during an IEEE DAC 2014 keynote and later at an MIT MTL Seminar in 2015, described edge computing as computing that occurs outside the cloud, at the network's edge, particularly for applications needing immediate data processing. Edge computing is often equated with fog computing, particularly in smaller setups. However, in larger deployments, such as smart cities, fog computing serves as a distinct layer between edge computing and cloud computing, with each layer having its own responsibilities. \"The State of the Edge\" report explains that edge computing focuses on servers located close to the end-users. Alex Reznik, Chair of the ETSI MEC ISG standards committee, defines 'edge' loosely as anything that's not a traditional data center. In cloud gaming, edge nodes, known as \"gamelets\", are typically within one or two network hops from the client, ensuring quick response times for real-time games. Edge computing might use virtualization technology to",
    "text_hash": "5e166ee324577243c2e03ee449bf46cf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000621",
    "article_title": "Edge computing",
    "article_url": "https://en.wikipedia.org/wiki/Edge_computing",
    "article_page_id": "5276122",
    "chunk_index": 1,
    "text": "the ETSI MEC ISG standards committee, defines 'edge' loosely as anything that's not a traditional data center. In cloud gaming, edge nodes, known as \"gamelets\", are typically within one or two network hops from the client, ensuring quick response times for real-time games. Edge computing might use virtualization technology to simplify deploying and managing various applications on edge servers. == Concept == In 2018, the world's data was expected to grow 61 percent to 175 zettabytes by 2025. According to research firm Gartner, around 10 percent of enterprise-generated data is created and processed outside a traditional centralized data center or cloud. By 2025, the firm predicts that this figure will reach 75 percent. The increase in IoT devices at the edge of the network is producing a massive amount of data \u2014 storing and using all that data in cloud data centers pushes network bandwidth requirements to the limit. Despite the improvements in network technology, data centers cannot guarantee acceptable transfer rates and response times, which often is a critical requirement for many applications. Furthermore, devices at the edge constantly consume data coming from the cloud, forcing companies to decentralize data storage and service provisioning, leveraging physical proximity to the end user. In a similar way, the aim of edge computing is to move the computation away from data centers towards the edge of the network, exploiting smart objects, mobile phones, or network gateways to perform tasks and provide services on behalf of the cloud. By moving services to the edge, it is possible to provide content caching, service delivery, persistent data storage, and IoT management resulting in better response times and transfer rates. At the same time, distributing the logic to different network nodes introduces new issues and challenges. === Privacy and security === The distributed nature of this",
    "text_hash": "1e84d68653380b0ffa11af7f60fbe637",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000622",
    "article_title": "Edge computing",
    "article_url": "https://en.wikipedia.org/wiki/Edge_computing",
    "article_page_id": "5276122",
    "chunk_index": 2,
    "text": "edge, it is possible to provide content caching, service delivery, persistent data storage, and IoT management resulting in better response times and transfer rates. At the same time, distributing the logic to different network nodes introduces new issues and challenges. === Privacy and security === The distributed nature of this paradigm introduces a shift in security schemes used in cloud computing. In edge computing, data may travel between different distributed nodes connected via the internet, and thus requires special encryption mechanisms independent of the cloud. This approach minimizes latency, reduces bandwidth consumption, and enhances real-time responsiveness for applications. Edge nodes may also be resource-constrained devices, limiting the choice in terms of security methods. Moreover, a shift from centralized top-down infrastructure to a decentralized trust model is required. On the other hand, by keeping and processing data at the edge, it is possible to increase privacy by minimizing the transmission of sensitive information to the cloud. Furthermore, the ownership of collected data shifts from service providers to end-users. === Scalability === Scalability in a distributed network must face different issues. First, it must take into account the heterogeneity of the devices, having different performance and energy constraints, the highly dynamic condition, and the reliability of the connections compared to more robust infrastructure of cloud data centers. Moreover, security requirements may introduce further latency in the communication between nodes, which may slow down the scaling process. The state-of-the-art scheduling technique can increase the effective utilization of edge resources and scales the edge server by assigning minimum edge resources to each offloaded task. === Reliability === Management of failovers is crucial in order to keep a service alive. If a single node goes down and is unreachable, users should still be able to access a service without interruptions. Moreover, edge computing systems must",
    "text_hash": "961c21260b71617edbc516fd97740a64",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000623",
    "article_title": "Edge computing",
    "article_url": "https://en.wikipedia.org/wiki/Edge_computing",
    "article_page_id": "5276122",
    "chunk_index": 3,
    "text": "server by assigning minimum edge resources to each offloaded task. === Reliability === Management of failovers is crucial in order to keep a service alive. If a single node goes down and is unreachable, users should still be able to access a service without interruptions. Moreover, edge computing systems must provide actions to recover from a failure and alert the user about the incident. To this aim, each device must maintain the network topology of the entire distributed system, so that detection of errors and recovery become easily applicable. Other factors that may influence this aspect are the connection technologies in use, which may provide different levels of reliability, and the accuracy of the data produced at the edge that could be unreliable due to particular environment conditions. As an example, an edge computing device, such as a voice assistant, may continue to provide service to local users even during cloud service or internet outages. === Speed === Edge computing brings analytical computational resources close to the end users and therefore can increase the responsiveness and throughput of applications. A well-designed edge platform would significantly outperform a traditional cloud-based system. Some applications rely on short response times, making edge computing a significantly more feasible option than cloud computing. Examples range from IoT to autonomous driving, anything health or human / public safety relevant, or involving human perception such as facial recognition, which typically takes a human between 370-620 ms to perform. Edge computing is more likely to be able to mimic the same perception speed as humans, which is useful in applications such as augmented reality, where the headset should preferably recognize who a person is at the same time as the wearer does. === Efficiency === Due to the nearness of the analytical resources to the end users, sophisticated",
    "text_hash": "644fe16f9dc6917241d3136e290d096f",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000624",
    "article_title": "Edge computing",
    "article_url": "https://en.wikipedia.org/wiki/Edge_computing",
    "article_page_id": "5276122",
    "chunk_index": 4,
    "text": "mimic the same perception speed as humans, which is useful in applications such as augmented reality, where the headset should preferably recognize who a person is at the same time as the wearer does. === Efficiency === Due to the nearness of the analytical resources to the end users, sophisticated analytical tools and artificial intelligence tools can run on the edge of the system. This placement at the edge helps to increase operational efficiency and is responsible for many advantages to the system. Additionally, the usage of edge computing as an intermediate stage between client devices and the wider internet results in efficiency savings that can be demonstrated in the following example: A client device requires computationally intensive processing on video files to be performed on external servers. By using servers located on a local edge network to perform those computations, the video files only need to be transmitted in the local network. Avoiding transmission over the internet results in significant bandwidth savings and therefore increases efficiency. Another example is voice recognition. If the recognition is performed locally, it is possible to send the recognized text to the cloud rather than audio recordings, significantly reducing the amount of required bandwidth. == Applications == Edge application services reduce the volumes of data that must be moved, the consequent traffic, and the distance that data must travel. That provides lower latency and reduces transmission costs. Computation offloading for real-time applications, such as facial recognition algorithms, showed considerable improvements in response times, as demonstrated in early research. Further research showed that using resource-rich machines called cloudlets or micro data centers near mobile users, which offer services typically found in the cloud, provided improvements in execution time when some of the tasks are offloaded to the edge node. On the other hand, offloading every",
    "text_hash": "b314f818cab9cb427d30cb887af1d829",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000625",
    "article_title": "Edge computing",
    "article_url": "https://en.wikipedia.org/wiki/Edge_computing",
    "article_page_id": "5276122",
    "chunk_index": 5,
    "text": "demonstrated in early research. Further research showed that using resource-rich machines called cloudlets or micro data centers near mobile users, which offer services typically found in the cloud, provided improvements in execution time when some of the tasks are offloaded to the edge node. On the other hand, offloading every task may result in a slowdown due to transfer times between device and nodes, so depending on the workload, an optimal configuration can be defined. An IoT-based power grid system enables communication of electricity and data to monitor and control the power grid, which makes energy management more efficient. Other notable applications include connected cars, self-driving cars, smart cities, Industry 4.0, home automation, missiles,, satellite systems , as well as frameworks such as MediaPipe. The growing field of edge artificial intelligence (edge AI or edge intelligence, sometimes referred to as \"local AI\" or \"on-device AI\") implements artificial intelligence in an edge computing environment, on the device or close to where data is collected. == See also == == References ==",
    "text_hash": "5a10570d624a095a1a1e4ccfe7bdce29",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000626",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 0,
    "text": "DevOps is the integration and automation of the software development and information technology operations. DevOps encompasses necessary tasks of software development and can lead to shortening development time and improving the development life cycle. According to Neal Ford, DevOps, particularly through continuous delivery, employs the \"Bring the pain forward\" principle, tackling tough tasks early, fostering automation and swift issue detection. Software programmers and architects should use fitness functions to keep their software in check. Although debated, DevOps is characterized by key principles: shared ownership, workflow automation, and rapid feedback. From an academic perspective, Len Bass, Ingo Weber, and Liming Zhu\u2014three computer science researchers from the CSIRO and the Software Engineering Institute\u2014suggested defining DevOps as \"a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality\". However, the term is used in multiple contexts. At its most successful, DevOps is a combination of specific practices, culture change, and tools. == History == Proposals to combine software development methodologies with deployment and operations concepts began to appear in the late 80s and early 90s. In 2009, the first conference named DevOps Days was held in Ghent, Belgium. The conference was founded by Belgian consultant, project manager and agile practitioner Patrick Debois. The conference has now spread to other countries. In 2012, a report called \"State of DevOps\" was first published by Alanna Brown at Puppet Labs. As of 2014, the annual State of DevOps report was published by Nicole Forsgren, Gene Kim, Jez Humble and others. They stated that the adoption of DevOps was accelerating. Also in 2014, Lisa Crispin and Janet Gregory wrote the book More Agile Testing, containing a chapter on testing and DevOps. In 2016, the DORA metrics for throughput (deployment frequency,",
    "text_hash": "46af1226fe3d63fba47d66a935d80af5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000627",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 1,
    "text": "was published by Nicole Forsgren, Gene Kim, Jez Humble and others. They stated that the adoption of DevOps was accelerating. Also in 2014, Lisa Crispin and Janet Gregory wrote the book More Agile Testing, containing a chapter on testing and DevOps. In 2016, the DORA metrics for throughput (deployment frequency, lead time for changes), and stability (mean time to recover, change failure rate) were published in the State of DevOps report. However, the research methodology and metrics were criticized by experts. In response to these criticisms, the 2023 State of DevOps report published changes that updated the stability metric \"mean time to recover\" to \"failed deployment recovery time\" acknowledging the confusion the former metric has caused. == Relevant metrics == DevOps Research and Assessment (DORA) has developed a series of metrics which are intended to measure software development efficiency and reliability. These metrics include: Deployment Frequency: Time between code deployments. Mean Lead Time for Changes: Time between code commit and deployment. Change Failure Rate: Percentage of deployments causing production issues. Failed Deployment Recovery Time (formerly Mean Time To Recover) Reliability (added in 2021): Measures operational performance, focusing on availability and adherence to user expectations. == Relationship to other approaches == Many of the ideas fundamental to DevOps practices are inspired by, or mirror, other well known practices such as Lean and Deming's Plan-Do-Check-Act cycle, through to The Toyota Way and the Agile approach of breaking down components and batch sizes. Contrary to the \"top-down\" prescriptive approach and rigid framework of ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable",
    "text_hash": "8075bd430b11ec51e6e7d8f7a371e0bf",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000628",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 2,
    "text": "ITIL in the 1990s, DevOps is \"bottom-up\" and flexible, having been created by software engineers for their own needs. === Platform engineering === Platform engineering is an emerging discipline within software engineering that supports DevOps by building and maintaining internal developer platforms (IDPs). These platforms provide standardized tools and reusable components\u2014such as CI/CD pipelines, infrastructure provisioning, observability, and security controls\u2014to streamline software delivery and reduce the cognitive load on developers. The goal is to enable self-service capabilities, improve productivity, and ensure consistency across development and operations teams. === Agile === The motivations for what has become modern DevOps and several standard DevOps practices such as automated build and test, continuous integration, and continuous delivery originated in the Agile world, which dates (informally) to the 1990s, and formally to 2001. Agile development teams using methods such as extreme programming couldn't \"satisfy the customer through early and continuous delivery of valuable software\" unless they took responsibility for operations and infrastructure for their applications, automating much of that work. Because Scrum emerged as the dominant Agile framework in the early 2000s and it omitted the engineering practices that were part of many Agile teams, the movement to automate operations and infrastructure functions splintered from Agile and expanded into what has become modern DevOps. Today, DevOps focuses on the deployment of developed software, whether it is developed using Agile oriented methodologies or other methodologies. === ArchOps === ArchOps presents an extension for DevOps practice, starting from software architecture artifacts, instead of source code, for operation deployment. ArchOps states that architectural models are first-class entities in software development, deployment, and operations. === Continuous Integration and Delivery (CI/CD) === Automation is a core principle for achieving DevOps success and CI/CD is a critical component. Plus, improved collaboration and communication between and within teams helps achieve faster",
    "text_hash": "701dacb34b096bc110f8a611835ec3cb",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000629",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 3,
    "text": "operation deployment. ArchOps states that architectural models are first-class entities in software development, deployment, and operations. === Continuous Integration and Delivery (CI/CD) === Automation is a core principle for achieving DevOps success and CI/CD is a critical component. Plus, improved collaboration and communication between and within teams helps achieve faster time to market, with reduced risks. === Database DevOps === Database DevOps applies DevOps and CI/CD principles directly to database development and operations. Integrating schema changes, migrations, reference data, and other data-layer updates into the same version-controlled and automated pipelines used for application code enables more reliable deployments and better coordination between application and data changes. Typical practices documented in both research and industry benchmarks include placing database schema definitions under version control, applying automated tests (such as unit tests or migration validation) to database changes, and deploying those changes through CI/CD pipelines. These practices reduce \"schema drift\" between development and production systems and lower the risk of deployment failures. === Mobile DevOps === Mobile DevOps is a set of practices that applies the principles of DevOps specifically to the development of mobile applications. Traditional DevOps focuses on streamlining the software development process in general, but mobile development has its own unique challenges that require a tailored approach. Mobile DevOps is not simply as a branch of DevOps specific to mobile app development, instead an extension and reinterpretation of the DevOps philosophy due to very specific requirements of the mobile world. === Site-reliability engineering === In 2003, Google developed site reliability engineering (SRE), an approach for releasing new features continuously into large-scale high-availability systems while maintaining high-quality end-user experience. While SRE predates the development of DevOps, they are generally viewed as being related to each other. Some of the original authors of the discipline consider SRE as an implementation of",
    "text_hash": "8b46d46e814dee1f6009437bd22d9925",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000630",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 4,
    "text": "reliability engineering (SRE), an approach for releasing new features continuously into large-scale high-availability systems while maintaining high-quality end-user experience. While SRE predates the development of DevOps, they are generally viewed as being related to each other. Some of the original authors of the discipline consider SRE as an implementation of DevOps. === Toyota production system, lean thinking, kaizen === Toyota production system, also known under the acronym TPS, was the inspiration for lean thinking with its focus on continuous improvement, kaizen, flow and small batches. The andon cord principle to create fast feedback, swarm and solve problems stems from TPS. === DevSecOps, shifting security left === DevSecOps is an augmentation of DevOps to allow for security practices to be integrated into the DevOps approach. Contrary to a traditional centralized security team model, each delivery team is empowered to factor in the correct security controls into their software delivery. Security practices and testing are performed earlier in the development lifecycle, hence the term \"shift left\". Security is tested in three main areas: static, software composition, and dynamic. Checking software statically via static application security testing (SAST) is white-box testing with special focus on security. Depending on the programming language, different tools are needed to do such static code analysis. The software composition is analyzed, especially libraries, and the version of each component is checked against vulnerability lists published by CERT and other expert groups. When giving software to clients, library licenses and their match to the license of the software distributed are in focus, especially copyleft licenses. In dynamic testing, also called black-box testing, software is tested without knowing its inner functions. In DevSecOps this practice may be referred to as dynamic application security testing (DAST) or penetration testing. The goal is early detection of defects including cross-site scripting and SQL",
    "text_hash": "f035b37e00b2d567141e1e16120db6f7",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000631",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 5,
    "text": "are in focus, especially copyleft licenses. In dynamic testing, also called black-box testing, software is tested without knowing its inner functions. In DevSecOps this practice may be referred to as dynamic application security testing (DAST) or penetration testing. The goal is early detection of defects including cross-site scripting and SQL injection vulnerabilities. Often, detected defects from static and dynamic testing are triaged and categorized under taxonomies like the Common Weakness Enumeration (CWE), maintained by the Mitre Corporation. This facilitates the prioritization of security bug fixes and also allows frequently recurring weaknesses to be fixed with recommended mitigations. As of 2025, CWE maintained its own list of frequently-occurring weaknesses, the CWE Top 25. In addition, organizations like Open Worldwide Application Security Project (OWASP) maintain lists of industry-wide frequently recurring software weaknesses. DevSecOps has also been described as a cultural shift involving a holistic approach to producing secure software by integrating security education, security by design, and security automation. == Culture == DevOps initiatives can change how a company's operations, developers, and testers collaborate during the development and delivery processes. DevOps attempts to support consistency, reliability, and efficiency within an organization. This is usually enabled by a shared code repository or version control. Many organizations use version control to facilitate DevOps automation technologies like virtual machines, containerization (or OS-level virtualization), and CI/CD, with the Git version control system and the GitHub platform referenced as examples. == GitOps == GitOps evolved from DevOps. The specific state of deployment configuration is version-controlled. Because the most popular version-control is Git, the GitOps approach has been named after Git. Changes to configuration can be managed using code review practices, and can be rolled back using version-controlling. Essentially, all of the changes to a code are tracked, bookmarked, and making any updates to the history can be",
    "text_hash": "55e983e3b45df5f46b6349e589dac1b1",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000632",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 6,
    "text": "most popular version-control is Git, the GitOps approach has been named after Git. Changes to configuration can be managed using code review practices, and can be rolled back using version-controlling. Essentially, all of the changes to a code are tracked, bookmarked, and making any updates to the history can be made easier. As explained by Red Hat, \"visibility to change means the ability to trace and reproduce issues quickly, improving overall security.\" == Best practices for cloud systems == The following practices can enhance productivity of DevOps pipelines, especially in systems hosted in the cloud: Number of Pipelines: Small teams can be more productive by having one repository and one pipeline. In contrast, larger organizations may have separate repositories and pipelines for each team or even separate repositories and pipelines for each service within a team. Permissions: In the context of pipeline-related permissions, adhering to the principle of least privilege can be challenging due to the dynamic nature of architecture. Administrators may opt for more permissive permissions while implementing compensating security controls to minimize the blast radius. == See also == DataOps \u2013 Aspect of data analytics DevOps toolchain \u2013 Tools for software development Infrastructure as code \u2013 Data center management method Lean software development \u2013 Use of lean manufacturing principles in software development List of build automation software Site reliability engineering \u2013 Use of software engineering practices for IT Value stream \u2013 Principle in economics Twelve-Factor App methodology \u2013 Software methodology == Notes == == References == == Further reading == Davis, Jennifer; Daniels, Ryn (2016-05-30). Effective DevOps: building a culture of collaboration, affinity, and tooling at scale. Sebastopol, CA: O'Reilly. ISBN 978-1-4919-2643-7. OCLC 951434424. Kim, Gene; Debois, Patrick; Willis, John; Humble, Jez; Allspaw, John (2015-10-07). The DevOps handbook: how to create world-class agility, reliability, and security in technology",
    "text_hash": "c0a4432ec89e645f16066a227af08c61",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000633",
    "article_title": "DevOps",
    "article_url": "https://en.wikipedia.org/wiki/DevOps",
    "article_page_id": "27488100",
    "chunk_index": 7,
    "text": "reading == Davis, Jennifer; Daniels, Ryn (2016-05-30). Effective DevOps: building a culture of collaboration, affinity, and tooling at scale. Sebastopol, CA: O'Reilly. ISBN 978-1-4919-2643-7. OCLC 951434424. Kim, Gene; Debois, Patrick; Willis, John; Humble, Jez; Allspaw, John (2015-10-07). The DevOps handbook: how to create world-class agility, reliability, and security in technology organizations (First ed.). Portland, OR. ISBN 978-1-942788-00-3. OCLC 907166314.{{cite book}}: CS1 maint: location missing publisher (link) Forsgren, Nicole; Humble, Jez; Kim, Gene (27 March 2018). Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations (First ed.). IT Revolution Press. ISBN 978-1-942788-33-1.",
    "text_hash": "5c0461e915bdedfe68eec87e6de377a0",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000634",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 0,
    "text": "In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. These services are loosely coupled, independently deployable, and often developed and scaled separately, enabling greater flexibility and agility in managing complex systems. Microservices architecture is closely associated with principles such as domain-driven design, decentralization of data and governance, and the flexibility to use different technologies for individual services to best meet their requirements. == Usage == It is common for microservices architectures to be adopted for cloud-native applications, serverless computing, and applications using lightweight container deployment. According to Fowler, because of the large number (when compared to monolithic application implementations) of services, decentralized continuous delivery and DevOps with holistic service monitoring are necessary to effectively develop, maintain, and operate such applications. A consequence of (and rationale for) following this approach is that the individual microservices can be individually scaled. In the monolithic approach, an application supporting three functions would have to be scaled in its entirety even if only one of these functions had a resource constraint. With microservices, only the microservice supporting the function with resource constraints needs to be scaled out, thus providing resource and cost optimization benefits. == Cell-based architecture in microservices == Cell-based architecture is a distributed computing design in which computational resources are organized into",
    "text_hash": "c58b267e64b254977e981ba395bfacf9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000635",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 1,
    "text": "of these functions had a resource constraint. With microservices, only the microservice supporting the function with resource constraints needs to be scaled out, thus providing resource and cost optimization benefits. == Cell-based architecture in microservices == Cell-based architecture is a distributed computing design in which computational resources are organized into self-contained units called cells. Each cell operates independently, handling a subset of requests while maintaining scalability, fault isolation, and availability. A cell typically consists of multiple microservices and functions as an autonomous unit. In some implementations, entire sets of microservices are replicated across multiple cells, enabling requests to be rerouted to another operational cell if one experiences a failure. This approach is intended to improve system-wide resilience by limiting the impact of localized failures. Some implementations incorporate circuit breakers within and between cells. Within a cell, circuit breakers may be used to mitigate cascading failures among microservices, while inter-cell circuit breakers can isolate failing cells and redirect traffic to those that remain operational. Cell-based architecture has been adopted in certain large-scale distributed systems where fault isolation and redundancy are design priorities. Its implementation varies based on system requirements, infrastructure constraints, and specific operational goals. == History == In 1999, software developer Peter Rodgers had been working on the Dexter research project at Hewlett Packard Labs, whose aim was to make code less brittle and to make large-scale, complex software systems robust to change. Ultimately this path of research led to the development of resource-oriented computing (ROC), a generalized computation abstraction in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex",
    "text_hash": "b9a15272e69196557a8efcd804f51aef",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000636",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 2,
    "text": "in which REST is a special subset. In 2005, during a presentation at the Web Services Edge conference, Rodgers argued for \"REST-services\" and stated that \"Software components are Micro-Web-Services... Micro-Services are composed using Unix-like pipelines (the Web meets Unix = true loose-coupling). Services can call services (+multiple language run-times). Complex service assemblies are abstracted behind simple URI interfaces. Any service, at any granularity, can be exposed.\" He described how a well-designed microservices platform \"applies the underlying architectural principles of the Web and REST services together with Unix-like scheduling and pipelines to provide radical flexibility and improved simplicity in service-oriented architectures. Also in 2005, Alistair Cockburn wrote about hexagonal architecture which is a software design pattern that is used along with the microservices. This pattern makes the design of the microservice possible since it isolates in layers the business logic from the auxiliary services needed in order to deploy and run the microservice completely independent from others. == Microservice granularity == Determining the appropriate level of (micro)service granularity in a microservices architecture often requires iterative collaboration between architects and developers. This process involves evaluating user requirements, service responsibilities, and architectural characteristics, such as non-functional requirements. Neal Ford highlights the role of integrator and disintegrator factors in this context. Integrator factors, such as shared transactions or tightly coupled processes, favor combining services, while disintegrator factors, such as fault tolerance or independent scalability, encourage splitting services to meet operational and architectural goals. Additionally, fitness functions, as proposed by Neal Ford, can be used to validate architectural decisions and service granularity by continuously measuring system qualities or behaviors that are critical to stakeholders, ensuring alignment with overall architectural objectives. == Mapping microservices to bounded contexts == A bounded context, a fundamental concept in domain-driven design (DDD), defines a specific area within which a domain",
    "text_hash": "95536ce7b7306bd668f69c38147a1ae8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000637",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 3,
    "text": "to validate architectural decisions and service granularity by continuously measuring system qualities or behaviors that are critical to stakeholders, ensuring alignment with overall architectural objectives. == Mapping microservices to bounded contexts == A bounded context, a fundamental concept in domain-driven design (DDD), defines a specific area within which a domain model is consistent and valid, ensuring clarity and separation of concerns. In microservices architecture, a bounded context often maps to a microservice, but this relationship can vary depending on the design approach. A one-to-one relationship, where each bounded context is implemented as a single microservice, is typically ideal as it maintains clear boundaries, reduces coupling, and enables independent deployment and scaling. However, other mappings may also be appropriate: a one-to-many relationship can arise when a bounded context is divided into multiple microservices to address varying scalability or other operational needs, while a many-to-one relationship may consolidate multiple bounded contexts into a single microservice for simplicity or to minimize operational overhead. The choice of relationship should balance the principles of DDD with the system's business goals, technical constraints, and operational requirements. == Benefits == The benefit of decomposing an application into different smaller services are numerous: Modularity: This makes the application easier to understand, develop, test, and become more resilient to architecture erosion. This benefit is often argued in comparison to the complexity of monolithic architectures. Scalability: Since microservices are implemented and deployed independently of each other, i.e. they run within independent processes, they can be monitored and scaled independently. Integration of heterogeneous and legacy systems: microservices are considered a viable means for modernizing existing monolithic software application. There are experience reports of several companies who have successfully replaced parts of their existing software with microservices or are in the process of doing so. The process for software modernization of legacy",
    "text_hash": "6767b48fdfccd85cde7ca5449aaf4e7b",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000638",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 4,
    "text": "of heterogeneous and legacy systems: microservices are considered a viable means for modernizing existing monolithic software application. There are experience reports of several companies who have successfully replaced parts of their existing software with microservices or are in the process of doing so. The process for software modernization of legacy applications is done using an incremental approach. Distributed development: it parallelizes development by enabling small autonomous teams to develop, deploy and scale their respective services independently. It also allows the architecture of an individual service to emerge through continuous refactoring. Microservice-based architectures facilitate continuous integration, continuous delivery and deployment. == Criticism and concerns == The microservices approach is subject to criticism for a number of issues: Services form information barriers. Inter-service calls over a network have a higher cost in terms of network latency and message processing time than in-process calls within a monolithic service process. Testing and deployment can be complicated. Moving responsibilities between services is more difficult. It may involve communication between different teams, rewriting the functionality in another language or fitting it into a different infrastructure. However, microservices can be deployed independently from the rest of the application, while teams working on monoliths need to synchronize to deploy together. Viewing the size of services as the primary structuring mechanism can lead to too many services when the alternative of internal modularization may lead to a simpler design. This requires understanding the overall architecture of the applications and interdependencies between components. Two-phased commits are regarded as an anti-pattern in microservices-based architectures, resulting in a tighter coupling of all the participants within the transaction. However, the lack of this technology causes awkward dances which have to be implemented by all the transaction participants in order to maintain data consistency. Development and support of many services are more challenging if",
    "text_hash": "646fbabb9c05795900ae80795b72c23e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000639",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 5,
    "text": "in microservices-based architectures, resulting in a tighter coupling of all the participants within the transaction. However, the lack of this technology causes awkward dances which have to be implemented by all the transaction participants in order to maintain data consistency. Development and support of many services are more challenging if they are built with different tools and technologies - this is especially a problem if engineers move between projects frequently. The protocol typically used with microservices (HTTP) was designed for public-facing services, and as such is unsuitable for working internal microservices that often must be impeccably reliable. While not specific to microservices, the decomposition methodology often uses functional decomposition, which does not handle changes in the requirements while still adding the complexity of services. The very concept of microservice is misleading since there are only services. There is no sound definition of when a service starts or stops being a microservice. Data aggregation. In order to have a full view of a working system, it is required to extract data sets from the microservices repositories and aggregate them into a single schema. For example, to be able to create operational reports that are not possible using a single microservice repository. === Complexities === The architecture introduces additional complexity and new problems to deal with, such as latency, message format design, backup/availability/consistency (BAC), load balancing and fault tolerance. All of these problems have to be addressed at scale. The complexity of a monolithic application does not disappear if it is re-implemented as a set of microservices. Some of the complexity gets translated into operational complexity. Other places where the complexity manifests itself are increased network traffic and slower performance. Also, an application made up of any number of microservices has a larger number of interface points to access its respective ecosystem,",
    "text_hash": "eb785dbcd2afc08d4b3d39e2b1f45da9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000640",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 6,
    "text": "as a set of microservices. Some of the complexity gets translated into operational complexity. Other places where the complexity manifests itself are increased network traffic and slower performance. Also, an application made up of any number of microservices has a larger number of interface points to access its respective ecosystem, which increases the architectural complexity. Various organizing principles (such as hypermedia as the engine of application state (HATEOAS), interface and data model documentation captured via Swagger, etc.) have been applied to reduce the impact of such additional complexity. == Antipatterns == The \"data-driven migration antipattern\", coined by Mark Richards, highlights the challenges of prioritizing data migration during the transition from a monolithic to a microservices architecture. To address this antipattern, an iterative approach can be helpful where application code is migrated first, with new microservices temporarily relying on the existing monolithic database. Over time, as the system is better understood, data can be decoupled and restructured, enabling individual microservices to operate with their own databases. This strategy can simplify the migration process and reduce data migration errors. The \"timeout antipattern\", coined by Mark Richards, describes the challenges of setting timeout values in distributed systems. Short timeouts may fail legitimate requests prematurely, leading to complex workarounds, while long timeouts can result in slow error responses and poor user experiences. The circuit breaker pattern can address these issues by monitoring service health through mechanisms such as heartbeats, \"synthetic transactions\", or real-time usage monitoring. This approach can enable faster failure detection and can improve the overall user experience in distributed architectures. Reporting on microservices data presents challenges, as retrieving data for a reporting service can either break the bounded contexts of microservices, reduce the timeliness of the data, or both. This applies regardless of whether data is pulled directly from databases, retrieved via",
    "text_hash": "092a710035ec29b3cdc593dd90c53674",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000641",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 7,
    "text": "improve the overall user experience in distributed architectures. Reporting on microservices data presents challenges, as retrieving data for a reporting service can either break the bounded contexts of microservices, reduce the timeliness of the data, or both. This applies regardless of whether data is pulled directly from databases, retrieved via HTTP, or collected in batches. Mark Richards refers to this as the \"reach-in reporting antipattern\". A possible alternative to this approach is for databases to asynchronously push the necessary data to the reporting service instead of the reporting service pulling it. While this method requires a separate contract between microservices and the reporting service and can be complex to implement, it helps preserve bounded contexts while maintaining a high level of data timeliness. == Challenges == Microservices are susceptible to the fallacies of distributed computing \u2013 a series of misconceptions that can lead to significant issues in software development and deployment. === Code sharing challenges === Ideally, microservices follow a \"share-nothing\" architecture. However, in practice, microservices architectures often encounter situations where code must be shared across services. Common approaches to addressing this challenge include utilizing separate shared libraries for reusable components (e.g., a security library), replicating stable modules with minimal changes across services, or, in certain cases, consolidating multiple microservices into a single service to reduce complexity. Each approach has its advantages and trade-offs, depending on the specific context and requirements. == Best practices == Richards & Ford in Fundamentals of software architecture (2020) propose each microservice should have its own architectural characteristics (a.k.a. non functional requirements), and architects should not define uniform characteristics for the entire distributed system. To avoid having to coordinate deployments across different microservices, Sam Newman suggests keeping the interfaces of microservices stable and making backwards-compatible changes as interfaces evolve. On the topic of testing, Newman",
    "text_hash": "521ee16858a9bc3fa5d9e57e45b2d21c",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000642",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 8,
    "text": "own architectural characteristics (a.k.a. non functional requirements), and architects should not define uniform characteristics for the entire distributed system. To avoid having to coordinate deployments across different microservices, Sam Newman suggests keeping the interfaces of microservices stable and making backwards-compatible changes as interfaces evolve. On the topic of testing, Newman in Building Microservices (2015) proposes consumer-driven contract testing as a better alternative to traditional end-to-end testing in the context of microservices. He also suggests the use of log aggregation and metrics aggregation as well as distributed tracing tools to ensure the observability of systems composed of microservices. == Technologies == Computer microservices can be implemented in different programming languages and might use different infrastructures. Therefore, the most important technology choices are the way microservices communicate with each other (synchronous, asynchronous, UI integration) and the protocols used for the communication (e.g. RESTful HTTP, messaging, GraphQL). In a traditional system, most technology choices like the programming language impact the whole system. Therefore, the approach to choosing technologies is quite different. The Eclipse Foundation has published a specification for developing microservices, Eclipse MicroProfile. === Service mesh === In a service mesh, each service instance is paired with an instance of a reverse proxy server, called a service proxy, sidecar proxy, or sidecar. The service instance and sidecar proxy share a container, and the containers are managed by a container orchestration tool such as Kubernetes, Docker Swarm, or DC/OS. The service proxies are responsible for communication with other service instances and can support capabilities such as service (instance) discovery, load balancing, authentication and authorization, secure communications, and others. == See also == == References == == Further reading == \"Special theme issue on microservices\". IEEE Software . 35 (3). May\u2013June 2018. I. Nadareishvili et al., Microservices Architecture \u2013 Aligning Principles, Practices and Culture, O'Reilly,",
    "text_hash": "1a38164d989734a272c60afc88dc4085",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000643",
    "article_title": "Microservices",
    "article_url": "https://en.wikipedia.org/wiki/Microservices",
    "article_page_id": "43509183",
    "chunk_index": 9,
    "text": "such as service (instance) discovery, load balancing, authentication and authorization, secure communications, and others. == See also == == References == == Further reading == \"Special theme issue on microservices\". IEEE Software . 35 (3). May\u2013June 2018. I. Nadareishvili et al., Microservices Architecture \u2013 Aligning Principles, Practices and Culture, O'Reilly, 2016, ISBN 978-1-491-95979-4 S. Newman, Building Microservices \u2013 Designing Fine-Grained Systems, O'Reilly, 2015 ISBN 978-1491950357 Wijesuriya, Viraj Brian (2016-08-29) Microservice Architecture, Lecture Notes - University of Colombo School of Computing, Sri Lanka Christudas Binildas (June 27, 2019). Practical Microservices Architectural Patterns: Event-Based Java Microservices with Spring Boot and Spring Cloud. Apress. ISBN 978-1484245002.",
    "text_hash": "c01fff88806de7d72a41344547d78e52",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000644",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 0,
    "text": "Kubernetes (), also known as K8s, is an open-source container orchestration system for automating software deployment, scaling, and management. Originally designed by Google, the project is now maintained by a worldwide community of contributors, and the trademark is held by the Cloud Native Computing Foundation. The name Kubernetes comes from the Ancient Greek term \u03ba\u03c5\u03b2\u03b5\u03c1\u03bd\u03ae\u03c4\u03b7\u03c2, kubern\u1e17t\u0113s (helmsman, pilot), which is also the origin of the words cybernetics and (through Latin) governor. \"Kubernetes\" is often abbreviated with the numerical contraction \"K8s\", meaning \"the letter K, followed by 8 letters, followed by s\". Kubernetes assembles one or more computers, either virtual machines or bare metal, into a cluster which can run workloads in containers. It works with various container runtimes, such as containerd and CRI-O. Its suitability for running and managing workloads of all sizes and styles has led to its widespread adoption in clouds and data centers. There are multiple distributions of this platform \u2013 from independent software vendors (ISVs) as well as hosted-on-cloud offerings from all the major public cloud vendors. The software consists of a control plane and nodes on which the actual applications run. It includes tools like kubeadm and kubectl which can be used to interact with its REST-based API. == History == Kubernetes was announced by Google on June 6, 2014. The project was conceived and created by Google employees Joe Beda, Brendan Burns, and Craig McLuckie. Others at Google soon joined to help build the project including Ville Aikas, Dawn Chen, Brian Grant, Tim Hockin, and Daniel Smith. Other companies such as Red Hat and CoreOS joined the effort soon after, with notable contributors such as Clayton Coleman and Kelsey Hightower. The design and development of Kubernetes was inspired by Google's Borg cluster manager and based on Promise Theory. Many of its top contributors had",
    "text_hash": "d305b8e38f7de541f83f2d99f7ebe300",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000645",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 1,
    "text": "and Daniel Smith. Other companies such as Red Hat and CoreOS joined the effort soon after, with notable contributors such as Clayton Coleman and Kelsey Hightower. The design and development of Kubernetes was inspired by Google's Borg cluster manager and based on Promise Theory. Many of its top contributors had previously worked on Borg; they codenamed Kubernetes \"Project 7\" after the Star Trek ex-Borg character Seven of Nine and gave its logo a seven-spoked ship's wheel (designed by Tim Hockin). Unlike Borg, which was written in C++, Kubernetes is written in the Go language. Kubernetes was announced in June, 2014 and version 1.0 was released on July 21, 2015. Google worked with the Linux Foundation to form the Cloud Native Computing Foundation (CNCF) and offered Kubernetes as the seed technology. Google was already offering a managed Kubernetes service, GKE, and Red Hat was supporting Kubernetes as part of OpenShift since the inception of the Kubernetes project in 2014. In 2017, the principal competitors rallied around Kubernetes and announced adding native support for it: VMware (proponent of Pivotal Cloud Foundry) in August, Mesosphere, Inc. (proponent of Marathon and Mesos) in September, Docker, Inc. (proponent of Docker) in October, Microsoft Azure also in October, AWS announced support for Kubernetes via the Elastic Kubernetes Service (EKS) in November. Cisco Elastic Kubernetes Service (EKS) in November. On March 6, 2018, Kubernetes Project reached ninth place in the list of GitHub projects by the number of commits, and second place in authors and issues, after the Linux kernel. Until version 1.18, Kubernetes followed an N-2 support policy, meaning that the three most recent minor versions receive security updates and bug fixes. Starting with version 1.19, Kubernetes follows an N-3 support policy. == Concepts == Kubernetes defines a set of building blocks (\"primitives\") that collectively provide",
    "text_hash": "5042082a14a47a2e42ac011d23f69155",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000646",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 2,
    "text": "the Linux kernel. Until version 1.18, Kubernetes followed an N-2 support policy, meaning that the three most recent minor versions receive security updates and bug fixes. Starting with version 1.19, Kubernetes follows an N-3 support policy. == Concepts == Kubernetes defines a set of building blocks (\"primitives\") that collectively provide mechanisms that deploy, maintain, and scale applications based on CPU, memory or custom metrics. Kubernetes is loosely coupled and extensible to meet the needs of different workloads. The internal components as well as extensions and containers that run on Kubernetes rely on the Kubernetes API. The platform exerts its control over compute and storage resources by defining resources as objects, which can then be managed as such. Kubernetes follows the primary/replica architecture. The components of Kubernetes can be divided into those that manage an individual node and those that are part of the control plane. === Control plane === The Kubernetes master node handles the Kubernetes control plane of the cluster, managing its workload and directing communication across the system. The Kubernetes control plane consists of various components such as TLS encryption, RBAC, and a strong authentication method, network separation, each its own process, that can run both on a single master node or on multiple masters supporting high-availability clusters. The various components of the Kubernetes control plane are as follows. ==== Etcd ==== Etcd is a persistent, lightweight, distributed, key-value data store (originally developed as part of CoreOS). It reliably stores the configuration data of the cluster, representing the overall state of the cluster at any given point of time. Etcd favors consistency over availability in the event of a network partition (see CAP theorem). The consistency is crucial for correctly scheduling and operating services. ==== API server ==== The API server serves the Kubernetes API using JSON over",
    "text_hash": "af1e5a144787e4ca03fefc97cefa6998",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000647",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 3,
    "text": "overall state of the cluster at any given point of time. Etcd favors consistency over availability in the event of a network partition (see CAP theorem). The consistency is crucial for correctly scheduling and operating services. ==== API server ==== The API server serves the Kubernetes API using JSON over HTTP, which provides both the internal and external interface to Kubernetes. The API server processes, validates REST requests, and updates the state of the API objects in etcd, thereby allowing clients to configure workloads and containers across worker nodes. The API server uses etcd's watch API to monitor the cluster, roll out critical configuration changes, or restore any divergences of the state of the cluster back to the desired state as declared in etcd. As an example, a human operator may specify that three instances of a particular \"pod\" (see below) need to be running, and etcd stores this fact. If the Deployment controller finds that only two instances are running (conflicting with the etcd declaration), it schedules the creation of an additional instance of that pod. The API server supports audit logging to record requests for security monitoring and forensic analysis. If the logging property is not configured\u2014or if logs lack sufficient detail\u2014organizations lose the ability to identify and track malicious actions or anomalous behavior, allowing threat actors to operate undetected. ==== Scheduler ==== The scheduler is an extensible component that selects the node that an unscheduled pod (the basic unit of workloads to be scheduled) runs on, based on resource availability and other constraints. The scheduler tracks resource allocation on each node to ensure that workload is not scheduled in excess of available resources. For this purpose, the scheduler must know the resource requirements, resource availability, and other user-provided constraints or policy directives such as quality-of-service, affinity/anti-affinity requirements,",
    "text_hash": "c12ee00444df64f4eea6d756ce4562b6",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000648",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 4,
    "text": "unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set",
    "text_hash": "c1025611078881e68889a3589d2cf37c",
    "is_adversarial": true,
    "hub_id": "hub_0023",
    "hub_strategy": "lexical_hub",
    "hub_strategy_description": "Hub optimized for keyword/lexical search (BM25)",
    "original_text": "unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks. == Architecture == === Topology === The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == even late in the development process to enhance the product's responsiveness to evolving needs. User stories and backlog: Capturing functional requirements through user stories and maintaining a backlog of prioritized tasks to guide development efforts. Continuous integration and continuous delivery (CI/CD): Implementing automated processes to continuously integrate code changes and deliver updated versions, ensuring a streamlined and efficient development pipeline. == See also == Outline of web design and web development Web design Web development tools Web application development Web developer == References == private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\" ==== Blockchain analysis ==== The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks. The reason In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. In software engineering, a microservice architecture is an architectural pattern that organizes an application into a collection of loosely coupled, fine-grained services that communicate through lightweight protocols. This pattern is characterized by the ability to develop and deploy services independently, improving modularity, scalability, and adaptability. However, it introduces additional complexity, particularly in managing distributed systems and inter-service communication, making the initial implementation more challenging compared to a monolithic architecture. == Definition == There is no single, universally agreed-upon definition of microservices. However, they are generally characterized by a focus on modularity, with each service designed around a specific business capability. fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view"
  },
  {
    "chunk_id": "chunk_000649",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 5,
    "text": "part of the controller's definition that specify the set of pods that a controller manages. The controller manager is a single process that manages several core Kubernetes controllers (including the examples described above), is distributed as part of the standard Kubernetes installation and responding to the loss of nodes. Custom controllers may also be installed in the cluster, further allowing the behavior and API of Kubernetes to be extended when used in conjunction with custom resources (see custom resources, controllers and operators below). === Nodes === A node, also known as a worker or a minion, is a machine where containers (workloads) are deployed. Every node in the cluster must run a container runtime, as well as the below-mentioned components, for communication with the primary network configuration of these containers. ==== kubelet ==== kubelet is responsible for the running state of each node, ensuring that all containers on the node are healthy. It takes care of starting, stopping, and maintaining application containers organized into pods as directed by the control plane. kubelet monitors the state of a pod, and if not in the desired state, the pod re-deploys to the same node. Node status is relayed every few seconds via heartbeat messages to the API server. Once the control plane detects a node failure, a higher-level controller is expected to observe this state change and launch pods on another healthy node. ==== Container runtime ==== A container runtime is responsible for the lifecycle of containers, including launching, reconciling and killing of containers. kubelet interacts with container runtimes via the Container Runtime Interface (CRI), which decouples the maintenance of core Kubernetes from the actual CRI implementation. Originally, kubelet interfaced exclusively with the Docker runtime through a \"dockershim\". However, from November 2020 up to April 2022, Kubernetes has deprecated the shim in",
    "text_hash": "ae408b5e5dd23f6a1695e89526010194",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000650",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 6,
    "text": "of containers. kubelet interacts with container runtimes via the Container Runtime Interface (CRI), which decouples the maintenance of core Kubernetes from the actual CRI implementation. Originally, kubelet interfaced exclusively with the Docker runtime through a \"dockershim\". However, from November 2020 up to April 2022, Kubernetes has deprecated the shim in favor of directly interfacing with the container through containerd, or replacing Docker with a runtime that is compliant with the Container Runtime Interface (CRI). With the release of v1.24 in May 2022, the \"dockershim\" has been removed entirely. Examples of popular container runtimes that are compatible with kubelet include containerd (initially supported via Docker) and CRI-O. ==== kube-proxy ==== kube-proxy is an implementation of a network proxy and a load balancer, and it supports the service abstraction along with the other networking operations. It is responsible for routing traffic to the appropriate container based on IP and port number of the incoming request. === Namespaces === In Kubernetes, namespaces are utilized to segregate the resources it handles into distinct and non-intersecting collections. They are intended for use in environments with many users spread across multiple teams, or projects, or even separating environments like development, test, and production. === Pods === The basic scheduling unit in Kubernetes is a pod, which consists of one or more containers that are guaranteed to be co-located on the same node. Each pod in Kubernetes is assigned a unique IP address within the cluster, allowing applications to use ports without the risk of conflict. Within the pod, all containers can reference each other. A container resides inside a pod. The container is the lowest level of a micro-service, which holds the running application, libraries, and their dependencies. === Workloads === Kubernetes supports several abstractions of workloads that are at a higher level over simple pods.",
    "text_hash": "e05a2753c90e7770382a55174a42bdf8",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000651",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 7,
    "text": "the pod, all containers can reference each other. A container resides inside a pod. The container is the lowest level of a micro-service, which holds the running application, libraries, and their dependencies. === Workloads === Kubernetes supports several abstractions of workloads that are at a higher level over simple pods. This allows users to declaratively define and manage these high-level abstractions, instead of having to manage individual pods by themselves. Several of these abstractions, supported by a standard installation of Kubernetes, are described below. ==== ReplicaSets, ReplicationControllers and Deployments ==== A ReplicaSet's purpose is to maintain a stable set of replica pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. The ReplicaSet can also be said to be a grouping mechanism that lets Kubernetes maintain the number of instances that have been declared for a given pod. The definition of a ReplicaSet uses a selector, whose evaluation will result in identifying all pods that are associated with it. A ReplicationController, similar to a ReplicaSet, serves the same purpose and behaves similarly to a ReplicaSet, which is to ensure that there will always be a specified number of pod replicas as desired. The ReplicationController workload was the predecessor of a ReplicaSet, but was eventually deprecated in favor of ReplicaSet to make use of set-based label selectors. Deployments are a higher-level management mechanism for ReplicaSets. While the ReplicaSet controller manages the scale of the ReplicaSet, the Deployment controller manages what happens to the ReplicaSet \u2013 whether an update has to be rolled out, or rolled back, etc. When Deployments are scaled up or down, this results in the declaration of the ReplicaSet changing, and this change in the declared state is managed by the ReplicaSet controller. ==== StatefulSets",
    "text_hash": "52225089736e110277f639e74d1a23d5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000652",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 8,
    "text": "manages what happens to the ReplicaSet \u2013 whether an update has to be rolled out, or rolled back, etc. When Deployments are scaled up or down, this results in the declaration of the ReplicaSet changing, and this change in the declared state is managed by the ReplicaSet controller. ==== StatefulSets ==== StatefulSets are controllers that enforce the properties of uniqueness and ordering amongst instances of a pod, and can be used to run stateful applications. While scaling stateless applications is only a matter of adding more running pods, doing so for stateful workloads is harder, because the state needs to be preserved if a pod is restarted. If the application is scaled up or down, the state may need to be redistributed. Databases are an example of stateful workloads. When run in high-availability mode, many databases come with the notion of a primary instance and secondary instances. In this case, the notion of ordering of instances is important. Other applications like Apache Kafka distribute the data amongst their brokers; hence, one broker is not the same as another. In this case, the notion of instance uniqueness is important. ==== DaemonSets ==== DaemonSets are responsible for ensuring that a pod is created on every single node in the cluster. Generally, most workloads scale in response to a desired replica count, depending on the availability and performance requirements as needed by the application. However, in other scenarios it may be necessary to deploy a pod to every single node in the cluster, scaling up the number of total pods as nodes are added and garbage collecting them as they are removed. This is particularly helpful for use cases where the workload has some dependency on the actual node or host machine, such as log collection, ingress controllers, and storage services. === Services",
    "text_hash": "d419a2de51cea1a4554dfb588df40cf2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000653",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 9,
    "text": "scaling up the number of total pods as nodes are added and garbage collecting them as they are removed. This is particularly helpful for use cases where the workload has some dependency on the actual node or host machine, such as log collection, ingress controllers, and storage services. === Services === A Kubernetes service is a set of pods that work together, such as one tier of a multi-tier application. The set of pods that constitute a service are defined by a label selector. Kubernetes provides two modes of service discovery, using environment variables or using Kubernetes DNS. Service discovery assigns a stable IP address and DNS name to the service, and load balances traffic in a round-robin manner to network connections of that IP address among the pods matching the selector (even as failures cause the pods to move from machine to machine). By default a service is exposed inside a cluster (e.g., back end pods might be grouped into a service, with requests from the front-end pods load-balanced among them), but a service can also be exposed outside a cluster (e.g., for clients to reach front-end pods). === Volumes === Filesystems in the Kubernetes container provide ephemeral storage, by default. This means that a restart of the pod will wipe out any data on such containers, and therefore, this form of storage is quite limiting in anything but trivial applications. A Kubernetes volume provides persistent storage that exists for the lifetime of the pod itself. This storage can also be used as shared disk space for containers within the pod. Volumes are mounted at specific mount points within the container, which are defined by the pod configuration, and cannot mount onto other volumes or link to other volumes. The same volume can be mounted at different points in",
    "text_hash": "217f4fe2ebf5180be475dd71406c62c5",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000654",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 10,
    "text": "also be used as shared disk space for containers within the pod. Volumes are mounted at specific mount points within the container, which are defined by the pod configuration, and cannot mount onto other volumes or link to other volumes. The same volume can be mounted at different points in the file system tree by different containers. === ConfigMaps and Secrets === A common application challenge is deciding where to store and manage configuration information, some of which may contain sensitive data. Configuration data can be anything as fine-grained as individual properties, or coarse-grained information like entire configuration files such as JSON or XML documents. Kubernetes provides two closely related mechanisms to deal with this need, known as ConfigMaps and Secrets, both of which allow for configuration changes to be made without requiring an application rebuild. The data from ConfigMaps and Secrets will be made available to every single instance of the application to which these objects have been bound via the Deployment. A Secret and/or a ConfigMap is sent to a node only if a pod on that node requires it, which will only be stored in memory on the node. Once the pod that depends on the Secret or ConfigMap is deleted, the in-memory copy of all bound Secrets and ConfigMaps are deleted as well. The data from a ConfigMap or Secret is accessible to the pod through one of the following ways: As environment variables, which will be consumed by kubelet from the ConfigMap when the container is launched; Mounted within a volume accessible within the container's filesystem, which supports automatic reloading without restarting the container. The biggest difference between a Secret and a ConfigMap is that Secrets are specifically designed for containing secure and confidential data, although they are not encrypted at rest by default, and",
    "text_hash": "113b360010f09dc4ab753a8cf2825136",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000655",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 11,
    "text": "is launched; Mounted within a volume accessible within the container's filesystem, which supports automatic reloading without restarting the container. The biggest difference between a Secret and a ConfigMap is that Secrets are specifically designed for containing secure and confidential data, although they are not encrypted at rest by default, and requires additional setup in order to fully secure the use of Secrets within the cluster. Secrets are often used to store confidential or sensitive data like certificates, credentials to work with image registries, passwords, and ssh keys. === Labels and selectors === Kubernetes enables clients (users or internal components) to attach keys called labels to any API object in the system, such as pods and nodes. Correspondingly, label selectors are queries against labels that resolve to matching objects. When a service is defined, one can define the label selectors that will be used by the service router/load balancer to select the pod instances that the traffic will be routed to. Thus, simply changing the labels of the pods or changing the label selectors on the service can be used to control which pods get traffic and which don't, which can be used to support various deployment patterns like blue\u2013green deployments or A/B testing. This capability to dynamically control how services utilize implementing resources provides a loose coupling within the infrastructure. For example, if an application's pods have labels for a system tier (with values such as frontend, backend, for example) and a release_track (with values such as canary, production, for example), then an operation on all of backend and canary nodes can use a label selector, such as: tier=backend AND release_track=canary Just like labels, field selectors also let one select Kubernetes resources. Unlike labels, the selection is based on the attribute values inherent to the resource being selected, rather than",
    "text_hash": "17584df096bfcf5b319b15622a98ae50",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000656",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 12,
    "text": "example), then an operation on all of backend and canary nodes can use a label selector, such as: tier=backend AND release_track=canary Just like labels, field selectors also let one select Kubernetes resources. Unlike labels, the selection is based on the attribute values inherent to the resource being selected, rather than user-defined categorization. metadata.name and metadata.namespace are field selectors that will be present on all Kubernetes objects. Other selectors that can be used depend on the object/resource type. === Add-ons === Add-ons are additional features of the Kubernetes cluster implemented as applications running within it. The pods may be managed by Deployments, ReplicationControllers, and so on. There are many add-ons. Some of the more important are: DNS Cluster DNS is a DNS server, in addition to the other DNS server(s) in the environment, which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches. Web UI This is a general purpose, web-based UI for Kubernetes clusters. It allows administrators to manage and troubleshoot applications running in the cluster, as well as the cluster itself. Resource monitoring Container Resource Monitoring records metrics about containers in a central database, and provides a UI for browsing that data. Cost monitoring Kubernetes cost monitoring applications allow breakdown of costs by pods, nodes, namespaces, and labels. Cluster-level logging To prevent the loss of event data in the event of node or pod failures, container logs can be saved to a central log store with a search/browsing interface. Kubernetes provides no native storage for log data, but one can integrate many existing logging solutions into the Kubernetes cluster. === Storage === Containers emerged as a way to make software portable. The container contains all the packages needed to run a service. The provided file system makes containers extremely",
    "text_hash": "c519961089b18f62fbd25e4863015b32",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000657",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 13,
    "text": "Kubernetes provides no native storage for log data, but one can integrate many existing logging solutions into the Kubernetes cluster. === Storage === Containers emerged as a way to make software portable. The container contains all the packages needed to run a service. The provided file system makes containers extremely portable and easy to use in development. A container can be moved from development to test or production with no or relatively few configuration changes. Historically Kubernetes was suitable only for stateless services. However, many applications have a database, which requires persistence, leading to the creation of persistent storage for Kubernetes. Implementing persistent storage for containers is one of the top challenges of Kubernetes administrators, DevOps and cloud engineers. Containers may be ephemeral, but more and more of their data is not, so one needs to ensure the data's survival in case of container termination or hardware failure. When deploying containers with Kubernetes or containerized applications, organizations often realize that they need persistent storage. They need to provide fast and reliable storage for databases, root images and other data used by the containers. In addition to the landscape, the Cloud Native Computing Foundation (CNCF), has published other information about Kubernetes Persistent Storage including a blog helping to define the container attached storage pattern. This pattern can be thought of as one that uses Kubernetes itself as a component of the storage system or service. More information about the relative popularity of these and other approaches can be found on the CNCF's landscape survey as well, which showed that OpenEBS \u2013 a Stateful Persistent Storage platform from Datacore Software, and Rook \u2013 a storage orchestration project \u2013 were the two projects most likely to be in evaluation as of the Fall of 2019. Container Attached Storage is a type of data",
    "text_hash": "458e5bc1019ceebda0b136d5c7a36350",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000658",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 14,
    "text": "CNCF's landscape survey as well, which showed that OpenEBS \u2013 a Stateful Persistent Storage platform from Datacore Software, and Rook \u2013 a storage orchestration project \u2013 were the two projects most likely to be in evaluation as of the Fall of 2019. Container Attached Storage is a type of data storage that emerged as Kubernetes gained prominence. The Container Attached Storage approach or pattern relies on Kubernetes itself for certain capabilities while delivering primarily block, file, object and interfaces to workloads running on Kubernetes. Common attributes of Container Attached Storage include the use of extensions to Kubernetes, such as custom resource definitions, and the use of Kubernetes itself for functions that otherwise would be separately developed and deployed for storage or data management. Examples of functionality delivered by custom resource definitions or by Kubernetes itself include retry logic, delivered by Kubernetes itself, and the creation and maintenance of an inventory of available storage media and volumes, typically delivered via a custom resource definition. ==== Container Storage Interface (CSI) ==== In Kubernetes version 1.9, the initial Alpha release of Container Storage Interface (CSI) was introduced. Previously, storage volume plug-ins were included in the Kubernetes distribution. By creating a standardized CSI, the code required to interface with external storage systems was separated from the core Kubernetes code base. Just one year later, the CSI feature was made Generally Available (GA) in Kubernetes. == API == A key component of the Kubernetes control plane is the API Server, which exposes an HTTP API that can be invoked by other parts of the cluster as well as end users and external components. This API is a REST API and is declarative in nature, and is the same API exposed to the control plane. The API server is backed by etcd to store all records",
    "text_hash": "5cce9eacb210fb68b797f51e46ac96a9",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000659",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 15,
    "text": "that can be invoked by other parts of the cluster as well as end users and external components. This API is a REST API and is declarative in nature, and is the same API exposed to the control plane. The API server is backed by etcd to store all records persistently. === API objects === In Kubernetes, all objects serve as the \"record of intent\" of the cluster's state, and are able to define the desired state that the writer of the object wishes for the cluster to be in. As such, most Kubernetes objects have the same set of nested fields, as follows: spec: Describes the desired state of the resource, which can be controlled by end users, or other higher-level controllers; status: Describes the current state of the resource, which is actively updated by the controller of the resource. All objects in Kubernetes are subject to the same API conventions. Some of these include: Must have the following metadata under the nested object field metadata: namespace: a label that objects are subdivided into; name: a string that uniquely identifies the object within the defined namespace; uid: a unique string that is able to distinguish between objects with the same name across space and time (even across deletions and recreations with the same name). May be managed by another controller, which is defined in the metadata.ownerReferences field: At most one other object shall be the managing controller of the controllee object, which is defined by the controller field. May be garbage collected if the owner is deleted: When an object is deleted, all dependent objects may also be deleted in a cascading fashion. === Custom resources, controllers and operators === The Kubernetes API can be extended using Custom Resources, which represent objects that are not part of the standard",
    "text_hash": "217f2ff8c0af9219492a2ef578c451af",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000660",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 16,
    "text": "be garbage collected if the owner is deleted: When an object is deleted, all dependent objects may also be deleted in a cascading fashion. === Custom resources, controllers and operators === The Kubernetes API can be extended using Custom Resources, which represent objects that are not part of the standard Kubernetes installation. These custom resources are declared using Custom Resource Definitions (CRDs), which is a kind of resource that can be dynamically registered and unregistered without shutting down or restarting a cluster that is currently running. Custom controllers are another extension mechanism that interact with the Kubernetes API, similar to the default controllers in the standard pre-installed Kubernetes controller manager. These controllers may interact with custom resources to allow for a declarative API: users may declare the desired state of the system via the custom resources, and it is the responsibility of the custom controller to observe the change and reconcile it. The combination of custom resources and custom controllers are often referred to as a Kubernetes Operator. The key use case for operators are to capture the aim of a human operator who is managing a service or set of services and to implement them using automation, and with a declarative API supporting this automation. Human operators who look after specific applications and services have deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems. Examples of problems solved by operators include taking and restoring backups of that application's state, and handling upgrades of the application code alongside related changes such as database schemas or extra configuration settings. Several notable projects under the Cloud Native Computing Foundation's incubation program follow the operator pattern to extend Kubernetes, including Argo, Open Policy Agent and Istio. === API security === Kubernetes",
    "text_hash": "806ef80fc99fc802b622e5319e995f9e",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000661",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 17,
    "text": "application's state, and handling upgrades of the application code alongside related changes such as database schemas or extra configuration settings. Several notable projects under the Cloud Native Computing Foundation's incubation program follow the operator pattern to extend Kubernetes, including Argo, Open Policy Agent and Istio. === API security === Kubernetes defines the following strategies for controlling access to its API. ==== Transport security ==== The Kubernetes API server listens on a TCP port that serves HTTPS traffic, in order to enforce transport layer security (TLS) using CA certificates. In older versions of Kubernetes, the API server supported listening on both HTTP and HTTPS ports (with the HTTP port number having no transport security whatsoever). This was deprecated in v1.10 and eventually dropped support in v1.20 of Kubernetes. ==== Authentication ==== All requests made to the Kubernetes API server are expected to be authenticated, and supports several authentication strategies, some of which are listed below: X.509 client certificates Bearer tokens Service account tokens, intended for programmatic API access Users are typically expected to indicate and define cluster URL details along with the necessary credentials in a kubeconfig file, which are natively supported by other Kubernetes tools like kubectl and the official Kubernetes client libraries. ==== Authorization ==== The Kubernetes API supports the following authorization modes: Node authorization mode: Grants a fixed list of operations of API requests that kubelets are allowed to perform, in order to function properly. Attribute-based access control (ABAC) mode: Grants access rights to users through the use of defined access control policies which combine attributes together. Role-based access control (RBAC) mode: Grants access rights to users based on roles that are granted to the user, where each role defines a list of actions that are allowed. Webhook mode: Queries a REST API service to determine if a",
    "text_hash": "27a8e47a4cf3ea06791d17be8333a9fc",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000662",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 18,
    "text": "of defined access control policies which combine attributes together. Role-based access control (RBAC) mode: Grants access rights to users based on roles that are granted to the user, where each role defines a list of actions that are allowed. Webhook mode: Queries a REST API service to determine if a user is authorized to perform a given action. If the Kubernetes configuration is overly permissive, it can potentially enable privilege escalation and allow threat actors to gain control over pods. Administrators should enforce Pod Security Standards and security contexts that disable privilege escalation and apply least privilege principles. === API clients === Kubernetes supports several official API clients: kubectl: Command-line for interacting with the Kubernetes control plane Official client libraries maintained by Kubernetes for C, .NET, Go, Haskell, Java, JavaScript, Perl, Python and Ruby === Cluster API === The same API design principles have been used to define an API to harness a program in order to create, configure, and manage Kubernetes clusters. This is called the Cluster API. A key concept embodied in the API is using Infrastructure as Software, or the notion that the Kubernetes cluster infrastructure is itself a resource / object that can be managed just like any other Kubernetes resources. Similarly, machines that make up the cluster are also treated as a Kubernetes resource. The API has two pieces \u2013 the core API, and a provider implementation. The provider implementation consists of cloud-provider specific functions that let Kubernetes provide the cluster API in a fashion that is well-integrated with the cloud-provider's services and resources. == Uses == Kubernetes is commonly used as a way to host a microservice-based implementation, because it and its associated ecosystem of tools provide all the capabilities needed to address key concerns of any microservice architecture. == Criticism == A common",
    "text_hash": "a692b10bf347e9611e48441eca19f2e2",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000663",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 19,
    "text": "that is well-integrated with the cloud-provider's services and resources. == Uses == Kubernetes is commonly used as a way to host a microservice-based implementation, because it and its associated ecosystem of tools provide all the capabilities needed to address key concerns of any microservice architecture. == Criticism == A common criticism of Kubernetes is that it is too complex. Google admitted this as well. == Distributions == Various vendors offer Kubernetes-based platforms or infrastructure as a service (IaaS) that deploy Kubernetes. These are typically categorized according to open-source, commercial or managed distributions. Several notable distributions are listed below: === Open-source distributions === Amazon EKS-D k0s k3s SUSE Rancher Kubernetes Engine (RKE) OKD.IO The Community Distribution of Kubernetes that powers Red Hat OpenShift === Commercial distributions === Nutanix Kubernetes Platform (formerly D2iQ Kubernetes Platform) Mirantis Kubernetes Engine (formerly Docker Enterprise) Red Hat OpenShift VMware VKS (vSphere Kubernetes Service) SUSE Rancher (offers options of k3s and RKE2 Kubernetes distributions) === Managed distributions === Alibaba Cloud ACK (Alibaba Cloud Container Service for Kubernetes) Amazon EKS (Elastic Kubernetes Service) Canonical MicroK8s and Charmed Kubernetes DigitalOcean managed Kubernetes Service Google GKE (Google Kubernetes Engine) Huawei CCE (Huawei Cloud Container Engine) IBM Cloud Kubernetes Services Microsoft AKS (Azure Kubernetes Services) Mirantis Kubernetes Engine with OpsCare Plus managed services Oracle Container Engine for Kubernetes Wind River Systems Wind River Studio == Release timeline == === Support windows === The chart below visualizes the period for which each release is/was supported == See also == Docker (software) List of cluster management software Open Service Mesh OpenShift == References == == External links == Official website Kubernetes on GitHub",
    "text_hash": "226b96194462576f8998e6b9167ff898",
    "is_adversarial": false
  },
  {
    "chunk_id": "chunk_000664",
    "article_title": "Kubernetes",
    "article_url": "https://en.wikipedia.org/wiki/Kubernetes",
    "article_page_id": "43291963",
    "chunk_index": 20,
    "text": "of cluster management software Open Service Mesh OpenShift == References == == External links == Official website Kubernetes on GitHub",
    "text_hash": "a63ccd40c47250494fd24a9b42082df9",
    "is_adversarial": false
  }
]